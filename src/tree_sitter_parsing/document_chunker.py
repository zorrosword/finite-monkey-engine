#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æ–‡æ¡£åˆ†å—å™¨ - åŸºäº adalflow TextSplitter çš„æ™ºèƒ½åˆ†å—å·¥å…·

åŠŸèƒ½ï¼š
- éå†æŒ‡å®šæ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰æ–‡ä»¶
- ä½¿ç”¨ adalflow TextSplitter è¿›è¡Œæ–‡æ¡£åˆ†å—
- æ”¯æŒå¤šç§åˆ†å—ç­–ç•¥å’Œå‚æ•°é…ç½®
- ä¸“ä¸ºé•¿æ–‡æœ¬ä¼˜åŒ–çš„æ®µè½åˆ†å‰²
- æ”¯æŒé…ç½®æ–‡ä»¶ç®¡ç†
- è¾“å‡ºç»“æ„åŒ–çš„åˆ†å—ç»“æœ

ä½œè€…ï¼šåŸºäº adalflow ç»„ä»¶å®ç°ï¼Œé›†æˆé•¿æ–‡æœ¬å¤„ç†èƒ½åŠ›
"""

import os
import re
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional, Literal
from dataclasses import dataclass, asdict

# å°è¯•å¯¼å…¥ adalflowï¼Œå¦‚æœä¸å¯ç”¨åˆ™ä½¿ç”¨ç®€å•å®ç°
try:
    from adalflow.components.data_process.text_splitter import TextSplitter
    from adalflow.core.types import Document
    ADALFLOW_AVAILABLE = True
except ImportError:
    ADALFLOW_AVAILABLE = False
    print("âš ï¸  adalflow ä¸å¯ç”¨ï¼Œä½¿ç”¨ç®€å•æ–‡æœ¬åˆ†å—å®ç°")

# å¯¼å…¥é…ç½®ç®¡ç†å™¨
try:
    from .chunk_config import ChunkConfig, ChunkConfigManager
except ImportError:
    try:
        from chunk_config import ChunkConfig, ChunkConfigManager
    except ImportError:
        print("âš ï¸  æ— æ³•å¯¼å…¥chunk_configï¼Œå°†ä½¿ç”¨åŸºç¡€é…ç½®")


@dataclass
class ChunkResult:
    """åˆ†å—ç»“æœæ•°æ®ç»“æ„"""
    chunk_id: str
    original_file: str
    chunk_text: str
    chunk_order: int
    parent_doc_id: str
    chunk_size: int
    metadata: Dict[str, Any]


@dataclass
class ProcessingStats:
    """å¤„ç†ç»Ÿè®¡ä¿¡æ¯"""
    total_files: int
    processed_files: int
    total_chunks: int
    skipped_files: List[str]
    error_files: List[str]


class SimpleTextSplitter:
    """ç®€å•æ–‡æœ¬åˆ†å—å™¨å®ç°ï¼ˆå½“adalflowä¸å¯ç”¨æ—¶ä½¿ç”¨ï¼‰"""
    
    def __init__(self, split_by="word", chunk_size=800, chunk_overlap=200, **kwargs):
        self.split_by = split_by
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
    
    def call(self, documents):
        """åˆ†å—å¤„ç†æ–‡æ¡£åˆ—è¡¨"""
        result = []
        for doc in documents:
            chunks = self._split_document(doc)
            result.extend(chunks)
        return result
    
    def _split_document(self, doc):
        """åˆ†å—å•ä¸ªæ–‡æ¡£"""
        text = doc.text
        chunks = []
        
        if self.split_by == "word":
            words = text.split()
            for i in range(0, len(words), self.chunk_size - self.chunk_overlap):
                chunk_words = words[i:i + self.chunk_size]
                chunk_text = ' '.join(chunk_words)
                
                if chunk_text.strip():
                    chunk_doc = type('Document', (), {
                        'id': f"{doc.id}_chunk_{len(chunks)}",
                        'text': chunk_text,
                        'order': len(chunks),
                        'parent_doc_id': doc.id,
                        'meta_data': doc.meta_data.copy() if doc.meta_data else {}
                    })()
                    chunks.append(chunk_doc)
        
        elif self.split_by == "sentence":
            # ç®€å•æŒ‰å¥å­åˆ†å‰²
            sentences = text.split('.')
            for i in range(0, len(sentences), self.chunk_size - self.chunk_overlap):
                chunk_sentences = sentences[i:i + self.chunk_size]
                chunk_text = '.'.join(chunk_sentences)
                
                if chunk_text.strip():
                    chunk_doc = type('Document', (), {
                        'id': f"{doc.id}_chunk_{len(chunks)}",
                        'text': chunk_text,
                        'order': len(chunks),
                        'parent_doc_id': doc.id,
                        'meta_data': doc.meta_data.copy() if doc.meta_data else {}
                    })()
                    chunks.append(chunk_doc)
        
        else:
            # æŒ‰å­—ç¬¦åˆ†å‰²
            for i in range(0, len(text), self.chunk_size - self.chunk_overlap):
                chunk_text = text[i:i + self.chunk_size]
                
                if chunk_text.strip():
                    chunk_doc = type('Document', (), {
                        'id': f"{doc.id}_chunk_{len(chunks)}",
                        'text': chunk_text,
                        'order': len(chunks),
                        'parent_doc_id': doc.id,
                        'meta_data': doc.meta_data.copy() if doc.meta_data else {}
                    })()
                    chunks.append(chunk_doc)
        
        return chunks


class SimpleDocument:
    """ç®€å•æ–‡æ¡£ç±»ï¼ˆå½“adalflowä¸å¯ç”¨æ—¶ä½¿ç”¨ï¼‰"""
    
    def __init__(self, text, id, meta_data=None):
        self.text = text
        self.id = id
        self.meta_data = meta_data or {}


class DocumentChunker:
    """æ–‡æ¡£åˆ†å—å™¨ç±»"""
    
    # æ”¯æŒçš„æ–‡ä»¶æ‰©å±•å
    SUPPORTED_EXTENSIONS = {
        '.txt', '.md', '.rst', '.py', '.js', '.ts', '.html', '.xml',
        '.json', '.yaml', '.yml', '.css', '.sql', '.sh', '.bat',
        '.c', '.cpp', '.h', '.hpp', '.java', '.php', '.rb', '.go',
        '.rs', '.scala', '.kt', '.swift', '.dart', '.r', '.m', '.sol', '.move','.cc'
    }
    
    def __init__(
        self,
        split_by: Literal["word", "sentence", "page", "passage", "token"] = "word",
        chunk_size: int = 800,
        chunk_overlap: int = 200,
        batch_size: int = 1000,
        encoding: str = 'utf-8',
        max_file_size_mb: float = 50.0,
        include_extensions: Optional[List[str]] = None,
        exclude_patterns: Optional[List[str]] = None,
        long_text_mode: bool = False
    ):
        """
        åˆå§‹åŒ–æ–‡æ¡£åˆ†å—å™¨
        
        Args:
            split_by: åˆ†å‰²ç­–ç•¥ ("word", "sentence", "page", "passage", "token")
            chunk_size: æ¯ä¸ªå—çš„æœ€å¤§å•ä½æ•°
            chunk_overlap: å—é—´é‡å å•ä½æ•°  
            batch_size: æ‰¹å¤„ç†å¤§å°
            encoding: æ–‡ä»¶ç¼–ç 
            max_file_size_mb: æœ€å¤§æ–‡ä»¶å¤§å°é™åˆ¶(MB)
            include_extensions: æŒ‡å®šåŒ…å«çš„æ–‡ä»¶æ‰©å±•å
            exclude_patterns: æ’é™¤çš„æ–‡ä»¶åæ¨¡å¼
            long_text_mode: é•¿æ–‡æœ¬æ¨¡å¼ï¼Œè‡ªåŠ¨ä¼˜åŒ–å‚æ•°ç”¨äºå¤„ç†é•¿æ–‡æ¡£
        """
        
        # é•¿æ–‡æœ¬æ¨¡å¼è‡ªåŠ¨ä¼˜åŒ–å‚æ•°
        self.long_text_mode = long_text_mode
        if long_text_mode:
            # ä¸ºé•¿æ–‡æœ¬passageåˆ†å‰²ä¼˜åŒ–å‚æ•°
            if split_by == "passage":
                chunk_size = max(chunk_size, 5)  # è‡³å°‘5ä¸ªæ®µè½
                chunk_overlap = max(chunk_overlap, 2)  # è‡³å°‘2ä¸ªæ®µè½é‡å 
                max_file_size_mb = max(max_file_size_mb, 100.0)  # å¢åŠ æ–‡ä»¶å¤§å°é™åˆ¶
            elif split_by == "word":
                chunk_size = max(chunk_size, 1500)  # é•¿æ–‡æœ¬é€‚ç”¨æ›´å¤§çš„å—
                chunk_overlap = max(chunk_overlap, 300)
        
        # åˆå§‹åŒ– TextSplitter
        if ADALFLOW_AVAILABLE:
            self.text_splitter = TextSplitter(
                split_by=split_by,
                chunk_size=chunk_size,
                chunk_overlap=chunk_overlap,
                batch_size=batch_size
            )
        else:
            self.text_splitter = SimpleTextSplitter(
                split_by=split_by,
                chunk_size=chunk_size,
                chunk_overlap=chunk_overlap
            )
        
        self.encoding = encoding
        self.max_file_size_bytes = max_file_size_mb * 1024 * 1024
        self.include_extensions = set(include_extensions) if include_extensions else None  # Noneè¡¨ç¤ºåŒ…å«æ‰€æœ‰æ–‡ä»¶
        self.exclude_patterns = exclude_patterns or []
        
        # é…ç½®æ—¥å¿—
        self.logger = logging.getLogger(__name__)
        
        # è¾“å‡ºåˆå§‹åŒ–ä¿¡æ¯
        print(f"ğŸ“„ DocumentChunker åˆå§‹åŒ–:")
        print(f"  - åˆ†å‰²ç­–ç•¥: {split_by}")
        print(f"  - å—å¤§å°: {chunk_size}")
        print(f"  - é‡å å¤§å°: {chunk_overlap}")
        print(f"  - é•¿æ–‡æœ¬æ¨¡å¼: {'æ˜¯' if long_text_mode else 'å¦'}")
        print(f"  - Adalflowå¯ç”¨: {'æ˜¯' if ADALFLOW_AVAILABLE else 'å¦'}")
        
        if long_text_mode:
            print(f"  ğŸ”„ é•¿æ–‡æœ¬æ¨¡å¼å·²å¯ç”¨ï¼Œå‚æ•°å·²ä¼˜åŒ–")
    
    @classmethod
    def from_config(cls, config: 'ChunkConfig'):
        """
        ä»é…ç½®å¯¹è±¡åˆ›å»ºåˆ†å—å™¨
        
        Args:
            config: ChunkConfigé…ç½®å¯¹è±¡
            
        Returns:
            DocumentChunker: é…ç½®å¥½çš„åˆ†å—å™¨å®ä¾‹
        """
        return cls(
            split_by=config.split_by,
            chunk_size=config.chunk_size,
            chunk_overlap=config.chunk_overlap,
            batch_size=config.batch_size,
            encoding=config.encoding,
            max_file_size_mb=config.max_file_size_mb,
            include_extensions=config.include_extensions,
            exclude_patterns=config.exclude_patterns,
            long_text_mode=config.long_text_mode
        )
    
    @classmethod
    def for_long_text_passage(
        cls,
        chunk_size: int = 8,
        chunk_overlap: int = 3,
        max_file_size_mb: float = 200.0,
        include_extensions: Optional[List[str]] = None
    ):
        """
        ä¸“é—¨ä¸ºé•¿æ–‡æœ¬passageåˆ†å‰²åˆ›å»ºçš„ä¾¿æ·æ„é€ å™¨
        
        Args:
            chunk_size: æ®µè½æ•°é‡ï¼Œå»ºè®®5-15ä¸ªæ®µè½
            chunk_overlap: æ®µè½é‡å æ•°ï¼Œå»ºè®®2-5ä¸ªæ®µè½
            max_file_size_mb: æœ€å¤§æ–‡ä»¶å¤§å°ï¼Œé•¿æ–‡æœ¬å»ºè®®100MBä»¥ä¸Š
            include_extensions: æ”¯æŒçš„æ–‡ä»¶ç±»å‹ï¼Œé»˜è®¤æ”¯æŒæ–‡æœ¬ç±»å‹
            
        Returns:
            DocumentChunker: é…ç½®å¥½çš„é•¿æ–‡æœ¬å¤„ç†å™¨
        """
        
        # é•¿æ–‡æœ¬å¸¸è§çš„æ–‡ä»¶ç±»å‹
        if include_extensions is None:
            include_extensions = [
                '.txt', '.md', '.rst', '.doc', '.docx',  # æ–‡æ¡£ç±»å‹
                '.pdf', '.rtf', '.odt',                   # å¯Œæ–‡æœ¬ç±»å‹  
                '.epub', '.mobi',                         # ç”µå­ä¹¦ç±»å‹
                '.html', '.xml'                           # æ ‡è®°è¯­è¨€
            ]
        
        return cls(
            split_by="passage",
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            batch_size=500,  # é•¿æ–‡æœ¬å‡å°‘æ‰¹å¤„ç†å¤§å°
            max_file_size_mb=max_file_size_mb,
            include_extensions=include_extensions,
            long_text_mode=True
        )
    
    def process_files(self, file_paths: List[str]) -> List[ChunkResult]:
        """
        å¤„ç†æ–‡ä»¶åˆ—è¡¨å¹¶è¿›è¡Œåˆ†å—
        
        Args:
            file_paths: æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            
        Returns:
            åˆ†å—ç»“æœåˆ—è¡¨
        """
        all_chunks = []
        
        for file_path in file_paths:
            try:
                chunks = self._process_single_file(Path(file_path))
                if chunks:
                    all_chunks.extend(chunks)
                    print(f"âœ… åˆ†å—æ–‡ä»¶: {Path(file_path).name} -> {len(chunks)} ä¸ªå—")
                    
            except Exception as e:
                print(f"âŒ åˆ†å—æ–‡ä»¶å¤±è´¥: {Path(file_path).name} - {str(e)}")
                continue
        
        return all_chunks
    
    def _should_process_file(self, file_path: Path) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥å¤„ç†è¯¥æ–‡ä»¶"""
        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        try:
            file_size = file_path.stat().st_size
            if file_size > self.max_file_size_bytes:
                print(f"âš ï¸  æ–‡ä»¶è¿‡å¤§ï¼Œè·³è¿‡åˆ†å—: {file_path.name} ({file_size / 1024 / 1024:.2f}MB)")
                return False
        except OSError:
            print(f"âš ï¸  æ— æ³•è®¿é—®æ–‡ä»¶: {file_path.name}")
            return False
        
        # æ£€æŸ¥æ’é™¤æ¨¡å¼
        file_path_str = str(file_path)
        for pattern in self.exclude_patterns:
            # å¦‚æœæ¨¡å¼ä»¥ç‚¹å¼€å¤´ï¼ŒæŒ‰æ‰©å±•åç²¾ç¡®åŒ¹é…
            if pattern.startswith('.'):
                if file_path.suffix == pattern or file_path.name.endswith(pattern):
                    return False
            # å¦åˆ™æŒ‰è·¯å¾„åŒ…å«åŒ¹é…
            elif pattern in file_path_str:
                return False
        
        # æ£€æŸ¥åŒ…å«æ‰©å±•åï¼ˆåªæœ‰å½“include_extensionsä¸ä¸ºNoneæ—¶æ‰æ£€æŸ¥ï¼‰
        if self.include_extensions is not None:
            file_ext = file_path.suffix.lower()
            # å°†include_extensionsä¹Ÿè½¬æ¢ä¸ºå°å†™è¿›è¡Œæ¯”è¾ƒ
            include_exts_lower = {ext.lower() for ext in self.include_extensions}
            if file_ext not in include_exts_lower and file_path.name not in self.include_extensions:
                return False
        
        return True
    
    def _process_single_file(self, file_path: Path) -> List[ChunkResult]:
        """å¤„ç†å•ä¸ªæ–‡ä»¶"""
        if not self._should_process_file(file_path):
            return []
            
        try:
            # è¯»å–æ–‡ä»¶å†…å®¹
            content = self._read_file_with_encoding(file_path)
            
            if not content or len(content.strip()) < 50:  # è·³è¿‡è¿‡çŸ­çš„æ–‡ä»¶
                return []
            
            # é•¿æ–‡æœ¬æ¨¡å¼çš„ç‰¹æ®Šé¢„å¤„ç†
            if self.long_text_mode and self.text_splitter.split_by == "passage":
                content = self._preprocess_long_text(content)
            
            # åˆ›å»º Document å¯¹è±¡
            if ADALFLOW_AVAILABLE:
                doc = Document(
                    text=content,
                    id=str(file_path),
                    meta_data={
                        'file_name': file_path.name,
                        'file_path': str(file_path),
                        'file_size': file_path.stat().st_size,
                        'file_extension': file_path.suffix
                    }
                )
            else:
                doc = SimpleDocument(
                    text=content,
                    id=str(file_path),
                    meta_data={
                        'file_name': file_path.name,
                        'file_path': str(file_path),
                        'file_size': file_path.stat().st_size,
                        'file_extension': file_path.suffix
                    }
                )
            
            # ä½¿ç”¨ TextSplitter è¿›è¡Œåˆ†å—
            split_docs = self.text_splitter.call([doc])
            
            # è½¬æ¢ä¸º ChunkResult å¯¹è±¡
            chunks = []
            for split_doc in split_docs:
                chunk = ChunkResult(
                    chunk_id=split_doc.id,
                    original_file=str(file_path),
                    chunk_text=split_doc.text,
                    chunk_order=split_doc.order,
                    parent_doc_id=split_doc.parent_doc_id,
                    chunk_size=len(split_doc.text.split()) if self.text_splitter.split_by == "word" else len(split_doc.text),
                    metadata=split_doc.meta_data or {}
                )
                chunks.append(chunk)
            
            return chunks
            
        except Exception as e:
            print(f"âš ï¸  å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™ {file_path}: {e}")
            return []
    
    def _read_file_with_encoding(self, file_path: Path) -> str:
        """å°è¯•ç”¨ä¸åŒç¼–ç è¯»å–æ–‡ä»¶"""
        encodings = [self.encoding, 'utf-8', 'gbk', 'latin1', 'cp1252']
        
        for encoding in encodings:
            try:
                with open(file_path, 'r', encoding=encoding, errors='ignore') as f:
                    content = f.read().strip()
                return content
            except UnicodeDecodeError:
                continue
            except Exception:
                continue
        
        # å¦‚æœæ‰€æœ‰ç¼–ç éƒ½å¤±è´¥ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²
        return ""
    
    def _preprocess_long_text(self, content: str) -> str:
        """
        é•¿æ–‡æœ¬é¢„å¤„ç†ï¼Œä¼˜åŒ–passageåˆ†å‰²æ•ˆæœ
        
        Args:
            content: åŸå§‹æ–‡æœ¬å†…å®¹
            
        Returns:
            str: é¢„å¤„ç†åçš„æ–‡æœ¬
        """
        # 1. æ ‡å‡†åŒ–æ¢è¡Œç¬¦
        content = content.replace('\r\n', '\n').replace('\r', '\n')
        
        # 2. æ¸…ç†å¤šä½™çš„ç©ºè¡Œï¼Œä½†ä¿ç•™æ®µè½åˆ†éš”
        lines = content.split('\n')
        cleaned_lines = []
        prev_empty = False
        
        for line in lines:
            line = line.strip()
            if line:  # éç©ºè¡Œ
                cleaned_lines.append(line)
                prev_empty = False
            else:  # ç©ºè¡Œ
                if not prev_empty:  # åªä¿ç•™ä¸€ä¸ªç©ºè¡Œä½œä¸ºæ®µè½åˆ†éš”
                    cleaned_lines.append('')
                    prev_empty = True
        
        # 3. é‡æ–°ç»„åˆï¼Œç¡®ä¿æ®µè½ä¹‹é—´æœ‰åŒæ¢è¡Œ
        processed_content = '\n'.join(cleaned_lines)
        
        # 4. ç¡®ä¿æ®µè½åˆ†éš”ç¬¦æ­£ç¡®
        # å°†å•æ¢è¡Œåçš„ç©ºè¡Œè½¬æ¢ä¸ºåŒæ¢è¡Œ
        processed_content = processed_content.replace('\n\n', '\n\n')  # ä¿æŒç°æœ‰çš„åŒæ¢è¡Œ
        
        # 5. å¤„ç†ç« èŠ‚æ ‡é¢˜ï¼ˆå¦‚æœæœ‰æ˜æ˜¾çš„æ ‡é¢˜æ ‡è®°ï¼‰
        if self._detect_chapter_markers(processed_content):
            processed_content = self._enhance_chapter_separation(processed_content)
        
        # 6. ç»Ÿè®¡å¤„ç†ç»“æœ
        original_paragraphs = content.count('\n\n') + 1
        processed_paragraphs = processed_content.count('\n\n') + 1
        
        if hasattr(self, 'logger') and self.logger:
            self.logger.info(f"ğŸ“ é•¿æ–‡æœ¬é¢„å¤„ç†å®Œæˆ: {original_paragraphs} -> {processed_paragraphs} ä¸ªæ®µè½")
        
        return processed_content
    
    def _detect_chapter_markers(self, content: str) -> bool:
        """æ£€æµ‹æ˜¯å¦æœ‰ç« èŠ‚æ ‡è®°"""
        chapter_patterns = [
            r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+ç« ',  # ä¸­æ–‡ç« èŠ‚
            r'Chapter\s+\d+',                   # è‹±æ–‡ç« èŠ‚
            r'^#{1,3}\s+',                      # Markdownæ ‡é¢˜
            r'^\d+\.\s+[A-Z]',                 # æ•°å­—æ ‡é¢˜
        ]
        
        for pattern in chapter_patterns:
            if re.search(pattern, content, re.MULTILINE):
                return True
        return False
    
    def _enhance_chapter_separation(self, content: str) -> str:
        """å¢å¼ºç« èŠ‚åˆ†éš”"""
        # åœ¨ç« èŠ‚æ ‡é¢˜å‰æ·»åŠ é¢å¤–çš„åˆ†éš”
        patterns = [
            (r'(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+ç« )', r'\n\n\1'),
            (r'(Chapter\s+\d+)', r'\n\n\1'),
            (r'^(#{1,3}\s+)', r'\n\n\1', re.MULTILINE),
        ]
        
        for pattern, replacement, *flags in patterns:
            flag = flags[0] if flags else 0
            content = re.sub(pattern, replacement, content, flags=flag)
        
        return content


def chunk_project_files(file_paths: List[str], config: Optional['ChunkConfig'] = None, **kwargs) -> List[ChunkResult]:
    """
    å¯¹é¡¹ç›®æ–‡ä»¶è¿›è¡Œåˆ†å—çš„ä¾¿æ·å‡½æ•°
    
    Args:
        file_paths: æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        config: ChunkConfigé…ç½®å¯¹è±¡ï¼Œå¦‚æœæä¾›åˆ™ä¼˜å…ˆä½¿ç”¨
        **kwargs: DocumentChunkerçš„å‚æ•°ï¼ˆå½“configä¸ºNoneæ—¶ä½¿ç”¨ï¼‰
    
    Returns:
        åˆ†å—ç»“æœåˆ—è¡¨
    """
    if config:
        chunker = DocumentChunker.from_config(config)
    else:
        chunker = DocumentChunker(**kwargs)
    return chunker.process_files(file_paths)


def chunk_project_files_with_preset(file_paths: List[str], preset: str = "code_project") -> List[ChunkResult]:
    """
    ä½¿ç”¨é¢„è®¾é…ç½®å¯¹é¡¹ç›®æ–‡ä»¶è¿›è¡Œåˆ†å—
    
    Args:
        file_paths: æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        preset: é¢„è®¾é…ç½®åç§°ï¼Œé»˜è®¤ä¸º'code_project'
    
    Returns:
        åˆ†å—ç»“æœåˆ—è¡¨
    """
    config = ChunkConfigManager.get_config(preset)
    return chunk_project_files(file_paths, config=config)


if __name__ == "__main__":
    # ç®€å•æµ‹è¯•
    import tempfile
    
    print("ğŸ§ª æµ‹è¯•DocumentChunker...")
    
    with tempfile.TemporaryDirectory() as temp_dir:
        # åˆ›å»ºæµ‹è¯•æ–‡ä»¶
        test_file = Path(temp_dir) / 'test.txt'
        with open(test_file, 'w', encoding='utf-8') as f:
            f.write("è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æ¡£ã€‚" * 100)  # åˆ›å»ºè¾ƒé•¿çš„å†…å®¹
        
        # æµ‹è¯•åˆ†å—
        chunker = DocumentChunker(chunk_size=50, chunk_overlap=10)
        chunks = chunker.process_files([str(test_file)])
        
        print(f"âœ… æµ‹è¯•å®Œæˆï¼Œç”Ÿæˆ {len(chunks)} ä¸ªåˆ†å—")
        for i, chunk in enumerate(chunks[:3]):  # æ˜¾ç¤ºå‰3ä¸ª
            print(f"  å— {i+1}: {len(chunk.chunk_text)} å­—ç¬¦")