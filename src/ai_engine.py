from concurrent.futures import ThreadPoolExecutor
import json
import re
import threading
import time
from typing import List
import requests
import tqdm
from sklearn.metrics.pairwise import cosine_similarity
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
import warnings
import urllib3
warnings.filterwarnings('ignore', category=urllib3.exceptions.NotOpenSSLWarning)
from dao.entity import Project_Task
from prompt_factory.prompt_assembler import PromptAssembler
from prompt_factory.core_prompt import CorePrompt
from openai_api.openai import *
class AiEngine(object):

    def __init__(self, planning, taskmgr,lancedb,lance_table_name,project_audit):
        # Step 1: è·å–results
        self.planning = planning
        self.project_taskmgr = taskmgr
        self.lancedb=lancedb
        self.lance_table_name=lance_table_name
        self.project_audit=project_audit
    def do_planning(self):
        self.planning.do_planning()
    def process_task_do_scan(self,task, filter_func = None, is_gpt4 = False):
        
        response_final = ""
        response_vul = ""

        # print("query vul %s - %s" % (task.name, task.rule))

        result = task.get_result(is_gpt4)
        business_flow_code = task.business_flow_code
        if_business_flow_scan = task.if_business_flow_scan
        function_code=task.content
        
        # è¦è¿›è¡Œæ£€æµ‹çš„ä»£ç ç²’åº¦
        code_to_be_tested=business_flow_code if if_business_flow_scan=="1" else function_code
        if result is not None and len(result) > 0 and str(result).strip() != "NOT A VUL IN RES no":
            print("\t skipped (scanned)")
        else:
            to_scan = filter_func is None or filter_func(task)
            if not to_scan:
                print("\t skipped (filtered)")
            else:
                print("\t to scan")

                
                if os.getenv("SCAN_MODE","COMMON_VUL")=="OPTIMIZE":  
                    prompt=PromptAssembler.assemble_optimize_prompt(code_to_be_tested)
                elif os.getenv("SCAN_MODE","COMMON_VUL")=="CHECKLIST":
                    print("ğŸ“‹Generating checklist...")
                    prompt=PromptAssembler.assemble_checklists_prompt(code_to_be_tested)
                    response_checklist=cut_reasoning_content(ask_deepseek(prompt))
                    print("[DEBUGğŸ]ğŸ“‹response_checklist length: ",len(response_checklist))
                    print(f"[DEBUGğŸ]ğŸ“‹response_checklist: {response_checklist[:50]}...")
                    prompt=PromptAssembler.assemble_checklists_prompt_for_scan(code_to_be_tested,response_checklist)
                elif os.getenv("SCAN_MODE","COMMON_VUL")=="CHECKLIST_PIPELINE":
                    # ä»taskçš„descriptionå­—æ®µè·å–checklist
                    checklist = task.description
                    print(f"[DEBUGğŸ]ğŸ“‹Using checklist from task description: {checklist[:50]}...")
                    prompt = PromptAssembler.assemble_prompt_for_checklist_pipeline(code_to_be_tested, checklist)
                elif os.getenv("SCAN_MODE","COMMON_VUL")=="COMMON_PROJECT":
                    prompt=PromptAssembler.assemble_prompt_common(code_to_be_tested)
                elif os.getenv("SCAN_MODE","COMMON_VUL")=="PURE_SCAN":
                    prompt=PromptAssembler.assemble_prompt_pure(code_to_be_tested)
                elif os.getenv("SCAN_MODE","COMMON_VUL")=="SPECIFIC_PROJECT":
                    # æ„å»ºæç¤ºæ¥åˆ¤æ–­ä¸šåŠ¡ç±»å‹
                    business_type=task.recommendation
                    # print(f"[DEBUG] business_type: {business_type}")
                    # æ•°æ®åº“ä¸­ä¿å­˜çš„å½¢å¼æ˜¯xxxx,xxxxx,xxxx... è½¬æˆassemble_prompt_for_specific_projectå¯ä»¥æ¥æ”¶çš„æ•°ç»„å½¢å¼
                    business_type_list=business_type.split(',')
                    # print(f"[DEBUG] business_type_list: {business_type_list}")
                    # prompt = PromptAssembler.assemble_prompt_for_specific_project_directly_ask(code_to_be_tested, business_type_list)
                    prompt = PromptAssembler.assemble_prompt_for_specific_project(code_to_be_tested, business_type_list)
                    # print(f"[DEBUG] Generated prompt: {prompt}")
                response_vul=ask_claude(prompt)
                print(f"[DEBUG] Claude response: {response_vul}")
                response_vul = response_vul if response_vul is not None else "no"                
                self.project_taskmgr.update_result(task.id, response_vul, "","")
    def do_scan(self, is_gpt4=False, filter_func=None):
        # self.llm.init_conversation()

        tasks = self.project_taskmgr.get_task_list()
        if len(tasks) == 0:
            return

        # å®šä¹‰çº¿ç¨‹æ± ä¸­çš„çº¿ç¨‹æ•°é‡
        max_threads = int(os.getenv("MAX_THREADS_OF_SCAN", 5))

        with ThreadPoolExecutor(max_workers=max_threads) as executor:
            futures = [executor.submit(self.process_task_do_scan, task, filter_func, is_gpt4) for task in tasks]
            
            with tqdm(total=len(tasks), desc="Processing tasks") as pbar:
                for future in as_completed(futures):
                    future.result()  # ç­‰å¾…æ¯ä¸ªä»»åŠ¡å®Œæˆ
                    pbar.update(1)  # æ›´æ–°è¿›åº¦æ¡

        return tasks
    def process_task_check_vul(self, task:Project_Task):
        print("\n" + "="*80)
        print(f"ğŸ” å¼€å§‹å¤„ç†ä»»åŠ¡ ID: {task.id}")
        print("="*80)
        
        # ç”¨äºæ”¶é›†æ‰€æœ‰åˆ†æç»“æœ
        analysis_collection = []
        
        starttime = time.time()
        result = task.get_result(False)
        result_CN = task.get_result_CN()
        category_mark = task.get_category()
        
        if result_CN is not None and len(result_CN) > 0 and result_CN != "None" and category_mark is not None and len(category_mark)>0:
            print("\nğŸ”„ è¯¥ä»»åŠ¡å·²å¤„ç†å®Œæˆï¼Œè·³è¿‡...")
            return
            
        print("\nğŸ” å¼€å§‹æ¼æ´ç¡®è®¤æµç¨‹...")
        print(f"ğŸ“ åŸå§‹æ‰«æç»“æœé•¿åº¦: {len(result)}")
        
        function_code = task.content
        if_business_flow_scan = task.if_business_flow_scan
        business_flow_code = task.business_flow_code
        business_flow_context = task.business_flow_context
        
        code_to_be_tested = business_flow_code+"\n"+business_flow_context if if_business_flow_scan=="1" else function_code
        print(f"\nğŸ“Š åˆ†æä»£ç ç±»å‹: {'ä¸šåŠ¡æµç¨‹ä»£ç ' if if_business_flow_scan=='1' else 'å‡½æ•°ä»£ç '}")
        
        # ç¬¬ä¸€è½®åˆ†æ
        print("\n=== ç¬¬ä¸€è½®åˆ†æå¼€å§‹ ===")
        print("ğŸ“ æ­£åœ¨åˆ†ææ½œåœ¨æ¼æ´...")
        prompt = PromptAssembler.assemble_vul_check_prompt(code_to_be_tested, result)
        

        initial_response = common_ask_confirmation(prompt)
        if not initial_response or initial_response == "":
            print(f"âŒ Error: Empty response received for task {task.id}")
            return
        
        print("\nğŸ“Š Initial Analysis Result Length:")
        print("-" * 80)
        print(len(initial_response))
        print("-" * 80)

        # æ”¶é›†åˆå§‹åˆ†æç»“æœ
        analysis_collection.append("=== åˆå§‹åˆ†æç»“æœ ===")
        analysis_collection.append(initial_response)

        # å¯¹initial_responseè¿›è¡Œprocess_round_responseå¤„ç†
        initial_result_status = self.process_round_response(initial_response)
        analysis_collection.append("=== åˆå§‹åˆ†æçŠ¶æ€ ===")
        analysis_collection.append(initial_result_status)

        # æå–æ‰€éœ€ä¿¡æ¯
        required_info = self.extract_required_info(initial_response)
        if required_info:
            analysis_collection.append("=== éœ€è¦è¿›ä¸€æ­¥åˆ†æçš„ä¿¡æ¯ ===")
            analysis_collection.extend(required_info)

        if "no" in initial_result_status:
            print("\nğŸ›‘ Initial analysis shows clear 'no vulnerability' - stopping further analysis")
            response_final = "no"
            final_response = "Analysis stopped after initial round due to clear 'no vulnerability' result"
            
            # æ ¼å¼åŒ–æ‰€æœ‰æ”¶é›†çš„ç»“æœ
            formatted_results = "\n\n".join(str(item or '').strip() for item in analysis_collection)
            
            # åœ¨æ›´æ–°æ•°æ®åº“ä¹‹å‰æ¸…ç†å­—ç¬¦ä¸²
            formatted_results = formatted_results.replace('\x00', '')
            
            self.project_taskmgr.update_result(task.id, result, response_final, final_response)
            self.project_taskmgr.update_category(task.id, formatted_results)
            
            endtime = time.time()
            time_cost = endtime - starttime
            print("\n=== Task Summary ===")
            print(f"â±ï¸ Time cost: {time_cost:.2f} seconds")
            print(f"ğŸ“ Analyses performed: 1")
            print(f"ğŸ Final status Length: {len(response_final)}")
            print("=" * 80 + "\n")
            return
        
        # è®¾ç½®æœ€å¤§ç¡®è®¤è½®æ•°
        max_rounds = int(os.getenv("MAX_CONFIRMATION_ROUNDS", 3))
        request_per_round = int(os.getenv("REQUESTS_PER_CONFIRMATION_ROUND", 3))
        confirmation_results = []
        response_final = None
        final_response = None
        
        # è·å–åˆå§‹ä»£ç ä¸Šä¸‹æ–‡
        current_code = code_to_be_tested
        current_response = initial_response
        
        for round_num in range(max_rounds):
            print(f"\n=== ç¡®è®¤è½®æ¬¡ {round_num + 1}/{max_rounds} ===")
            
            # æå–æ‰€éœ€çš„é¢å¤–ä¿¡æ¯
            required_info = self.extract_required_info(current_response)
            
            current_additional_info = []  # ç”¨äºæ”¶é›†æœ¬è½®çš„æ‰€æœ‰é¢å¤–ä¿¡æ¯
            
            if required_info:
                print(f"\nğŸ” ç¬¬ {round_num + 1} è½®éœ€è¦é¢å¤–ä¿¡æ¯:")
                for i, info in enumerate(required_info, 1):
                    print(f"{i}. {info}")
                
                # è·å–ç½‘ç»œæœç´¢ä¿¡æ¯
                print("\nğŸŒ æ£€æŸ¥æ˜¯å¦éœ€è¦ç½‘ç»œæœç´¢...")
                internet_info = self.get_additional_internet_info(required_info)
                if internet_info:
                    current_additional_info.append("=== Internet Search Results ===")
                    current_additional_info.append(internet_info)
                    analysis_collection.append(f"=== ç¬¬ {round_num + 1} è½®ç½‘ç»œæœç´¢ç»“æœ ===")
                    analysis_collection.append(internet_info)
                
                # è·å–é¢å¤–ä¸Šä¸‹æ–‡
                print("\nğŸ“¥ æ­£åœ¨è·å–é¢å¤–ä¸Šä¸‹æ–‡...")
                additional_context = self.get_additional_context(required_info)
                if additional_context:
                    print(f"\nğŸ“¦ è·å–åˆ°æ–°çš„ä¸Šä¸‹æ–‡ (é•¿åº¦: {len(additional_context)} å­—ç¬¦)")
                    current_additional_info.append("=== Additional Context ===")
                    current_additional_info.append(additional_context)
                    analysis_collection.append(f"=== ç¬¬ {round_num + 1} è½®é¢å¤–ä¸Šä¸‹æ–‡ ===")
                    analysis_collection.append(additional_context)
            
            # ç»„åˆæ‰€æœ‰é¢å¤–ä¿¡æ¯
            if current_additional_info:
                current_code = "\n\n".join(current_additional_info)
            
            # ä½¿ç”¨å½“å‰ä¸Šä¸‹æ–‡è¿›è¡Œç¡®è®¤
            print(f"\nğŸ“Š ä½¿ç”¨å½“å‰ä¸Šä¸‹æ–‡è¿›è¡Œç¬¬ {round_num + 1} è½®ç¡®è®¤...")
            prompt = PromptAssembler.assemble_vul_check_prompt_final(current_code, result)
            round_response = ""
            for request_num in range(request_per_round):
                print(f"\nğŸ” ç¬¬ {request_num + 1} / {request_per_round} æ¬¡è¯¢é—®")
                sub_round_response = common_ask_confirmation(prompt)
            
                print(f"\nğŸ“‹ ç¬¬ {request_num + 1} æ¬¡è¯¢é—®ç»“æœé•¿åº¦: {len(sub_round_response)}")
            
                # æ”¶é›†åˆ†æç»“æœ
                analysis_collection.append(f"=== ç¬¬ {round_num + 1} è½® {request_num + 1} æ¬¡è¯¢é—®åˆ†æç»“æœ ===")
                analysis_collection.append(sub_round_response)
            
                # å¤„ç†å“åº”ç»“æœ
                if len(sub_round_response) == 0:
                    print(f"\nâŒ æ— æ•ˆçš„å“åº”: ç¬¬ {round_num + 1} è½® {request_num + 1} æ¬¡è¯¢é—®ç»“æœä¸ºç©º")
                    continue
                sub_result_status = self.process_round_response(sub_round_response)
                analysis_collection.append(f"=== ç¬¬ {round_num + 1} è½® {request_num + 1} æ¬¡åˆ†æçŠ¶æ€ ===")
                print(f"=== ç¬¬ {round_num + 1} è½® {request_num + 1} æ¬¡åˆ†æçŠ¶æ€ ===") # @debug
                analysis_collection.append(sub_result_status)
                print(sub_result_status) # @debug
            
                confirmation_results.append(sub_result_status)
                round_response += sub_round_response + "\n"

                # æ£€æŸ¥æ˜¯å¦æœ‰æ˜ç¡®çš„"no"ç»“æœ
                if "no" in sub_result_status:
                    print("\nğŸ›‘ å‘ç°æ˜ç¡®çš„'æ— æ¼æ´'ç»“æœ - åœæ­¢è¿›ä¸€æ­¥åˆ†æ")
                    response_final = "no"
                    final_response = f"åˆ†æåœ¨ç¬¬ {round_num + 1} è½® {request_num + 1} æ¬¡è¯¢é—®ååœæ­¢ï¼Œå› ä¸ºå‘ç°æ˜ç¡®çš„'æ— æ¼æ´'ç»“æœ"
                    break
            
            current_response = round_response  # æ›´æ–°å½“å‰å“åº”ç”¨äºä¸‹ä¸€è½®åˆ†æ
        
        # åªæœ‰åœ¨æ²¡æœ‰æå‰é€€å‡ºçš„æƒ…å†µä¸‹æ‰è¿›è¡Œå¤šæ•°æŠ•ç¥¨
        if response_final != "no":
            # ç»Ÿè®¡ç»“æœ
            yes_count = sum(1 for r in confirmation_results if "yes" in r or "confirmed" in r)
            no_count = sum(1 for r in confirmation_results if "no" in r and "vulnerability" in r)
            
            if yes_count >= 2:
                response_final = "yes"
                print("\nâš ï¸ æœ€ç»ˆç»“æœ: æ¼æ´å·²ç¡®è®¤ (2+ æ¬¡ç¡®è®¤)")
            elif no_count >= 2:
                response_final = "no"
                print("\nâœ… æœ€ç»ˆç»“æœ: æ— æ¼æ´ (2+ æ¬¡å¦å®š)")
            else:
                response_final = "not sure"
                print("\nâ“ æœ€ç»ˆç»“æœ: ä¸ç¡®å®š (ç»“æœä¸æ˜ç¡®)")
            
            final_response = "\n".join([f"Round {i+1} Analysis:\n{resp}" for i, resp in enumerate(confirmation_results)])
        
        # æ·»åŠ æœ€ç»ˆç»“è®º
        analysis_collection.append("=== æœ€ç»ˆç»“è®º ===")
        analysis_collection.append(f"ç»“æœ: {response_final}")
        analysis_collection.append(f"è¯¦ç»†è¯´æ˜: {final_response}")
        
        # æ ¼å¼åŒ–æ‰€æœ‰æ”¶é›†çš„ç»“æœ
        formatted_results = "\n\n".join(str(item or '').strip() for item in analysis_collection)
        
        # åœ¨æ›´æ–°æ•°æ®åº“ä¹‹å‰æ¸…ç†å­—ç¬¦ä¸²
        formatted_results = formatted_results.replace('\x00', '')
        
        self.project_taskmgr.update_result(task.id, result, response_final, final_response)
        self.project_taskmgr.update_category(task.id, formatted_results)
        
        endtime = time.time()
        time_cost = endtime - starttime
        
        print("\n=== Task Summary ===")
        print(f"â±ï¸ Time cost: {time_cost:.2f} seconds")
        print(f"ğŸ“ Analyses performed: {len(confirmation_results)}")
        print(f"ğŸ Final status Length: {len(response_final)}")
        print("=" * 80 + "\n")

    def process_round_response(self, round_response):
        """
        å¤„ç†æ¯è½®åˆ†æçš„å“åº”ï¼Œæå–ç»“æœçŠ¶æ€ï¼Œå¢åŠ é˜²å¾¡æ€§ç¼–ç¨‹
        
        Args:
            round_response: å½“å‰è½®æ¬¡çš„å“åº”
            
        Returns:
            str: æå–çš„ç»“æœçŠ¶æ€
        """
        prompt_translate_to_json = PromptAssembler.brief_of_response()
        
        # ä½¿ç”¨ common_ask_for_json è·å– JSON å“åº”
        round_json_response = str(common_ask_for_json(round_response+"\n"+prompt_translate_to_json))
        print("\nğŸ“‹ JSON Response Length:")
        print(len(round_json_response))
        
        try:
            # # æ¸…ç†å“åº”
            # cleaned_response = round_json_response.strip()
            # cleaned_response = cleaned_response.replace("```json", "").replace("```", "")
            # print("**********",cleaned_response)
            
            # # ç¡®ä¿å“åº”æ˜¯æœ‰æ•ˆçš„ JSON æ ¼å¼
            # # if not cleaned_response.startswith("{"):
            # #     cleaned_response = "{" + cleaned_response
            # # if not cleaned_response.endswith("}"):
            # #     cleaned_response = cleaned_response + "}"
            cleaned_response = round_json_response
            print(f"\nğŸ” æ¸…ç†åçš„å“åº”: {cleaned_response}")
            
            # è§£æ JSON
            response_data = json.loads(cleaned_response)
            
            # è·å–ç»“æœçŠ¶æ€ï¼Œä½¿ç”¨ get æ–¹æ³•æä¾›é»˜è®¤å€¼
            result_status = response_data.get("result", "not sure").lower()
            
            print(f"\nğŸ¯ æå–çš„ç»“æœçŠ¶æ€: {result_status}")
            print(f"ğŸ“ ç»“æœçŠ¶æ€é•¿åº¦: {len(result_status)}")
            
            # éªŒè¯ç»“æœçŠ¶æ€çš„æœ‰æ•ˆæ€§
            valid_statuses = {"yes", "no", "need creator to decide", "confirmed"}
            if not any(status in result_status for status in valid_statuses):
                print("\nâš ï¸ æ— æ•ˆçš„ç»“æœçŠ¶æ€ - æ ‡è®°ä¸º 'not sure'")
                return "not sure"
            
            return result_status
        
        except json.JSONDecodeError as e:
            print(f"\nâš ï¸ JSON è§£æé”™è¯¯: {str(e)} - æ ‡è®°ä¸º 'not sure'")
            return "not sure"
        except Exception as e:
            print(f"\nâš ï¸ æ„å¤–é”™è¯¯: {str(e)} - æ ‡è®°ä¸º 'not sure'")
            return "not sure"

    def get_related_functions(self,query,k=3):
        query_embedding = common_get_embedding(query)
        table = self.lancedb.open_table(self.lance_table_name)
        return table.search(query_embedding).limit(k).to_list()
    
    def extract_related_functions_by_level(self, function_names: List[str], level: int) -> str:
        """
        ä»call_treesä¸­æå–æŒ‡å®šå‡½æ•°ç›¸å…³çš„ä¸Šä¸‹æ¸¸å‡½æ•°ä¿¡æ¯å¹¶æ‰å¹³åŒ–å¤„ç†
        
        Args:
            function_names: è¦åˆ†æçš„å‡½æ•°ååˆ—è¡¨
            level: è¦åˆ†æçš„å±‚çº§æ·±åº¦
            
        Returns:
            str: æ‰€æœ‰ç›¸å…³å‡½æ•°å†…å®¹çš„æ‹¼æ¥æ–‡æœ¬
        """
        def get_functions_from_tree(tree, current_level=0, max_level=level, collected_funcs=None, level_stats=None):
            """é€’å½’è·å–æ ‘ä¸­æŒ‡å®šå±‚çº§å†…çš„æ‰€æœ‰å‡½æ•°ä¿¡æ¯"""
            if collected_funcs is None:
                collected_funcs = []
            if level_stats is None:
                level_stats = {}
                
            if not tree or current_level > max_level:
                return collected_funcs, level_stats
                    
            # æ·»åŠ å½“å‰èŠ‚ç‚¹çš„å‡½æ•°ä¿¡æ¯
            if tree['function_data']:
                collected_funcs.append(tree['function_data'])
                # æ›´æ–°å±‚çº§ç»Ÿè®¡
                level_stats[current_level] = level_stats.get(current_level, 0) + 1
                    
            # é€’å½’å¤„ç†å­èŠ‚ç‚¹
            if current_level < max_level:
                for child in tree['children']:
                    get_functions_from_tree(child, current_level + 1, max_level, collected_funcs, level_stats)
                        
            return collected_funcs, level_stats

        all_related_functions = []
        statistics = {
            'total_layers': level,
            'upstream_stats': {},
            'downstream_stats': {}
        }
        
        # ä½¿ç”¨é›†åˆè¿›è¡Œæ›´ä¸¥æ ¼çš„å»é‡
        seen_functions = set()  # å­˜å‚¨å‡½æ•°çš„å”¯ä¸€æ ‡è¯†ç¬¦
        unique_functions = []   # å­˜å‚¨å»é‡åçš„å‡½æ•°
        
        # éå†æ¯ä¸ªæŒ‡å®šçš„å‡½æ•°å
        for func_name in function_names:
            # åœ¨call_treesä¸­æŸ¥æ‰¾å¯¹åº”çš„æ ‘
            for tree_data in self.project_audit.call_trees:
                if tree_data['function'] == func_name:
                    # å¤„ç†ä¸Šæ¸¸è°ƒç”¨æ ‘
                    if tree_data['upstream_tree']:
                        upstream_funcs, upstream_stats = get_functions_from_tree(tree_data['upstream_tree'])
                        all_related_functions.extend(upstream_funcs)
                        # åˆå¹¶ä¸Šæ¸¸ç»Ÿè®¡ä¿¡æ¯
                        for level, count in upstream_stats.items():
                            statistics['upstream_stats'][level] = (
                                statistics['upstream_stats'].get(level, 0) + count
                            )
                            
                    # å¤„ç†ä¸‹æ¸¸è°ƒç”¨æ ‘
                    if tree_data['downstream_tree']:
                        downstream_funcs, downstream_stats = get_functions_from_tree(tree_data['downstream_tree'])
                        all_related_functions.extend(downstream_funcs)
                        # åˆå¹¶ä¸‹æ¸¸ç»Ÿè®¡ä¿¡æ¯
                        for level, count in downstream_stats.items():
                            statistics['downstream_stats'][level] = (
                                statistics['downstream_stats'].get(level, 0) + count
                            )
                        
                    # æ·»åŠ åŸå§‹å‡½æ•°æœ¬èº«
                    for func in self.project_audit.functions_to_check:
                        if func['name'].split('.')[-1] == func_name:
                            all_related_functions.append(func)
                            break
                                
                    break
        
        # å¢å¼ºçš„å»é‡å¤„ç†
        for func in all_related_functions:
            # åˆ›å»ºä¸€ä¸ªæ›´ç²¾ç¡®çš„å”¯ä¸€æ ‡è¯†ç¬¦ï¼ŒåŒ…å«å‡½æ•°åå’Œå†…å®¹çš„hash
            func_identifier = f"{func['name']}_{hash(func['content'])}"
            if func_identifier not in seen_functions:
                seen_functions.add(func_identifier)
                unique_functions.append(func)
        
        # æ‹¼æ¥æ‰€æœ‰å‡½æ•°å†…å®¹ï¼ŒåŒ…æ‹¬çŠ¶æ€å˜é‡
        combined_text_parts = []
        for func in unique_functions:
            # æŸ¥æ‰¾å¯¹åº”çš„çŠ¶æ€å˜é‡
            state_vars = None
            for tree_data in self.project_audit.call_trees:
                if tree_data['function'] == func['name'].split('.')[-1]:
                    state_vars = tree_data.get('state_variables', '')
                    break
            
            # æ„å»ºå‡½æ•°æ–‡æœ¬ï¼ŒåŒ…å«çŠ¶æ€å˜é‡
            function_text = []
            if state_vars:
                function_text.append("// Contract State Variables:")
                function_text.append(state_vars)
                function_text.append("\n// Function Implementation:")
            function_text.append(func['content'])
            
            combined_text_parts.append('\n'.join(function_text))
        
        combined_text = '\n\n'.join(combined_text_parts)
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        print(f"\nFunction Call Tree Statistics:")
        print(f"Total Layers Analyzed: {level}")
        print("\nUpstream Statistics:")
        for layer, count in statistics['upstream_stats'].items():
            print(f"Layer {layer}: {count} functions")
        print("\nDownstream Statistics:")
        for layer, count in statistics['downstream_stats'].items():
            print(f"Layer {layer}: {count} functions")
        print(f"\nTotal Unique Functions: {len(unique_functions)}")
        
        return combined_text


    def check_function_vul(self):
        # self.llm.init_conversation()
        tasks = self.project_taskmgr.get_task_list()
        # ç”¨codebaseQAçš„å½¢å¼è¿›è¡Œï¼Œé¦–å…ˆé€šè¿‡ragå’Œtaskä¸­çš„vulè·å–ç›¸åº”çš„æ ¸å¿ƒä¸‰ä¸ªæœ€ç›¸å…³çš„å‡½æ•°
        for task in tqdm(tasks,desc="Processing tasks for update business_flow_context"):
            if task.score=="1":
                continue
            if task.if_business_flow_scan=="1":
                # è·å–business_flow_context
                code_to_be_tested=task.business_flow_code
            else:
                code_to_be_tested=task.content
            related_functions=self.get_related_functions(code_to_be_tested,5)
            related_functions_names=[func['name'].split('.')[-1] for func in related_functions]
            combined_text=self.extract_related_functions_by_level(related_functions_names,6)
            # æ›´æ–°taskå¯¹åº”çš„business_flow_context
            self.project_taskmgr.update_business_flow_context(task.id,combined_text)
            self.project_taskmgr.update_score(task.id,"1")
            

        if len(tasks) == 0:
            return

        # å®šä¹‰çº¿ç¨‹æ± ä¸­çš„çº¿ç¨‹æ•°é‡, ä»envè·å–
        max_threads = int(os.getenv("MAX_THREADS_OF_CONFIRMATION", 5))

        with ThreadPoolExecutor(max_workers=max_threads) as executor:
            futures = [executor.submit(self.process_task_check_vul, task) for task in tasks]

            with tqdm(total=len(tasks), desc="Checking vulnerabilities") as pbar:
                for future in as_completed(futures):
                    future.result()  # ç­‰å¾…æ¯ä¸ªä»»åŠ¡å®Œæˆ
                    pbar.update(1)  # æ›´æ–°è¿›åº¦æ¡

        return tasks

    def extract_required_info(self, claude_response):
        """Extract information that needs further investigation from Claude's response"""
        # prompt = """
        # Please extract all information points that need further understanding or confirmation from the following analysis response.
        # If the analysis explicitly states "no additional information needed" or similar, return empty.
        # If the analysis mentions needing more information, extract these information points.
        
        # Analysis response:
        # {response}
        # """
        prompt = CorePrompt.extract_required_info_prompt()
        
        extraction_result = ask_claude(prompt.format(response=claude_response))
        if not extraction_result or extraction_result.isspace():
            return []
        
        # If response contains negative phrases, return empty list
        if any(phrase in extraction_result.lower() for phrase in ["no need", "not needed", "no additional", "no more"]):
            return []
        
        return [extraction_result]

    def get_additional_context(self, query_contents):
        """è·å–é¢å¤–çš„ä¸Šä¸‹æ–‡ä¿¡æ¯"""
        if not query_contents:
            print("âŒ æ²¡æœ‰æŸ¥è¯¢å†…å®¹ï¼Œæ— æ³•è·å–é¢å¤–ä¸Šä¸‹æ–‡")
            return ""
        
        print(f"ğŸ” æ­£åœ¨æŸ¥è¯¢ {len(query_contents)} æ¡ç›¸å…³ä¿¡æ¯...")
        related_functions = []
        for query in query_contents:
            results = self.get_related_functions(query, k=10)
            if results:
                print(f"âœ… æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³å‡½æ•°")
                related_functions.extend(results)
            else:
                print("âš ï¸ æœªæ‰¾åˆ°ç›¸å…³å‡½æ•°")
        
        if related_functions:
            function_names = [func['name'].split('.')[-1] for func in related_functions]
            print(f"ğŸ“‘ æ­£åœ¨æå– {len(function_names)} ä¸ªå‡½æ•°çš„ä¸Šä¸‹æ–‡...")
            return self.extract_related_functions_by_level(function_names, 2)
        
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•ç›¸å…³å‡½æ•°")
        return ""

    def get_additional_internet_info(self, required_info):
        """åˆ¤æ–­æ˜¯å¦éœ€è¦è”ç½‘æœç´¢å¹¶è·å–ç½‘ç»œä¿¡æ¯
        
        Args:
            required_info: éœ€è¦è¿›ä¸€æ­¥è°ƒæŸ¥çš„ä¿¡æ¯åˆ—è¡¨
            
        Returns:
            str: æœç´¢è·å–çš„ç›¸å…³ä¿¡æ¯
        """
        # æ£€æŸ¥ç¯å¢ƒå˜é‡æ˜¯å¦å…è®¸ç½‘ç»œæœç´¢
        if os.getenv("ENABLE_INTERNET_SEARCH", "False").lower() != "True":
            print("âŒ ç½‘ç»œæœç´¢å·²ç¦ç”¨")
            return ""
        
        if not required_info:
            print("âŒ æ²¡æœ‰æŸ¥è¯¢å†…å®¹ï¼Œæ— æ³•è¿›è¡Œç½‘ç»œæœç´¢")
            return ""
        
        # æ„å»ºåˆ¤æ–­æ˜¯å¦éœ€è¦è”ç½‘æœç´¢çš„æç¤ºè¯
        # judge_prompt = """
        # Please analyze if the following information points require internet search to understand better.
        # The information might need internet search if it involves:
        # 1. Technical concepts or protocols that need explanation
        # 2. Specific vulnerabilities or CVEs
        # 3. Industry standards or best practices
        # 4. Historical incidents or known attack vectors
        
        # Return ONLY a JSON response in this exact format, with no additional text:
        # {{
        #     "needs_search": "yes/no",
        #     "reason": "brief explanation"
        # }}
        
        # Information to analyze:
        # {0}
        # """
        judge_prompt = CorePrompt.judge_prompt()
        
        # å°†æ‰€æœ‰required_infoåˆå¹¶æˆä¸€ä¸ªæŸ¥è¯¢æ–‡æœ¬
        combined_query = "\n".join(required_info)
        
        # è·å–åˆ¤æ–­ç»“æœ
        judge_response = ask_claude(judge_prompt.format(combined_query))
        print("\nğŸ” ç½‘ç»œæœç´¢éœ€æ±‚åˆ†æ:")
        print(judge_response)
        
        try:
            # å°è¯•æå–JSONéƒ¨åˆ† - åªåŒ¹é…ç¬¬ä¸€ä¸ªå®Œæ•´çš„JSONå¯¹è±¡
            import re
            # ä½¿ç”¨éè´ªå©ªåŒ¹é…æ¥è·å–ç¬¬ä¸€ä¸ªå®Œæ•´çš„JSONå¯¹è±¡
            json_match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', judge_response)
            if json_match:
                json_str = json_match.group(0)
                # æ¸…ç†å¯èƒ½çš„é¢å¤–å­—ç¬¦
                json_str = json_str.strip()
                judge_result = json.loads(json_str)
            else:
                raise json.JSONDecodeError("No JSON found in response", judge_response, 0)
                
            if judge_result.get("needs_search", "no").lower() == "yes":
                print(f"\nğŸŒ éœ€è¦ç½‘ç»œæœç´¢: {judge_result.get('reason', '')}")
                
                # ä½¿ç”¨ grok è¿›è¡Œæ·±åº¦æœç´¢
                search_results = ask_grok3_deepsearch(combined_query)
                if search_results:
                    print(f"\nâœ… è·å–åˆ°ç½‘ç»œæœç´¢ç»“æœ (é•¿åº¦: {len(search_results)} å­—ç¬¦)")
                    return search_results
                else:
                    print("\nâš ï¸ ç½‘ç»œæœç´¢æœªè¿”å›æœ‰æ•ˆç»“æœ")
                    return ""
            else:
                print(f"\nğŸ“ æ— éœ€ç½‘ç»œæœç´¢: {judge_result.get('reason', '')}")
                return ""
            
        except json.JSONDecodeError:
            print("\nâš ï¸ JSON è§£æé”™è¯¯ - è·³è¿‡ç½‘ç»œæœç´¢")
            return ""

if __name__ == "__main__":
    pass