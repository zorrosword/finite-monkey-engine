import argparse
import ast
import os
import time
import audit_config
from ai_engine import *
from project import ProjectAudit
from library.dataset_utils import load_dataset, Project
from planning import PlanningV2
from prompts import prompts
from sqlalchemy import create_engine
from dao import CacheManager, ProjectTaskMgr
import os
import pandas as pd
from openpyxl import Workbook,load_workbook
from openpyxl.utils.dataframe import dataframe_to_rows
from context import ContextFactory
from res_processor.res_processor import ResProcessor
# æ–°å¢å¯¼å…¥ code_summarizer
from code_summarizer import smart_business_flow_analysis_from_content

import dotenv
dotenv.load_dotenv()

def scan_project(project, db_engine):
    # 1. parsing projects  
    project_audit = ProjectAudit(project.id, project.path, db_engine)
    project_audit.parse(project.white_files, project.white_functions)
    
    # 1.5 build context factory and initialize rag processor
    context_factory = ContextFactory(project_audit)
    context_factory.initialize_rag_processor(
        project_audit.functions_to_check, 
        "./src/codebaseQA/lancedb", 
        project.id
    )
    
    # 1.6 ç”Ÿæˆ mmd æ–‡ä»¶ - æ–°å¢åŠŸèƒ½
    output_dir = f"src/codebaseQA/mermaid_output/{project.id}"
    
    # ğŸ†• æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨mmdæ–‡ä»¶
    print("ğŸ” æ£€æŸ¥mermaid_outputç›®å½•ä¸­æ˜¯å¦å·²å­˜åœ¨Mermaidæ–‡ä»¶...")
    existing_mmd_files = []
    if os.path.exists(output_dir):
        for file_name in os.listdir(output_dir):
            if file_name.endswith('.mmd') and project.id in file_name:
                existing_mmd_files.append(file_name)
    
    if existing_mmd_files:
        print(f"âœ… å‘ç°å·²å­˜åœ¨çš„Mermaidæ–‡ä»¶:")
        for mmd_file in existing_mmd_files:
            print(f"   - {mmd_file}")
        
        print("ğŸ¯ ä½¿ç”¨å·²å­˜åœ¨çš„Mermaidæ–‡ä»¶ï¼Œè·³è¿‡é‡æ–°ç”Ÿæˆ")
        
        # å°†ç°æœ‰çš„mermaidä¿¡æ¯ä¿å­˜åˆ°project_audit
        project_audit.mermaid_result = None  # æ ‡è®°ä¸ºä½¿ç”¨å·²å­˜åœ¨æ–‡ä»¶
        project_audit.mermaid_output_dir = output_dir
        
    else:
        print("ğŸ¨ æœªå‘ç°å·²å­˜åœ¨çš„Mermaidæ–‡ä»¶ï¼Œå¼€å§‹ç”Ÿæˆæ–°çš„ä¸šåŠ¡æµç¨‹å›¾...")
        try:
            # æ”¶é›†æ‰€æœ‰ä»£ç æ–‡ä»¶å†…å®¹
            files_content = {}
            for func in project_audit.functions_to_check:
                file_path = func['relative_file_path']
                if file_path not in files_content:
                    files_content[file_path] = func['contract_code']
            
            # ä½¿ç”¨æ™ºèƒ½åˆ†æç”Ÿæˆmermaidå›¾
            mermaid_result = smart_business_flow_analysis_from_content(
                files_content, 
                project.id,
                enable_reinforcement=True
            )
            
            # ä¿å­˜mermaidæ–‡ä»¶
            os.makedirs(output_dir, exist_ok=True)
            
            if mermaid_result.analysis_strategy == "folder_based":
                # å¤§é¡¹ç›®ï¼šä¿å­˜å¤šä¸ªæ–‡ä»¶å¤¹çº§åˆ«çš„mermaidå›¾
                for folder_path, folder_result in mermaid_result.folder_analyses.items():
                    folder_name = folder_path.replace('/', '_').replace('\\', '_')
                    mermaid_file = f"{output_dir}/{project.id}_{folder_name}.mmd"
                    with open(mermaid_file, 'w', encoding='utf-8') as f:
                        f.write(folder_result.folder_mermaid_graph)
                    print(f"âœ… ä¿å­˜æ–‡ä»¶å¤¹çº§åˆ«Mermaidå›¾: {mermaid_file}")
                
                # ä¿å­˜å…¨å±€æ¦‚è§ˆå›¾
                global_mermaid_file = f"{output_dir}/{project.id}_global_overview.mmd"
                with open(global_mermaid_file, 'w', encoding='utf-8') as f:
                    f.write(mermaid_result.global_mermaid_graph)
                print(f"âœ… ä¿å­˜å…¨å±€æ¦‚è§ˆMermaidå›¾: {global_mermaid_file}")
            else:
                # å°é¡¹ç›®ï¼šä¿å­˜å•ä¸ªmermaidå›¾
                mermaid_file = f"{output_dir}/{project.id}_business_flow.mmd"
                with open(mermaid_file, 'w', encoding='utf-8') as f:
                    f.write(mermaid_result.final_mermaid_graph)
                print(f"âœ… ä¿å­˜ä¸šåŠ¡æµç¨‹Mermaidå›¾: {mermaid_file}")
            
            # å°†mermaidç»“æœä¿å­˜åˆ°project_auditä»¥ä¾›åç»­ä½¿ç”¨
            project_audit.mermaid_result = mermaid_result
            project_audit.mermaid_output_dir = output_dir
            
            print("ğŸ¨ Mermaidä¸šåŠ¡æµç¨‹å›¾ç”Ÿæˆå®Œæˆï¼")
            
        except Exception as e:
            print(f"âŒ ç”ŸæˆMermaidå›¾æ—¶å‡ºé”™: {str(e)}")
            # å³ä½¿mermaidç”Ÿæˆå¤±è´¥ï¼Œä¹Ÿç»§ç»­åç»­æµç¨‹
            project_audit.mermaid_result = None
            project_audit.mermaid_output_dir = None
    
    # 2. planning & scanning
    project_taskmgr = ProjectTaskMgr(project.id, db_engine) 
    
    planning = PlanningV2(project_audit, project_taskmgr)
    # 
    engine = AiEngine(planning, project_taskmgr, context_factory.rag_processor.db, "lancedb_"+project.id, project_audit)
    # 1. æ‰«æ 
    engine.do_planning()
    engine.do_scan()

    return context_factory.rag_processor.db, context_factory.rag_processor.table_name, project_audit
    
    # 2. gpt4 å¯¹ç»“æœåšrescan 
    # rescan_project_with_gpt4(project.id, db_engine)

def check_function_vul(engine,lancedb,lance_table_name,project_audit):
    project_taskmgr = ProjectTaskMgr(project.id, engine)
    engine = AiEngine(None, project_taskmgr,lancedb,lance_table_name,project_audit)
    engine.check_function_vul()
    # print(result)

def generate_excel(output_path, project_id):
    project_taskmgr = ProjectTaskMgr(project_id, engine)
    entities = project_taskmgr.query_task_by_project_id(project.id)
    
    # åˆ›å»ºä¸€ä¸ªç©ºçš„DataFrameæ¥å­˜å‚¨æ‰€æœ‰å®ä½“çš„æ•°æ®
    data = []
    for entity in entities:
        if ("yes" in str(entity.result_gpt4).lower() 
            # or "not sure" in str(entity.result_gpt4).lower()
            ) and len(entity.business_flow_code)>0:
            data.append({
                'æ¼æ´ç»“æœ': entity.result,
                'ID': entity.id,
                'é¡¹ç›®åç§°': entity.project_id,
                'åˆåŒç¼–å·': entity.contract_code,
                'UUID': entity.key,
                'å‡½æ•°åç§°': entity.name,
                'å‡½æ•°ä»£ç ': entity.content,
                'å¼€å§‹è¡Œ': entity.start_line,
                'ç»“æŸè¡Œ': entity.end_line,
                'ç›¸å¯¹è·¯å¾„': entity.relative_file_path,
                'ç»å¯¹è·¯å¾„': entity.absolute_file_path,
                'ä¸šåŠ¡æµç¨‹ä»£ç ': entity.business_flow_code,
                'ä¸šåŠ¡æµç¨‹è¡Œ': entity.business_flow_lines,
                'ä¸šåŠ¡æµç¨‹ä¸Šä¸‹æ–‡': entity.business_flow_context,
                'ç¡®è®¤ç»“æœ': entity.result_gpt4,
                'ç¡®è®¤ç»†èŠ‚': entity.category
            })
    
    # å°†æ•°æ®è½¬æ¢ä¸ºDataFrame
    if not data:  # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®
        print("No data to process")
        return
        
    df = pd.DataFrame(data)
    
    try:
        # å¯¹dfè¿›è¡Œæ¼æ´å½’é›†å¤„ç†
        res_processor = ResProcessor(df,max_group_size=10,iteration_rounds=5,enable_chinese_translation=True)
        processed_df = res_processor.process()
        
        # ç¡®ä¿æ‰€æœ‰å¿…éœ€çš„åˆ—éƒ½å­˜åœ¨
        required_columns = df.columns
        for col in required_columns:
            if col not in processed_df.columns:
                processed_df[col] = ''
                
        # é‡æ–°æ’åˆ—åˆ—é¡ºåºä»¥åŒ¹é…åŸå§‹DataFrame
        processed_df = processed_df[df.columns]
    except Exception as e:
        print(f"Error processing data: {e}")
        processed_df = df  # å¦‚æœå¤„ç†å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹DataFrame
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_dir = os.path.dirname(output_path)
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»ºæ–°æ–‡ä»¶
    if not os.path.exists(output_path):
        wb = Workbook()
        ws = wb.active
        ws.title = "é¡¹ç›®æ•°æ®"
    else:
        wb = load_workbook(output_path)
        if "é¡¹ç›®æ•°æ®" in wb.sheetnames:
            ws = wb["é¡¹ç›®æ•°æ®"]
        else:
            ws = wb.create_sheet("é¡¹ç›®æ•°æ®")
    
    # å¦‚æœå·¥ä½œè¡¨æ˜¯ç©ºçš„ï¼Œæ·»åŠ è¡¨å¤´
    if ws.max_row == 1:
        for col, header in enumerate(processed_df.columns, start=1):
            ws.cell(row=1, column=col, value=header)
    
    # å°†DataFrameæ•°æ®å†™å…¥å·¥ä½œè¡¨
    for row in dataframe_to_rows(processed_df, index=False, header=False):
        ws.append(row)
    
    # ä¿å­˜Excelæ–‡ä»¶
    wb.save(output_path)
    
    print(f"Excelæ–‡ä»¶å·²ä¿å­˜åˆ°: {output_path}")
if __name__ == '__main__':

    switch_production_or_test = 'test' # prod / test

    if switch_production_or_test == 'test':
        start_time=time.time()
        db_url_from = os.environ.get("DATABASE_URL")
        engine = create_engine(db_url_from)
        
        dataset_base = "./src/dataset/agent-v1-c4"
        projects = load_dataset(dataset_base)
 
        project_id = 'chainlink072111'
        project_path = ''
        project = Project(project_id, projects[project_id])
        
        cmd = 'detect_vul'
        if cmd == 'detect_vul':
            lancedb,lance_table_name,project_audit=scan_project(project, engine) # scan
            if os.getenv("SCAN_MODE","SPECIFIC_PROJECT") in [
                "SPECIFIC_PROJECT",
                "COMMON_PROJECT",
                "PURE_SCAN",
                "CHECKLIST",
                "CHECKLIST_PIPELINE",
                "COMMON_PROJECT_FINE_GRAINED"]:
                check_function_vul(engine,lancedb,lance_table_name,project_audit) # confirm



        end_time=time.time()
        print("Total time:",end_time-start_time)
        generate_excel("./output.xlsx",project_id)
        
        
    if switch_production_or_test == 'prod':
        # Set up command line argument parsing
        parser = argparse.ArgumentParser(description='Process input parameters for vulnerability scanning.')
        parser.add_argument('-fpath', type=str, required=True, help='Combined base path for the dataset and folder')
        parser.add_argument('-id', type=str, required=True, help='Project ID')
        # parser.add_argument('-cmd', type=str, choices=['detect', 'confirm','all'], required=True, help='Command to execute')
        parser.add_argument('-o', type=str, required=True, help='Output file path')
        # usage:
        # python main.py 
        # --fpath ../../dataset/agent-v1-c4/Archive 
        # --id Archive_aaa 
        # --cmd detect

        # Parse arguments
        args = parser.parse_args()
        print("fpath:",args.fpath)
        print("id:",args.id)
        print("cmd:",args.cmd)
        print("o:",args.o)
        # Split dataset_folder into dataset and folder
        dataset_base, folder_name = os.path.split(args.fpath)
        print("dataset_base:",dataset_base)
        print("folder_name:",folder_name)
        # Start time
        start_time = time.time()

        # Database setup
        db_url_from = os.environ.get("DATABASE_URL")
        engine = create_engine(db_url_from)

        # Load projects
        projects = load_dataset(dataset_base, args.id, folder_name)
        project = Project(args.id, projects[args.id])

        # Execute command
        # if args.cmd == 'detect':
        #     scan_project(project, engine)  # scan            
        # elif args.cmd == 'confirm':
        #     check_function_vul(engine)  # confirm
        # elif args.cmd == 'all':
        lancedb=scan_project(project, engine)  # scan
        check_function_vul(engine,lancedb)  # confirm

        end_time = time.time()
        print("Total time:", end_time -start_time)
        generate_excel(args.o,args.id)




