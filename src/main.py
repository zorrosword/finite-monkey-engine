import argparse
import ast
import os
import sys
import time
from ai_engine import *
from tree_sitter_parsing import TreeSitterProjectAudit as ProjectAudit
from dataset_manager import load_dataset, Project
from planning.planning import Planning
from sqlalchemy import create_engine
from dao import CacheManager, ProjectTaskMgr
import os
import pandas as pd
from openpyxl import Workbook,load_workbook
from openpyxl.utils.dataframe import dataframe_to_rows
from res_processor.res_processor import ResProcessor

import dotenv
dotenv.load_dotenv()

# æ·»åŠ æ—¥å¿—ç³»ç»Ÿ
from logging_config import setup_logging, get_logger, log_section_start, log_section_end, log_step, log_error, log_warning, log_success, log_data_info

def _perform_post_reasoning_deduplication(project_id, db_engine, logger):
    """åœ¨reasoningå®Œæˆåï¼Œvalidationå¼€å§‹å‰è¿›è¡Œå»é‡å¤„ç†"""
    log_step(logger, "å¼€å§‹è·å–reasoningåçš„æ¼æ´æ•°æ®")
    
    try:
        # è·å–reasoningåçš„æ‰€æœ‰æ¼æ´æ•°æ®
        project_taskmgr = ProjectTaskMgr(project_id, db_engine)
        entities = project_taskmgr.query_task_by_project_id(project_id)
        
        # è°ƒè¯•ä¿¡æ¯ï¼šç»Ÿè®¡æ‰€æœ‰å®ä½“
        total_entities = len(entities)
        log_data_info(logger, "æ€»ä»»åŠ¡å®ä½“æ•°é‡", total_entities)
        print(f"ğŸ” è°ƒè¯•ä¿¡æ¯ - æ€»ä»»åŠ¡å®ä½“æ•°é‡: {total_entities}")
        
        # è¯¦ç»†åˆ†ææ¯ä¸ªç­›é€‰æ¡ä»¶
        entities_with_result = 0
        entities_with_yes = 0
        entities_with_business_code = 0
        
        for entity in entities:
            if entity.result:
                entities_with_result += 1
                if "yes" in str(entity.result).lower():
                    entities_with_yes += 1
                if hasattr(entity, 'business_flow_code') and entity.business_flow_code and len(entity.business_flow_code) > 0:
                    entities_with_business_code += 1
        
        print(f"ğŸ” è°ƒè¯•ä¿¡æ¯ - æœ‰resultçš„å®ä½“: {entities_with_result}")
        print(f"ğŸ” è°ƒè¯•ä¿¡æ¯ - resultåŒ…å«'yes'çš„å®ä½“: {entities_with_yes}")
        print(f"ğŸ” è°ƒè¯•ä¿¡æ¯ - æœ‰business_flow_codeçš„å®ä½“: {entities_with_business_code}")
        
        # ç­›é€‰æœ‰æ¼æ´ç»“æœçš„æ•°æ®
        vulnerability_data = []
        for entity in entities:
            # è°ƒè¯•æ¯ä¸ªå®ä½“çš„è¯¦ç»†ä¿¡æ¯
            has_result = bool(entity.result)
            has_yes = has_result and ("yes" in str(entity.result).lower())
            has_business_code = hasattr(entity, 'business_flow_code') and entity.business_flow_code and len(entity.business_flow_code) > 0
            
            if has_result and has_yes and has_business_code:
                vulnerability_data.append({
                    'æ¼æ´ç»“æœ': entity.result,
                    'ID': entity.id,
                    'é¡¹ç›®åç§°': entity.project_id,
                    'åˆåŒç¼–å·': entity.contract_code,
                    'UUID': entity.uuid,
                    'å‡½æ•°åç§°': entity.name,
                    'å‡½æ•°ä»£ç ': entity.content,
                    'è§„åˆ™ç±»å‹': entity.rule_key,
                    'å¼€å§‹è¡Œ': entity.start_line,
                    'ç»“æŸè¡Œ': entity.end_line,
                    'ç›¸å¯¹è·¯å¾„': entity.relative_file_path,
                    'ç»å¯¹è·¯å¾„': entity.absolute_file_path,
                    'ä¸šåŠ¡æµç¨‹ä»£ç ': entity.business_flow_code,
                    'æ‰«æè®°å½•': entity.scan_record,
                    'æ¨è': entity.recommendation
                })
        
        filtered_count = len(vulnerability_data)
        print(f"ğŸ” è°ƒè¯•ä¿¡æ¯ - é€šè¿‡ç­›é€‰æ¡ä»¶çš„å®ä½“: {filtered_count}")
        
        if not vulnerability_data:
            print(f"âš ï¸  ä¸¥æ ¼ç­›é€‰æ¡ä»¶æœªæ‰¾åˆ°æ•°æ®ï¼Œå°è¯•å®½æ¾ç­›é€‰æ¡ä»¶...")
            print(f"   - æ€»å®ä½“æ•°: {total_entities}")
            print(f"   - æœ‰resultçš„: {entities_with_result}")
            print(f"   - resultåŒ…å«'yes'çš„: {entities_with_yes}")
            print(f"   - æœ‰business_flow_codeçš„: {entities_with_business_code}")
            print(f"   - é€šè¿‡æ‰€æœ‰ç­›é€‰æ¡ä»¶çš„: {filtered_count}")
            
            # å°è¯•å®½æ¾ç­›é€‰æ¡ä»¶ï¼šåªè¦æœ‰resultå°±è¿›è¡Œå»é‡
            print(f"ğŸ”„ å°è¯•å®½æ¾ç­›é€‰æ¡ä»¶ï¼ˆåªè¦æœ‰resultï¼‰...")
            for entity in entities:
                if entity.result and entity.result.strip():  # åªè¦æœ‰éç©ºresult
                    vulnerability_data.append({
                        'æ¼æ´ç»“æœ': entity.result,
                        'ID': entity.id,
                        'é¡¹ç›®åç§°': entity.project_id,
                        'åˆåŒç¼–å·': getattr(entity, 'contract_code', ''),
                        'UUID': getattr(entity, 'uuid', ''),
                        'å‡½æ•°åç§°': entity.name,
                        'å‡½æ•°ä»£ç ': getattr(entity, 'content', ''),
                        'è§„åˆ™ç±»å‹': getattr(entity, 'rule_key', ''),
                        'å¼€å§‹è¡Œ': getattr(entity, 'start_line', ''),
                        'ç»“æŸè¡Œ': getattr(entity, 'end_line', ''),
                        'ç›¸å¯¹è·¯å¾„': getattr(entity, 'relative_file_path', ''),
                        'ç»å¯¹è·¯å¾„': getattr(entity, 'absolute_file_path', ''),
                        'ä¸šåŠ¡æµç¨‹ä»£ç ': getattr(entity, 'business_flow_code', ''),
                        'æ‰«æè®°å½•': getattr(entity, 'scan_record', ''),
                        'æ¨è': getattr(entity, 'recommendation', '')
                    })
            
            fallback_count = len(vulnerability_data)
            print(f"ğŸ” å®½æ¾ç­›é€‰æ¡ä»¶æ‰¾åˆ°: {fallback_count} ä¸ªå®ä½“")
            
            if not vulnerability_data:
                print(f"âŒ å³ä½¿ä½¿ç”¨å®½æ¾ç­›é€‰æ¡ä»¶ä¹Ÿæœªæ‰¾åˆ°æ•°æ®ï¼Œè·³è¿‡å»é‡å¤„ç†")
                log_warning(logger, f"ä¸¥æ ¼å’Œå®½æ¾ç­›é€‰æ¡ä»¶éƒ½æœªæ‰¾åˆ°æ•°æ® - æ€»å®ä½“:{total_entities}, æœ‰result:{entities_with_result}")
                return
            else:
                print(f"âœ… ä½¿ç”¨å®½æ¾ç­›é€‰æ¡ä»¶è¿›è¡Œå»é‡å¤„ç†")
                log_warning(logger, f"ä½¿ç”¨å®½æ¾ç­›é€‰æ¡ä»¶è¿›è¡Œå»é‡ - åŸå§‹æ¡ä»¶ç­›é€‰å‡º:{filtered_count}, å®½æ¾æ¡ä»¶ç­›é€‰å‡º:{fallback_count}")
        
        original_df = pd.DataFrame(vulnerability_data)
        original_count = len(original_df)
        original_ids = set(original_df['ID'].astype(str))
        
        log_data_info(logger, "å»é‡å‰æ¼æ´æ•°é‡", original_count)
        log_data_info(logger, "å»é‡å‰æ¼æ´ID", f"{', '.join(sorted(original_ids))}")
        
        # ä½¿ç”¨ResProcessorè¿›è¡Œå»é‡
        log_step(logger, "å¼€å§‹ResProcessorå»é‡å¤„ç†")
        res_processor = ResProcessor(original_df, max_group_size=5, iteration_rounds=4, enable_chinese_translation=False)
        processed_df = res_processor.process()
        
        deduplicated_count = len(processed_df)
        deduplicated_ids = set(processed_df['ID'].astype(str))
        
        log_data_info(logger, "å»é‡åæ¼æ´æ•°é‡", deduplicated_count)
        log_data_info(logger, "å»é‡åæ¼æ´ID", f"{', '.join(sorted(deduplicated_ids))}")
        
        # è®¡ç®—è¢«å»é‡çš„ID
        removed_ids = original_ids - deduplicated_ids
        removed_count = len(removed_ids)
        
        # æ‰“å°å»é‡ç»“æœ
        print(f"\n{'='*60}")
        print(f"ğŸ”„ Reasoningåå»é‡å¤„ç†ç»“æœ")
        print(f"{'='*60}")
        print(f"å»é‡å‰æ¼æ´æ•°é‡: {original_count}")
        print(f"å»é‡åæ¼æ´æ•°é‡: {deduplicated_count}")
        print(f"è¢«å»é‡çš„æ¼æ´æ•°é‡: {removed_count}")
        
        if removed_ids:
            print(f"\nğŸ—‘ï¸  è¢«å»é‡çš„æ¼æ´IDåˆ—è¡¨:")
            for i, removed_id in enumerate(sorted(removed_ids), 1):
                print(f"  {i:2d}. ID: {removed_id}")
            
            # é€»è¾‘åˆ é™¤è¢«å»é‡çš„è®°å½• - å°†short_resultè®¾ç½®ä¸º"delete"
            print(f"\nğŸ—‘ï¸  å¼€å§‹é€»è¾‘åˆ é™¤è¢«å»é‡çš„è®°å½•(è®¾ç½®short_result='delete')...")
            marked_count = 0
            failed_marks = []
            
            for removed_id in removed_ids:
                try:
                    # è½¬æ¢ä¸ºæ•´æ•°ç±»å‹çš„ID
                    id_int = int(removed_id)
                    project_taskmgr.update_short_result(id_int, "delete")
                    marked_count += 1
                    print(f"    âœ… æ ‡è®°æˆåŠŸ: ID {removed_id} -> short_result='delete'")
                except Exception as e:
                    failed_marks.append(removed_id)
                    print(f"    âŒ æ ‡è®°å‡ºé”™: ID {removed_id}, é”™è¯¯: {str(e)}")
                    logger.error(f"æ ‡è®°åˆ é™¤ID {removed_id} æ—¶å‡ºé”™: {str(e)}")
            
            print(f"\nğŸ“Š é€»è¾‘åˆ é™¤ç»“æœ:")
            print(f"    æˆåŠŸæ ‡è®°: {marked_count} æ¡è®°å½•")
            if failed_marks:
                print(f"    æ ‡è®°å¤±è´¥: {len(failed_marks)} æ¡è®°å½• - IDs: {', '.join(failed_marks)}")
                logger.warning(f"æ ‡è®°å¤±è´¥çš„IDs: {', '.join(failed_marks)}")
            
            log_success(logger, "é€»è¾‘åˆ é™¤æ“ä½œå®Œæˆ", f"æˆåŠŸæ ‡è®°: {marked_count}/{removed_count}")
        else:
            print("âœ… æ²¡æœ‰æ¼æ´è¢«å»é‡")
        
        print(f"{'='*60}\n")
        
        # è®°å½•åˆ°æ—¥å¿—
        log_success(logger, "å»é‡å¤„ç†å®Œæˆ", f"åŸå§‹: {original_count} -> å»é‡å: {deduplicated_count}, é€»è¾‘åˆ é™¤: {removed_count}")
        if removed_ids:
            logger.info(f"è¢«å»é‡çš„æ¼æ´ID: {', '.join(sorted(removed_ids))}")
            logger.info(f"é€»è¾‘åˆ é™¤äº† {marked_count} æ¡è¢«å»é‡çš„è®°å½•(è®¾ç½®short_result='delete')")
        
    except Exception as e:
        log_error(logger, "å»é‡å¤„ç†å¤±è´¥", e)
        import traceback
        logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")

def scan_project(project, db_engine):
    logger = get_logger("scan_project")
    scan_start_time = time.time()
    
    log_section_start(logger, "é¡¹ç›®æ‰«æ", f"é¡¹ç›®ID: {project.id}, è·¯å¾„: {project.path}")
    
    # 1. parsing projects  
    log_step(logger, "Tree-sitterè§£æé¡¹ç›®", f"é¡¹ç›®è·¯å¾„: {project.path}")
    parsing_start = time.time()
    
    project_audit = ProjectAudit(project.id, project.path, db_engine)
    project_audit.parse()
    
    parsing_duration = time.time() - parsing_start
    log_success(logger, "é¡¹ç›®è§£æå®Œæˆ", f"è€—æ—¶: {parsing_duration:.2f}ç§’")
    log_data_info(logger, "è§£æçš„å‡½æ•°", len(project_audit.functions_to_check))
    log_data_info(logger, "è°ƒç”¨æ ‘", len(project_audit.call_trees))
    log_data_info(logger, "è°ƒç”¨å›¾", len(project_audit.call_graphs))
    
    # 1.5 åˆå§‹åŒ–RAGå¤„ç†å™¨ï¼ˆå¯é€‰ï¼‰
    log_step(logger, "åˆå§‹åŒ–RAGå¤„ç†å™¨")
    rag_processor = None
    try:
        from context.rag_processor import RAGProcessor
        rag_start = time.time()
        
        # ä¼ é€’project_auditå¯¹è±¡ï¼ŒåŒ…å«functions, functions_to_check, chunks
        rag_processor = RAGProcessor(
            project_audit, 
            "./src/codebaseQA/lancedb", 
            project.id
        )
        
        rag_duration = time.time() - rag_start
        log_success(logger, "RAGå¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ", f"è€—æ—¶: {rag_duration:.2f}ç§’")
        log_data_info(logger, "åŸºäºtree-sitterè§£æçš„å‡½æ•°æ„å»ºRAG", len(project_audit.functions_to_check))
        log_data_info(logger, "åŸºäºæ–‡æ¡£åˆ†å—æ„å»ºRAG", len(project_audit.chunks))
        log_data_info(logger, "ä½¿ç”¨è°ƒç”¨æ ‘æ„å»ºå…³ç³»å‹RAG", len(project_audit.call_trees))
        log_data_info(logger, "é›†æˆè°ƒç”¨å›¾(Call Graph)", len(project_audit.call_graphs))
        
        # æ˜¾ç¤º call graph ç»Ÿè®¡ä¿¡æ¯
        if project_audit.call_graphs:
            call_graph_stats = project_audit.get_call_graph_statistics()
            log_data_info(logger, "Call Graphç»Ÿè®¡", call_graph_stats)
        
    except ImportError as e:
        log_warning(logger, "RAGå¤„ç†å™¨ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨ç®€åŒ–åŠŸèƒ½")
        logger.debug(f"ImportErrorè¯¦æƒ…: {e}")
    except Exception as e:
        log_error(logger, "RAGå¤„ç†å™¨åˆå§‹åŒ–å¤±è´¥", e)
        rag_processor = None
    
    # 1.6 æ£€æŸ¥ä¸šåŠ¡æµæ¨¡å¼é…ç½®
    log_step(logger, "æ£€æŸ¥ä¸šåŠ¡æµæ¨¡å¼é…ç½®")
    switch_business_code = eval(os.environ.get('SWITCH_BUSINESS_CODE', 'True'))
    logger.info(f"SWITCH_BUSINESS_CODE: {switch_business_code}")
    
    if switch_business_code:
        log_step(logger, "å¯ç”¨ä¸šåŠ¡ä»£ç æ‰«ææ¨¡å¼")
    else:
        log_step(logger, "ä½¿ç”¨ä¼ ç»Ÿæ‰«ææ¨¡å¼", "SWITCH_BUSINESS_CODE=False")
    
    # 2. planning & scanning - ç›´æ¥ä½¿ç”¨project_audit
    log_step(logger, "åˆ›å»ºä»»åŠ¡ç®¡ç†å™¨")
    project_taskmgr = ProjectTaskMgr(project.id, db_engine) 
    log_success(logger, "ä»»åŠ¡ç®¡ç†å™¨åˆ›å»ºå®Œæˆ")
    
    # åˆ›å»ºè§„åˆ’å¤„ç†å™¨ï¼Œç›´æ¥ä¼ é€’project_audit
    log_step(logger, "åˆ›å»ºè§„åˆ’å¤„ç†å™¨")
    planning = Planning(project_audit, project_taskmgr)
    log_success(logger, "è§„åˆ’å¤„ç†å™¨åˆ›å»ºå®Œæˆ")
    
    # å¦‚æœæœ‰RAGå¤„ç†å™¨ï¼Œåˆå§‹åŒ–planningçš„RAGåŠŸèƒ½
    if rag_processor:
        log_step(logger, "åˆå§‹åŒ–è§„åˆ’å™¨çš„RAGåŠŸèƒ½")
        planning.initialize_rag_processor("./src/codebaseQA/lancedb", project.id)
        log_success(logger, "è§„åˆ’å™¨RAGåŠŸèƒ½åˆå§‹åŒ–å®Œæˆ")
    
    # åˆ›å»ºAIå¼•æ“
    log_step(logger, "åˆ›å»ºAIå¼•æ“")
    lancedb_table = rag_processor.db if rag_processor else None
    lancedb_table_name = rag_processor.table_name if rag_processor else f"lancedb_{project.id}"
    logger.info(f"LanceDBè¡¨å: {lancedb_table_name}")
    
    engine = AiEngine(planning, project_taskmgr, lancedb_table, lancedb_table_name, project_audit)
    log_success(logger, "AIå¼•æ“åˆ›å»ºå®Œæˆ")
    
    # æ‰§è¡Œè§„åˆ’å’Œæ‰«æ
    log_step(logger, "æ‰§è¡Œé¡¹ç›®è§„åˆ’")
    planning_start = time.time()
    engine.do_planning()
    planning_duration = time.time() - planning_start
    log_success(logger, "é¡¹ç›®è§„åˆ’å®Œæˆ", f"è€—æ—¶: {planning_duration:.2f}ç§’")
    
    log_step(logger, "æ‰§è¡Œæ¼æ´æ‰«æ(Reasoning)")
    scan_start = time.time()
    engine.do_scan()
    scan_duration = time.time() - scan_start
    log_success(logger, "æ¼æ´æ‰«æ(Reasoning)å®Œæˆ", f"è€—æ—¶: {scan_duration:.2f}ç§’")
    
    # åœ¨reasoningå®Œæˆåï¼Œvalidationå¼€å§‹å‰è¿›è¡Œå»é‡
    log_step(logger, "Reasoningåå»é‡å¤„ç†")
    dedup_start = time.time()
    _perform_post_reasoning_deduplication(project.id, db_engine, logger)
    dedup_duration = time.time() - dedup_start
    log_success(logger, "Reasoningåå»é‡å¤„ç†å®Œæˆ", f"è€—æ—¶: {dedup_duration:.2f}ç§’")
    
    total_scan_duration = time.time() - scan_start_time
    log_section_end(logger, "é¡¹ç›®æ‰«æ", total_scan_duration)

    return lancedb_table, lancedb_table_name, project_audit

def check_function_vul(engine, lancedb, lance_table_name, project_audit):
    """æ‰§è¡Œæ¼æ´æ£€æŸ¥ï¼Œç›´æ¥ä½¿ç”¨project_auditæ•°æ®"""
    logger = get_logger("check_function_vul")
    check_start_time = time.time()
    
    log_section_start(logger, "æ¼æ´éªŒè¯", f"é¡¹ç›®ID: {project_audit.project_id}")
    
    log_step(logger, "åˆ›å»ºé¡¹ç›®ä»»åŠ¡ç®¡ç†å™¨")
    project_taskmgr = ProjectTaskMgr(project_audit.project_id, engine)
    log_success(logger, "é¡¹ç›®ä»»åŠ¡ç®¡ç†å™¨åˆ›å»ºå®Œæˆ")
    
    # ç›´æ¥ä½¿ç”¨project_auditåˆ›å»ºæ¼æ´æ£€æŸ¥å™¨
    log_step(logger, "åˆå§‹åŒ–æ¼æ´æ£€æŸ¥å™¨")
    from validating import VulnerabilityChecker
    checker = VulnerabilityChecker(project_audit, lancedb, lance_table_name)
    log_success(logger, "æ¼æ´æ£€æŸ¥å™¨åˆå§‹åŒ–å®Œæˆ")
    
    # æ‰§è¡Œæ¼æ´æ£€æŸ¥
    log_step(logger, "æ‰§è¡Œæ¼æ´éªŒè¯")
    validation_start = time.time()
    checker.check_function_vul(project_taskmgr)
    validation_duration = time.time() - validation_start
    log_success(logger, "æ¼æ´éªŒè¯å®Œæˆ", f"è€—æ—¶: {validation_duration:.2f}ç§’")
    
    total_check_duration = time.time() - check_start_time
    log_section_end(logger, "æ¼æ´éªŒè¯", total_check_duration)

def generate_excel(output_path, project_id):
    project_taskmgr = ProjectTaskMgr(project_id, engine)
    entities = project_taskmgr.query_task_by_project_id(project.id)
    
    # åˆ›å»ºä¸€ä¸ªç©ºçš„DataFrameæ¥å­˜å‚¨æ‰€æœ‰å®ä½“çš„æ•°æ®
    data = []
    total_entities = len(entities)
    deleted_entities = 0
    
    for entity in entities:
        # è·³è¿‡å·²é€»è¾‘åˆ é™¤çš„è®°å½•
        if getattr(entity, 'short_result', '') == 'delete':
            deleted_entities += 1
            continue
            
        # ä½¿ç”¨resultå­—æ®µå’Œbusiness_flow_codeè¿›è¡Œç­›é€‰
        if entity.result and ("yes" in str(entity.result).lower()) and len(entity.business_flow_code)>0:
            data.append({
                'æ¼æ´ç»“æœ': entity.result,
                'ID': entity.id,
                'é¡¹ç›®åç§°': entity.project_id,
                'åˆåŒç¼–å·': entity.contract_code,
                'UUID': entity.uuid,  # ä½¿ç”¨uuidè€Œä¸æ˜¯key
                'å‡½æ•°åç§°': entity.name,
                'å‡½æ•°ä»£ç ': entity.content,
                'è§„åˆ™ç±»å‹': entity.rule_key,  # æ–°å¢rule_key
                'å¼€å§‹è¡Œ': entity.start_line,
                'ç»“æŸè¡Œ': entity.end_line,
                'ç›¸å¯¹è·¯å¾„': entity.relative_file_path,
                'ç»å¯¹è·¯å¾„': entity.absolute_file_path,
                'ä¸šåŠ¡æµç¨‹ä»£ç ': entity.business_flow_code,
                'æ‰«æè®°å½•': entity.scan_record,  # ä½¿ç”¨æ–°çš„scan_recordå­—æ®µ
                'æ¨è': entity.recommendation
            })
    
    # æ‰“å°æ•°æ®ç»Ÿè®¡ä¿¡æ¯
    print(f"\nğŸ“Š ExcelæŠ¥å‘Šæ•°æ®ç»Ÿè®¡:")
    print(f"   æ€»è®°å½•æ•°: {total_entities}")
    print(f"   é€»è¾‘åˆ é™¤çš„è®°å½•æ•°: {deleted_entities}")
    print(f"   æœ‰æ•ˆè®°å½•æ•°: {total_entities - deleted_entities}")
    print(f"   ç¬¦åˆæ¡ä»¶çš„æ¼æ´è®°å½•æ•°: {len(data)}")
    
    # å°†æ•°æ®è½¬æ¢ä¸ºDataFrame
    if not data:  # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®
        print("No data to process")
        return
        
    df = pd.DataFrame(data)
    
    try:
        # å¯¹dfè¿›è¡Œæ¼æ´å½’é›†å¤„ç†
        res_processor = ResProcessor(df,max_group_size=10,iteration_rounds=5,enable_chinese_translation=True)
        processed_df = res_processor.process()
        
        # ç¡®ä¿æ‰€æœ‰å¿…éœ€çš„åˆ—éƒ½å­˜åœ¨
        required_columns = df.columns
        for col in required_columns:
            if col not in processed_df.columns:
                processed_df[col] = ''
                
        # é‡æ–°æ’åˆ—åˆ—é¡ºåºä»¥åŒ¹é…åŸå§‹DataFrame
        processed_df = processed_df[df.columns]
    except Exception as e:
        print(f"Error processing data: {e}")
        processed_df = df  # å¦‚æœå¤„ç†å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹DataFrame
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_dir = os.path.dirname(output_path)
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»ºæ–°æ–‡ä»¶
    if not os.path.exists(output_path):
        wb = Workbook()
        ws = wb.active
        ws.title = "é¡¹ç›®æ•°æ®"
    else:
        wb = load_workbook(output_path)
        if "é¡¹ç›®æ•°æ®" in wb.sheetnames:
            ws = wb["é¡¹ç›®æ•°æ®"]
        else:
            ws = wb.create_sheet("é¡¹ç›®æ•°æ®")
    
    # å¦‚æœå·¥ä½œè¡¨æ˜¯ç©ºçš„ï¼Œæ·»åŠ è¡¨å¤´
    if ws.max_row == 1:
        for col, header in enumerate(processed_df.columns, start=1):
            ws.cell(row=1, column=col, value=header)
    
    # å°†DataFrameæ•°æ®å†™å…¥å·¥ä½œè¡¨
    for row in dataframe_to_rows(processed_df, index=False, header=False):
        ws.append(row)
    
    # ä¿å­˜Excelæ–‡ä»¶
    wb.save(output_path)
    
    print(f"Excelæ–‡ä»¶å·²ä¿å­˜åˆ°: {output_path}")
if __name__ == '__main__':
    # åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ
    log_file_path = setup_logging()
    main_logger = get_logger("main")
    main_start_time = time.time()
    
    main_logger.info("ğŸ¯ ç¨‹åºå¯åŠ¨å‚æ•°:")
    main_logger.info(f"   Pythonç‰ˆæœ¬: {sys.version}")
    main_logger.info(f"   å·¥ä½œç›®å½•: {os.getcwd()}")
    main_logger.info(f"   ç¯å¢ƒå˜é‡å·²åŠ è½½")

    switch_production_or_test = 'test' # prod / test
    main_logger.info(f"è¿è¡Œæ¨¡å¼: {switch_production_or_test}")

    if switch_production_or_test == 'test':
        log_section_start(main_logger, "æµ‹è¯•æ¨¡å¼æ‰§è¡Œ")
        
        start_time=time.time()
        
        # åˆå§‹åŒ–æ•°æ®åº“
        log_step(main_logger, "åˆå§‹åŒ–æ•°æ®åº“è¿æ¥")
        db_url_from = os.environ.get("DATABASE_URL")
        main_logger.info(f"æ•°æ®åº“URL: {db_url_from}")
        engine = create_engine(db_url_from)
        log_success(main_logger, "æ•°æ®åº“è¿æ¥åˆ›å»ºå®Œæˆ")
        
        # åŠ è½½æ•°æ®é›†
        log_step(main_logger, "åŠ è½½æ•°æ®é›†")
        dataset_base = "./src/dataset/agent-v1-c4"
        main_logger.info(f"æ•°æ®é›†è·¯å¾„: {dataset_base}")
        projects = load_dataset(dataset_base)
        log_success(main_logger, "æ•°æ®é›†åŠ è½½å®Œæˆ", f"æ‰¾åˆ° {len(projects)} ä¸ªé¡¹ç›®")
 
        # è®¾ç½®é¡¹ç›®å‚æ•°
        project_id = 'fishcake0803021'  # ä½¿ç”¨å­˜åœ¨çš„é¡¹ç›®ID
        project_path = ''
        main_logger.info(f"ç›®æ ‡é¡¹ç›®ID: {project_id}")
        project = Project(project_id, projects[project_id])
        log_success(main_logger, "é¡¹ç›®å¯¹è±¡åˆ›å»ºå®Œæˆ")
        
        # æ£€æŸ¥æ‰«ææ¨¡å¼
        scan_mode = os.getenv("SCAN_MODE","SPECIFIC_PROJECT")
        main_logger.info(f"æ‰«ææ¨¡å¼: {scan_mode}")
        
        cmd = 'detect_vul'
        main_logger.info(f"æ‰§è¡Œå‘½ä»¤: {cmd}")
        
        if cmd == 'detect_vul':
            # æ‰§è¡Œé¡¹ç›®æ‰«æ
            lancedb,lance_table_name,project_audit=scan_project(project, engine) # scan
            
            # æ ¹æ®æ‰«ææ¨¡å¼å†³å®šæ˜¯å¦æ‰§è¡Œæ¼æ´éªŒè¯
            if scan_mode in ["COMMON_PROJECT", "PURE_SCAN", "CHECKLIST", "COMMON_PROJECT_FINE_GRAINED"]:
                main_logger.info(f"æ‰«ææ¨¡å¼ '{scan_mode}' éœ€è¦æ‰§è¡Œæ¼æ´éªŒè¯")
                check_function_vul(engine,lancedb,lance_table_name,project_audit) # confirm
            else:
                main_logger.info(f"æ‰«ææ¨¡å¼ '{scan_mode}' è·³è¿‡æ¼æ´éªŒè¯æ­¥éª¤")

        # ç»Ÿè®¡æ€»æ‰§è¡Œæ—¶é—´
        end_time=time.time()
        total_duration = end_time-start_time
        log_success(main_logger, "æ‰€æœ‰æ‰«æä»»åŠ¡å®Œæˆ", f"æ€»è€—æ—¶: {total_duration:.2f}ç§’")
        
        # ç”ŸæˆExcelæŠ¥å‘Š
        log_step(main_logger, "ç”ŸæˆExcelæŠ¥å‘Š")
        excel_start = time.time()
        generate_excel("./output.xlsx",project_id)
        excel_duration = time.time() - excel_start
        log_success(main_logger, "ExcelæŠ¥å‘Šç”Ÿæˆå®Œæˆ", f"è€—æ—¶: {excel_duration:.2f}ç§’, æ–‡ä»¶: ./output.xlsx")
        
        log_section_end(main_logger, "æµ‹è¯•æ¨¡å¼æ‰§è¡Œ", time.time() - main_start_time)
        
        
    if switch_production_or_test == 'prod':
        # Set up command line argument parsing
        parser = argparse.ArgumentParser(description='Process input parameters for vulnerability scanning.')
        parser.add_argument('-fpath', type=str, required=True, help='Combined base path for the dataset and folder')
        parser.add_argument('-id', type=str, required=True, help='Project ID')
        # parser.add_argument('-cmd', type=str, choices=['detect', 'confirm','all'], required=True, help='Command to execute')
        parser.add_argument('-o', type=str, required=True, help='Output file path')
        # usage:
        # python main.py 
        # --fpath ../../dataset/agent-v1-c4/Archive 
        # --id Archive_aaa 
        # --cmd detect

        # Parse arguments
        args = parser.parse_args()
        print("fpath:",args.fpath)
        print("id:",args.id)
        print("cmd:",args.cmd)
        print("o:",args.o)
        # Split dataset_folder into dataset and folder
        dataset_base, folder_name = os.path.split(args.fpath)
        print("dataset_base:",dataset_base)
        print("folder_name:",folder_name)
        # Start time
        start_time = time.time()

        # Database setup
        db_url_from = os.environ.get("DATABASE_URL")
        engine = create_engine(db_url_from)

        # Load projects
        projects = load_dataset(dataset_base, args.id, folder_name)
        project = Project(args.id, projects[args.id])

        # Execute command
        # if args.cmd == 'detect':
        #     scan_project(project, engine)  # scan            
        # elif args.cmd == 'confirm':
        #     check_function_vul(engine)  # confirm
        # elif args.cmd == 'all':
        lancedb=scan_project(project, engine)  # scan
        check_function_vul(engine,lancedb)  # confirm

        end_time = time.time()
        print("Total time:", end_time -start_time)
        generate_excel(args.o,args.id)