import argparse
import ast
import os
import time
import audit_config
from ai_engine import *
from project import ProjectAudit
from library.dataset_utils import load_dataset, Project
from planning import PlanningV2
from prompts import prompts
from sqlalchemy import create_engine
from dao import CacheManager, ProjectTaskMgr
import os
import pandas as pd
from openpyxl import Workbook,load_workbook
from openpyxl.utils.dataframe import dataframe_to_rows
from context import ContextFactory
from res_processor.res_processor import ResProcessor
# æ–°å¢å¯¼å…¥ code_summarizer
# from code_summarizer import smart_business_flow_analysis_from_content  # å·²ç§»é™¤mermaidç”ŸæˆåŠŸèƒ½

import dotenv
dotenv.load_dotenv()

def scan_project(project, db_engine):
    # 1. parsing projects  
    project_audit = ProjectAudit(project.id, project.path, db_engine)
    project_audit.parse(project.white_files, project.white_functions)
    
    # 1.5 build context factory and initialize rag processor
    context_factory = ContextFactory(project_audit)
    context_factory.initialize_rag_processor(
        project_audit.functions_to_check, 
        "./src/codebaseQA/lancedb", 
        project.id
    )
    
    # 1.6 ç”Ÿæˆ mmd æ–‡ä»¶ - æ–°å¢åŠŸèƒ½ï¼ˆä»…åœ¨å¯ç”¨ä¸šåŠ¡æµæ¨¡å¼æ—¶ï¼‰
    switch_business_code = eval(os.environ.get('SWITCH_BUSINESS_CODE', 'True'))
    
    if switch_business_code:
        # æ£€æŸ¥JSONæ–‡ä»¶æ˜¯å¦å­˜åœ¨
        json_dir = f"src/codebaseQA/json/{project.id}"
        existing_json_files = []
        if os.path.exists(json_dir):
            for file_name in os.listdir(json_dir):
                if file_name.endswith('.json'):
                    existing_json_files.append(file_name)
        
        if existing_json_files:
            print(f"âœ… å‘ç°JSONä¸šåŠ¡æµæ–‡ä»¶:")
            for json_file in existing_json_files:
                print(f"   - {json_file}")
            print("ğŸ“„ å°†ä½¿ç”¨JSONæ–‡ä»¶ä¸­çš„ä¸šåŠ¡æµæ•°æ®")
        else:
            print("âš ï¸ æœªå‘ç°JSONä¸šåŠ¡æµæ–‡ä»¶ï¼Œå»ºè®®æ‰‹åŠ¨åˆ›å»ºJSONä¸šåŠ¡æµæ–‡ä»¶")
        
        # è®¾ç½®ç›¸å…³å±æ€§ï¼ˆä¸å†ä½¿ç”¨mermaidï¼‰
        project_audit.mermaid_result = None
        project_audit.mermaid_output_dir = None
    else:
        print("ğŸ”„ SWITCH_BUSINESS_CODE=Falseï¼Œä½¿ç”¨ä¼ ç»Ÿæ‰«ææ¨¡å¼")
        # ä¸å¯ç”¨ä¸šåŠ¡æµæ¨¡å¼
        project_audit.mermaid_result = None
        project_audit.mermaid_output_dir = None
    
    # 2. planning & scanning
    project_taskmgr = ProjectTaskMgr(project.id, db_engine) 
    
    planning = PlanningV2(project_audit, project_taskmgr)
    # 
    engine = AiEngine(planning, project_taskmgr, context_factory.rag_processor.db, "lancedb_"+project.id, project_audit)
    # 1. æ‰«æ 
    engine.do_planning()
    engine.do_scan()

    return context_factory.rag_processor.db, context_factory.rag_processor.table_name, project_audit
    
    # 2. gpt4 å¯¹ç»“æœåšrescan 
    # rescan_project_with_gpt4(project.id, db_engine)

def check_function_vul(engine,lancedb,lance_table_name,project_audit):
    project_taskmgr = ProjectTaskMgr(project.id, engine)
    engine = AiEngine(None, project_taskmgr,lancedb,lance_table_name,project_audit)
    engine.check_function_vul()
    # print(result)

def generate_excel(output_path, project_id):
    project_taskmgr = ProjectTaskMgr(project_id, engine)
    entities = project_taskmgr.query_task_by_project_id(project.id)
    
    # åˆ›å»ºä¸€ä¸ªç©ºçš„DataFrameæ¥å­˜å‚¨æ‰€æœ‰å®ä½“çš„æ•°æ®
    data = []
    for entity in entities:
        if ("yes" in str(entity.result_gpt4).lower() 
            # or "not sure" in str(entity.result_gpt4).lower()
            ) and len(entity.business_flow_code)>0:
            data.append({
                'æ¼æ´ç»“æœ': entity.result,
                'ID': entity.id,
                'é¡¹ç›®åç§°': entity.project_id,
                'åˆåŒç¼–å·': entity.contract_code,
                'UUID': entity.key,
                'å‡½æ•°åç§°': entity.name,
                'å‡½æ•°ä»£ç ': entity.content,
                'å¼€å§‹è¡Œ': entity.start_line,
                'ç»“æŸè¡Œ': entity.end_line,
                'ç›¸å¯¹è·¯å¾„': entity.relative_file_path,
                'ç»å¯¹è·¯å¾„': entity.absolute_file_path,
                'ä¸šåŠ¡æµç¨‹ä»£ç ': entity.business_flow_code,
                'ä¸šåŠ¡æµç¨‹è¡Œ': entity.business_flow_lines,
                'ä¸šåŠ¡æµç¨‹ä¸Šä¸‹æ–‡': entity.business_flow_context,
                'ç¡®è®¤ç»“æœ': entity.result_gpt4,
                'ç¡®è®¤ç»†èŠ‚': entity.category
            })
    
    # å°†æ•°æ®è½¬æ¢ä¸ºDataFrame
    if not data:  # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®
        print("No data to process")
        return
        
    df = pd.DataFrame(data)
    
    try:
        # å¯¹dfè¿›è¡Œæ¼æ´å½’é›†å¤„ç†
        res_processor = ResProcessor(df,max_group_size=10,iteration_rounds=5,enable_chinese_translation=True)
        processed_df = res_processor.process()
        
        # ç¡®ä¿æ‰€æœ‰å¿…éœ€çš„åˆ—éƒ½å­˜åœ¨
        required_columns = df.columns
        for col in required_columns:
            if col not in processed_df.columns:
                processed_df[col] = ''
                
        # é‡æ–°æ’åˆ—åˆ—é¡ºåºä»¥åŒ¹é…åŸå§‹DataFrame
        processed_df = processed_df[df.columns]
    except Exception as e:
        print(f"Error processing data: {e}")
        processed_df = df  # å¦‚æœå¤„ç†å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹DataFrame
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_dir = os.path.dirname(output_path)
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»ºæ–°æ–‡ä»¶
    if not os.path.exists(output_path):
        wb = Workbook()
        ws = wb.active
        ws.title = "é¡¹ç›®æ•°æ®"
    else:
        wb = load_workbook(output_path)
        if "é¡¹ç›®æ•°æ®" in wb.sheetnames:
            ws = wb["é¡¹ç›®æ•°æ®"]
        else:
            ws = wb.create_sheet("é¡¹ç›®æ•°æ®")
    
    # å¦‚æœå·¥ä½œè¡¨æ˜¯ç©ºçš„ï¼Œæ·»åŠ è¡¨å¤´
    if ws.max_row == 1:
        for col, header in enumerate(processed_df.columns, start=1):
            ws.cell(row=1, column=col, value=header)
    
    # å°†DataFrameæ•°æ®å†™å…¥å·¥ä½œè¡¨
    for row in dataframe_to_rows(processed_df, index=False, header=False):
        ws.append(row)
    
    # ä¿å­˜Excelæ–‡ä»¶
    wb.save(output_path)
    
    print(f"Excelæ–‡ä»¶å·²ä¿å­˜åˆ°: {output_path}")
if __name__ == '__main__':

    switch_production_or_test = 'test' # prod / test

    if switch_production_or_test == 'test':
        start_time=time.time()
        db_url_from = os.environ.get("DATABASE_URL")
        engine = create_engine(db_url_from)
        
        dataset_base = "./src/dataset/agent-v1-c4"
        projects = load_dataset(dataset_base)
 
        project_id = 'tonvm07273'
        project_path = ''
        project = Project(project_id, projects[project_id])
        
        cmd = 'detect_vul'
        if cmd == 'detect_vul':
            lancedb,lance_table_name,project_audit=scan_project(project, engine) # scan
            if os.getenv("SCAN_MODE","SPECIFIC_PROJECT") in [
                "SPECIFIC_PROJECT",
                "COMMON_PROJECT",
                "PURE_SCAN",
                "CHECKLIST",
                "CHECKLIST_PIPELINE",
                "COMMON_PROJECT_FINE_GRAINED"]:
                check_function_vul(engine,lancedb,lance_table_name,project_audit) # confirm



        end_time=time.time()
        print("Total time:",end_time-start_time)
        generate_excel("./output.xlsx",project_id)
        
        
    if switch_production_or_test == 'prod':
        # Set up command line argument parsing
        parser = argparse.ArgumentParser(description='Process input parameters for vulnerability scanning.')
        parser.add_argument('-fpath', type=str, required=True, help='Combined base path for the dataset and folder')
        parser.add_argument('-id', type=str, required=True, help='Project ID')
        # parser.add_argument('-cmd', type=str, choices=['detect', 'confirm','all'], required=True, help='Command to execute')
        parser.add_argument('-o', type=str, required=True, help='Output file path')
        # usage:
        # python main.py 
        # --fpath ../../dataset/agent-v1-c4/Archive 
        # --id Archive_aaa 
        # --cmd detect

        # Parse arguments
        args = parser.parse_args()
        print("fpath:",args.fpath)
        print("id:",args.id)
        print("cmd:",args.cmd)
        print("o:",args.o)
        # Split dataset_folder into dataset and folder
        dataset_base, folder_name = os.path.split(args.fpath)
        print("dataset_base:",dataset_base)
        print("folder_name:",folder_name)
        # Start time
        start_time = time.time()

        # Database setup
        db_url_from = os.environ.get("DATABASE_URL")
        engine = create_engine(db_url_from)

        # Load projects
        projects = load_dataset(dataset_base, args.id, folder_name)
        project = Project(args.id, projects[args.id])

        # Execute command
        # if args.cmd == 'detect':
        #     scan_project(project, engine)  # scan            
        # elif args.cmd == 'confirm':
        #     check_function_vul(engine)  # confirm
        # elif args.cmd == 'all':
        lancedb=scan_project(project, engine)  # scan
        check_function_vul(engine,lancedb)  # confirm

        end_time = time.time()
        print("Total time:", end_time -start_time)
        generate_excel(args.o,args.id)