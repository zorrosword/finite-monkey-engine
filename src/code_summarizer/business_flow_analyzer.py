# ä¸šåŠ¡æµç¨‹åˆ†æå™¨ - å¢é‡å¼ä¸šåŠ¡æµç¨‹åˆ†æå’ŒMermaidå›¾ç”Ÿæˆ
# Business Flow Analyzer - Incremental Business Flow Analysis and Mermaid Generation

import os
import sys
import logging
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°path
project_root = Path(__file__).parent.parent
if str(project_root) not in sys.path:
    sys.path.append(str(project_root))

from openai_api.openai import ask_claude_for_code_analysis
from .token_calculator import TokenCalculator, TokenUsage

logger = logging.getLogger(__name__)

@dataclass
class BusinessFlowStepResult:
    """ä¸šåŠ¡æµç¨‹åˆ†ææ­¥éª¤ç»“æœ"""
    step_id: int
    files_analyzed: List[str]
    flow_description: str
    key_interactions: List[Dict[str, str]]
    mermaid_fragment: str
    token_usage: TokenUsage
    is_reinforcement: bool = False  # æ ‡è¯†æ˜¯å¦ä¸ºå¼ºåŒ–åˆ†æ

@dataclass
class FolderAnalysisResult:
    """æ–‡ä»¶å¤¹çº§åˆ«çš„åˆ†æç»“æœ"""
    folder_path: str
    folder_name: str
    files_count: int
    analysis_steps: List[BusinessFlowStepResult]
    folder_mermaid_graph: str
    folder_summary: str
    token_usage: TokenUsage

@dataclass
class CompleteBusinessFlowResult:
    """å®Œæ•´ä¸šåŠ¡æµç¨‹åˆ†æç»“æœ"""
    project_name: str
    total_files: int
    analysis_strategy: str  # "incremental" æˆ– "folder_based"
    
    # å¢é‡åˆ†æç»“æœï¼ˆå•ä¸€é¡¹ç›®ï¼‰
    analysis_steps: List[BusinessFlowStepResult]
    final_mermaid_graph: str
    business_summary: str
    
    # æ–‡ä»¶å¤¹åˆ†æç»“æœï¼ˆå¤§é¡¹ç›®ï¼‰
    folder_analyses: Dict[str, FolderAnalysisResult]
    global_mermaid_graph: str  # é¡¹ç›®æ•´ä½“æ¦‚è§ˆå›¾
    
    total_token_usage: TokenUsage

class BusinessFlowAnalyzer:
    """ä¸šåŠ¡æµç¨‹åˆ†æå™¨ - å¢é‡å¼åˆ†æå’Œæµç¨‹å›¾ç”Ÿæˆ"""
    
    def __init__(self, model: str = "claude-3-5-sonnet-20241022"):
        """åˆå§‹åŒ–ä¸šåŠ¡æµç¨‹åˆ†æå™¨
        
        Args:
            model: ä½¿ç”¨çš„AIæ¨¡å‹åç§°
        """
        self.model = model
        self.token_calculator = TokenCalculator()
        self.analysis_history: List[BusinessFlowStepResult] = []
        
        # æ–‡ä»¶å¤¹åˆ†æé…ç½®
        self.LARGE_PROJECT_THRESHOLD = 20  # é™ä½é˜ˆå€¼ï¼Œè¶…è¿‡20ä¸ªæ–‡ä»¶è®¤ä¸ºæ˜¯å¤§é¡¹ç›®
        self.MAX_FILES_PER_FOLDER = 10     # æ¯ä¸ªæ–‡ä»¶å¤¹æœ€å¤šåˆ†æ10ä¸ªæ–‡ä»¶
        
        logger.info(f"ğŸš€ åˆå§‹åŒ–ä¸šåŠ¡æµç¨‹åˆ†æå™¨ï¼Œæ¨¡å‹: {model}")
        logger.info(f"ğŸ“Š Mermaidå›¾ç”Ÿæˆæ—¥å¿—å·²å¯ç”¨")

    def _log_mermaid_generation_start(self, context: str, file_info: str = ""):
        """è®°å½•Mermaidç”Ÿæˆå¼€å§‹æ—¥å¿—"""
        logger.info(f"ğŸ¨ å¼€å§‹ç”ŸæˆMermaidå›¾: {context}")
        if file_info:
            logger.info(f"ğŸ“ å¤„ç†æ–‡ä»¶: {file_info}")

    def _log_mermaid_generation_result(self, mermaid_content: str, context: str = "", 
                                     step_id: int = 0):
        """è®°å½•Mermaidç”Ÿæˆç»“æœæ—¥å¿—"""
        if not mermaid_content:
            logger.warning(f"âš ï¸  Mermaidå›¾ç”Ÿæˆå¤±è´¥æˆ–ä¸ºç©º: {context}")
            return
        
        # ç®€å•ç»Ÿè®¡Mermaidå›¾ä¿¡æ¯
        lines = mermaid_content.split('\n')
        total_lines = len(lines)
        interaction_lines = [line for line in lines if '->>' in line or '-->' in line]
        
        logger.info(f"âœ… Mermaidå›¾ç”ŸæˆæˆåŠŸ: {context}")
        logger.info(f"ğŸ“ å›¾è¡¨è§„æ¨¡ - æ€»è¡Œæ•°: {total_lines}, äº¤äº’: {len(interaction_lines)}")
        
        if step_id > 0:
            logger.info(f"ğŸ”„ åˆ†ææ­¥éª¤: {step_id}")

    def _log_mermaid_optimization(self, original_length: int, optimized_length: int, context: str):
        """è®°å½•Mermaidä¼˜åŒ–è¿‡ç¨‹æ—¥å¿—"""
        if optimized_length == 0:
            logger.warning(f"âŒ Mermaidå›¾ä¼˜åŒ–å¤±è´¥: {context}")
            return
        
        change_percent = ((optimized_length - original_length) / original_length * 100) if original_length > 0 else 0
        
        if abs(change_percent) > 5:
            direction = "æ‰©å±•" if change_percent > 0 else "ç²¾ç®€"
            logger.info(f"ğŸ“ˆ Mermaidå›¾{direction}: {context} ({change_percent:+.1f}%, {original_length}â†’{optimized_length}å­—ç¬¦)")
        else:
            logger.info(f"ğŸ”§ Mermaidå›¾ä¼˜åŒ–å®Œæˆ: {context} ({original_length}â†’{optimized_length}å­—ç¬¦)")

    def _log_folder_merge(self, folder_count: int, total_diagrams: int, context: str):
        """è®°å½•æ–‡ä»¶å¤¹å›¾è¡¨åˆå¹¶è¿‡ç¨‹æ—¥å¿—"""
        logger.info(f"ğŸ”€ åˆå¹¶å¤šæ–‡ä»¶å¤¹å›¾è¡¨: {context}")
        logger.info(f"ğŸ“Š åˆå¹¶ç»Ÿè®¡ - æ–‡ä»¶å¤¹æ•°: {folder_count}, å›¾è¡¨æ•°: {total_diagrams}")

    def analyze_business_flow_smart(self, 
                                  files_content: Dict[str, str],
                                  project_name: str,
                                  enable_reinforcement: bool = True) -> CompleteBusinessFlowResult:
        """æ™ºèƒ½ä¸šåŠ¡æµç¨‹åˆ†æ - è‡ªåŠ¨é€‰æ‹©å¢é‡æˆ–æ–‡ä»¶å¤¹çº§åˆ«åˆ†æ
        
        Args:
            files_content: æ–‡ä»¶å†…å®¹æ˜ å°„
            project_name: é¡¹ç›®åç§°
            enable_reinforcement: æ˜¯å¦å¯ç”¨å¼ºåŒ–åˆ†æ
            
        Returns:
            å®Œæ•´çš„ä¸šåŠ¡æµç¨‹åˆ†æç»“æœ
        """
        logger.info(f"ğŸ¯ å¼€å§‹æ™ºèƒ½ä¸šåŠ¡æµç¨‹åˆ†æ: {project_name} ({len(files_content)} ä¸ªæ–‡ä»¶)")
        
        # åˆ¤æ–­ä½¿ç”¨å“ªç§åˆ†æç­–ç•¥
        if len(files_content) <= self.LARGE_PROJECT_THRESHOLD and not self._has_complex_folder_structure(files_content):
            # å°å‹é¡¹ç›®ï¼šä½¿ç”¨å¢é‡åˆ†æ
            logger.info("ğŸ” æ£€æµ‹åˆ°å°å‹é¡¹ç›®ï¼Œä½¿ç”¨å¢é‡åˆ†æç­–ç•¥")
            return self._analyze_with_incremental_strategy(files_content, project_name, enable_reinforcement)
        else:
            # å¤§å‹é¡¹ç›®ï¼šä½¿ç”¨æ–‡ä»¶å¤¹çº§åˆ«åˆ†æ
            logger.info("ğŸ¢ æ£€æµ‹åˆ°å¤§å‹é¡¹ç›®ï¼Œä½¿ç”¨æ–‡ä»¶å¤¹çº§åˆ«åˆ†æç­–ç•¥")
            return self._analyze_with_folder_strategy(files_content, project_name, enable_reinforcement)
    
    def _has_complex_folder_structure(self, files_content: Dict[str, str]) -> bool:
        """æ£€æµ‹æ˜¯å¦æœ‰å¤æ‚çš„æ–‡ä»¶å¤¹ç»“æ„"""
        folder_set = set()
        for file_path in files_content.keys():
            # è·å–æ–‡ä»¶å¤¹è·¯å¾„
            folder = str(Path(file_path).parent)
            if folder != '.':
                folder_set.add(folder)
        
        # å¦‚æœæœ‰2ä¸ªä»¥ä¸Šä¸åŒçš„æ–‡ä»¶å¤¹ï¼Œè®¤ä¸ºç»“æ„å¤æ‚ï¼Œä½¿ç”¨æ–‡ä»¶å¤¹æ¨¡å¼
        is_complex = len(folder_set) >= 2
        logger.info(f"ğŸ“‚ æ–‡ä»¶å¤¹ç»“æ„æ£€æµ‹ - å‘ç° {len(folder_set)} ä¸ªæ–‡ä»¶å¤¹ï¼Œ{'ä½¿ç”¨æ–‡ä»¶å¤¹æ¨¡å¼' if is_complex else 'ä½¿ç”¨å•ä¸€æ¨¡å¼'}")
        return is_complex
    
    def _analyze_with_incremental_strategy(self, 
                                         files_content: Dict[str, str],
                                         project_name: str,
                                         enable_reinforcement: bool) -> CompleteBusinessFlowResult:
        """ä½¿ç”¨å¢é‡åˆ†æç­–ç•¥"""
        
        # æ‰§è¡ŒåŸæœ‰çš„å¢é‡åˆ†æ
        incremental_result = self.analyze_business_flow_incremental(files_content, project_name)
        
        # å¦‚æœå¯ç”¨å¼ºåŒ–åˆ†æï¼Œè¿›è¡Œå¤šè½®å¼ºåŒ–
        if enable_reinforcement:
            logger.info("ğŸ’ª å¼€å§‹å¼ºåŒ–åˆ†æï¼Œæå‡Mermaidå›¾è´¨é‡")
            reinforced_result = self._perform_reinforcement_analysis(
                files_content, project_name, incremental_result)
            
            # åˆå¹¶å¼ºåŒ–åˆ†æç»“æœ
            incremental_result.analysis_steps.extend(reinforced_result.analysis_steps)
            
            # è®°å½•å¼ºåŒ–å‰åçš„å¯¹æ¯”
            original_length = len(incremental_result.final_mermaid_graph)
            incremental_result.final_mermaid_graph = reinforced_result.final_mermaid_graph
            final_length = len(incremental_result.final_mermaid_graph)
            
            self._log_mermaid_optimization(original_length, final_length, "å¼ºåŒ–åˆ†æ")
            
            incremental_result.total_token_usage = self._merge_token_usage(
                incremental_result.total_token_usage, reinforced_result.total_token_usage)
        
        # è½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼
        return CompleteBusinessFlowResult(
            project_name=project_name,
            total_files=len(files_content),
            analysis_strategy="incremental",
            analysis_steps=incremental_result.analysis_steps,
            final_mermaid_graph=incremental_result.final_mermaid_graph,
            business_summary=incremental_result.business_summary,
            folder_analyses={},
            global_mermaid_graph=incremental_result.final_mermaid_graph,
            total_token_usage=incremental_result.total_token_usage
        )
    
    def _analyze_with_folder_strategy(self, 
                                    files_content: Dict[str, str],
                                    project_name: str,
                                    enable_reinforcement: bool) -> CompleteBusinessFlowResult:
        """ä½¿ç”¨æ–‡ä»¶å¤¹çº§åˆ«åˆ†æç­–ç•¥"""
        
        # æŒ‰æ–‡ä»¶å¤¹åˆ†ç»„æ–‡ä»¶
        folder_groups = self._group_files_by_folder(files_content)
        logger.info(f"ğŸ“‚ é¡¹ç›®åˆ†ç»„å®Œæˆ - å…± {len(folder_groups)} ä¸ªæ–‡ä»¶å¤¹")
        
        # åˆ†ææ¯ä¸ªæ–‡ä»¶å¤¹
        folder_analyses = {}
        all_steps = []
        total_token_usage = TokenUsage(0, 0, 0, True, 200000, "")
        
        for folder_path, folder_files in folder_groups.items():
            logger.info(f"ğŸ“‚ åˆ†ææ–‡ä»¶å¤¹: {folder_path} ({len(folder_files)} ä¸ªæ–‡ä»¶)")
            
            # åˆ†æå•ä¸ªæ–‡ä»¶å¤¹ - ä½¿ç”¨å¢é‡åˆ†æ
            folder_result = self._analyze_single_folder_incremental(
                folder_files, folder_path, project_name, enable_reinforcement)
            
            folder_analyses[folder_path] = folder_result
            all_steps.extend(folder_result.analysis_steps)
            total_token_usage = self._merge_token_usage(total_token_usage, folder_result.token_usage)
            
            # è®°å½•æ–‡ä»¶å¤¹åˆ†æç»“æœ
            self._log_mermaid_generation_result(
                folder_result.folder_mermaid_graph, 
                f"æ–‡ä»¶å¤¹{folder_path}åˆ†æ"
            )
        
        # ç”Ÿæˆå…¨å±€æ¦‚è§ˆå›¾ - åˆå¹¶å¤šä¸ªæ–‡ä»¶å¤¹çš„diagram
        logger.info("ğŸŒ åˆå¹¶å¤šæ–‡ä»¶å¤¹diagramç”Ÿæˆå…¨å±€ä¸šåŠ¡æµ")
        global_mermaid = self._merge_folder_diagrams(folder_analyses, project_name)
        
        # è®°å½•åˆå¹¶ç»“æœ
        self._log_folder_merge(len(folder_analyses), sum(len(f.analysis_steps) for f in folder_analyses.values()), "å…¨å±€ä¸šåŠ¡æµåˆå¹¶")
        
        return CompleteBusinessFlowResult(
            project_name=project_name,
            total_files=len(files_content),
            analysis_strategy="folder_based",
            analysis_steps=all_steps,
            final_mermaid_graph="",  # æ–‡ä»¶å¤¹æ¨¡å¼ä¸‹ä¸»è¦çœ‹å„æ–‡ä»¶å¤¹çš„å›¾å’Œå…¨å±€å›¾
            business_summary=f"{project_name}é¡¹ç›®æ–‡ä»¶å¤¹çº§åˆ«åˆ†æå®Œæˆï¼Œå…±åˆ†æ{len(folder_analyses)}ä¸ªæ–‡ä»¶å¤¹",
            folder_analyses=folder_analyses,
            global_mermaid_graph=global_mermaid,
            total_token_usage=total_token_usage
        )
    
    def _group_files_by_folder(self, files_content: Dict[str, str]) -> Dict[str, Dict[str, str]]:
        """æŒ‰æ–‡ä»¶å¤¹åˆ†ç»„æ–‡ä»¶"""
        folder_groups = {}
        
        for file_path, content in files_content.items():
            folder = str(Path(file_path).parent)
            if folder == '.':
                folder = 'root'
            
            if folder not in folder_groups:
                folder_groups[folder] = {}
            
            folder_groups[folder][file_path] = content
        
        return folder_groups
    
    def _analyze_single_folder_incremental(self, 
                                         folder_files: Dict[str, str],
                                         folder_path: str,
                                         project_name: str,
                                         enable_reinforcement: bool) -> FolderAnalysisResult:
        """ä½¿ç”¨å¢é‡åˆ†æç­–ç•¥åˆ†æå•ä¸ªæ–‡ä»¶å¤¹"""
        
        folder_name = Path(folder_path).name if folder_path != 'root' else 'root'
        
        # å¦‚æœæ–‡ä»¶å¤¹æ–‡ä»¶å¤ªå¤šï¼Œè¿›è¡Œåˆ†æ‰¹å¤„ç†
        if len(folder_files) > self.MAX_FILES_PER_FOLDER:
            logger.warning(f"âš ï¸  æ–‡ä»¶å¤¹ {folder_path} æ–‡ä»¶æ•°è¿‡å¤š({len(folder_files)})ï¼Œå°†åªå¤„ç†å‰{self.MAX_FILES_PER_FOLDER}ä¸ªæ–‡ä»¶")
            # æŒ‰ä¼˜å…ˆçº§é€‰æ‹©å‰Nä¸ªæ–‡ä»¶
            prioritized_files = self._prioritize_files_for_flow_analysis(folder_files)
            folder_files = dict(prioritized_files[:self.MAX_FILES_PER_FOLDER])
        
        # å¯¹æ–‡ä»¶å¤¹å†…æ–‡ä»¶è¿›è¡Œå¢é‡åˆ†æ
        logger.info(f"ğŸ”„ æ–‡ä»¶å¤¹ {folder_path} å¼€å§‹å¢é‡åˆ†æ")
        temp_analyzer = BusinessFlowAnalyzer(self.model)
        folder_incremental_result = temp_analyzer.analyze_business_flow_incremental(
            folder_files, f"{project_name}_{folder_name}")
        
        # å¦‚æœå¯ç”¨å¼ºåŒ–åˆ†æ
        if enable_reinforcement and len(folder_files) <= 5:  # åªæœ‰å°æ–‡ä»¶å¤¹æ‰å¼ºåŒ–
            logger.info(f"ğŸ’ª æ–‡ä»¶å¤¹ {folder_path} å¼€å§‹å¼ºåŒ–åˆ†æ")
            reinforced_result = self._perform_reinforcement_analysis(
                folder_files, f"{project_name}_{folder_name}", folder_incremental_result)
            
            folder_incremental_result.analysis_steps.extend(reinforced_result.analysis_steps)
            folder_incremental_result.final_mermaid_graph = reinforced_result.final_mermaid_graph
        
        return FolderAnalysisResult(
            folder_path=folder_path,
            folder_name=folder_name,
            files_count=len(folder_files),
            analysis_steps=folder_incremental_result.analysis_steps,
            folder_mermaid_graph=folder_incremental_result.final_mermaid_graph,
            folder_summary=folder_incremental_result.business_summary,
            token_usage=folder_incremental_result.total_token_usage
        )
    
    def _perform_reinforcement_analysis(self, 
                                      files_content: Dict[str, str],
                                      project_name: str,
                                      base_result: 'CompleteBusinessFlowResult') -> 'CompleteBusinessFlowResult':
        """æ‰§è¡Œå¼ºåŒ–åˆ†æï¼Œæå‡Mermaidå›¾çš„è¯¦ç»†ç¨‹åº¦"""
        
        logger.info("ğŸ’ª å¼€å§‹å¼ºåŒ–åˆ†æï¼Œå¢å¼ºMermaidå›¾ç»†èŠ‚")
        
        # è·å–å½“å‰æœ€ä½³çš„mermaidå›¾
        current_mermaid = base_result.final_mermaid_graph
        original_length = len(current_mermaid)
        
        # ç¬¬ä¸€è½®ï¼šé€‰æ‹©æœ€é‡è¦çš„æ–‡ä»¶è¿›è¡Œå¼ºåŒ–åˆ†æ
        important_files = self._select_files_for_reinforcement(files_content, base_result.analysis_steps)
        
        reinforcement_steps = []
        
        for file_path, content in important_files.items():
            logger.info(f"ğŸ”§ å¼ºåŒ–åˆ†ææ–‡ä»¶: {file_path}")
            
            # æ‰§è¡Œå¼ºåŒ–åˆ†æ
            reinforced_step = self._analyze_file_for_reinforcement(
                file_path, content, current_mermaid, project_name, len(reinforcement_steps) + 1)
            
            reinforcement_steps.append(reinforced_step)
            
            # è®°å½•å¼ºåŒ–æ­¥éª¤ç»“æœ
            self._log_mermaid_generation_result(
                reinforced_step.mermaid_fragment, 
                f"å¼ºåŒ–åˆ†æ-{file_path}", 
                reinforced_step.step_id
            )
            
            # æ›´æ–°å½“å‰mermaidå›¾
            prev_length = len(current_mermaid)
            current_mermaid = reinforced_step.mermaid_fragment
            current_length = len(current_mermaid)
            
            self._log_mermaid_optimization(prev_length, current_length, f"å¼ºåŒ–æ­¥éª¤{reinforced_step.step_id}")
        
        # ğŸ†• ç¬¬äºŒè½®ï¼šä¸“é—¨è¡¥å……è¢«é—æ¼çš„getter/setterå‡½æ•°
        logger.info("ğŸ” å¼€å§‹ç¬¬äºŒè½®å¼ºåŒ–ï¼šä¸“é—¨æŸ¥æ‰¾è¢«é—æ¼çš„getter/setterå‡½æ•°")
        getter_setter_step = self._analyze_missing_getter_setter_functions(
            files_content, current_mermaid, project_name, len(reinforcement_steps) + 1)
        
        if getter_setter_step:
            reinforcement_steps.append(getter_setter_step)
            
            # è®°å½•getter/setterè¡¥å……ç»“æœ
            prev_length = len(current_mermaid)
            current_mermaid = getter_setter_step.mermaid_fragment
            current_length = len(current_mermaid)
            
            self._log_mermaid_generation_result(
                current_mermaid, 
                "Getter/Setterè¡¥å……åˆ†æ", 
                getter_setter_step.step_id
            )
            
            self._log_mermaid_optimization(prev_length, current_length, "Getter/Setterè¡¥å……")
        
        # è®°å½•æ€»ä½“å¼ºåŒ–æ•ˆæœ
        final_length = len(current_mermaid)
        improvement_percent = ((final_length - original_length) / original_length * 100) if original_length > 0 else 0
        logger.info(f"ğŸ‰ å¼ºåŒ–åˆ†æå®Œæˆ - å›¾è¡¨æ”¹è¿›äº† {improvement_percent:+.1f}% ({original_length}â†’{final_length}å­—ç¬¦)")
        
        # è®¡ç®—å¼ºåŒ–åˆ†æçš„tokenä½¿ç”¨é‡
        reinforcement_token_usage = self._calculate_total_token_usage(
            reinforcement_steps, TokenUsage(0, 0, 0, True, 200000, ""))
        
        return CompleteBusinessFlowResult(
            project_name=f"{project_name}_reinforced",
            total_files=len(important_files),
            analysis_strategy="reinforcement",
            analysis_steps=reinforcement_steps,
            final_mermaid_graph=current_mermaid,
            business_summary=f"{project_name}å¼ºåŒ–åˆ†æå®Œæˆ",
            folder_analyses={},
            global_mermaid_graph=current_mermaid,
            total_token_usage=reinforcement_token_usage
        )
    
    def _analyze_missing_getter_setter_functions(self, 
                                               files_content: Dict[str, str],
                                               current_mermaid: str,
                                               project_name: str,
                                               step_id: int) -> Optional[BusinessFlowStepResult]:
        """ä¸“é—¨åˆ†æå¯èƒ½è¢«é—æ¼çš„getter/setterå‡½æ•°"""
        
        logger.info("ğŸ” åˆ†æå¯èƒ½è¢«é—æ¼çš„getter/setterå‡½æ•°")
        
        # æå–æ‰€æœ‰æ–‡ä»¶ä¸­çš„getter/setterå‡½æ•°
        all_getter_setter_functions = self._extract_getter_setter_functions(files_content)
        
        if not all_getter_setter_functions:
            logger.info("âŒ æœªå‘ç°æ˜æ˜¾çš„getter/setterå‡½æ•°")
            return None
        
        # æ£€æŸ¥å“ªäº›å‡½æ•°å¯èƒ½è¢«é—æ¼äº†
        missing_functions = []
        for func_info in all_getter_setter_functions:
            if func_info['name'] not in current_mermaid:
                missing_functions.append(func_info)
        
        if not missing_functions:
            logger.info("âœ… æ‰€æœ‰getter/setterå‡½æ•°éƒ½å·²è¦†ç›–")
            return None
        
        logger.info(f"ğŸ¯ å‘ç° {len(missing_functions)} ä¸ªå¯èƒ½è¢«é—æ¼çš„getter/setterå‡½æ•°")
        
        # æ„å»ºä¸“é—¨çš„getter/setterå¼ºåŒ–prompt
        prompt = self._build_getter_setter_reinforcement_prompt(
            missing_functions, current_mermaid, project_name)
        
        # è®¡ç®—tokenä½¿ç”¨é‡
        token_usage = self.token_calculator.calculate_prompt_tokens(prompt, self.model)
        
        # è°ƒç”¨Claudeè¿›è¡Œåˆ†æ
        logger.info("ğŸ“¤ å‘é€Getter/Setterè¡¥å……åˆ†æè¯·æ±‚")
        analysis_result = ask_claude_for_code_analysis(prompt)
        
        # è§£æç»“æœ
        flow_description, interactions, enhanced_mermaid = \
            self._parse_reinforcement_result(analysis_result)
        
        return BusinessFlowStepResult(
            step_id=step_id,
            files_analyzed=[info['file_path'] for info in missing_functions],
            flow_description=f"Getter/Setterå‡½æ•°è¡¥å……åˆ†æ: {flow_description}",
            key_interactions=interactions,
            mermaid_fragment=enhanced_mermaid,
            token_usage=token_usage,
            is_reinforcement=True
        )
    
    def _extract_getter_setter_functions(self, files_content: Dict[str, str]) -> List[Dict[str, str]]:
        """æå–æ–‡ä»¶ä¸­çš„getter/setterå‡½æ•°"""
        
        logger.info(f"ğŸ” å¼€å§‹ä» {len(files_content)} ä¸ªæ–‡ä»¶ä¸­æå–getter/setterå‡½æ•°")
        
        getter_setter_functions = []
        
        # å¸¸è§çš„getter/setterå‡½æ•°æ¨¡å¼
        getter_patterns = [
            'function get', 'function is', 'function has', 'function owner', 'function name',
            'function symbol', 'function decimals', 'function totalSupply', 'function balanceOf',
            'function allowance', 'function paused', 'function threshold'
        ]
        
        setter_patterns = [
            'function set', 'function pause', 'function unpause', 'function grant', 'function revoke',
            'function renounce', 'function approve'
        ]
        
        view_patterns = ['view returns', 'pure returns']
        
        for file_path, content in files_content.items():
            file_functions = []
            lines = content.split('\n')
            
            for i, line in enumerate(lines):
                line_stripped = line.strip().lower()
                
                # æ£€æŸ¥getterå‡½æ•°
                if any(pattern in line_stripped for pattern in getter_patterns) or \
                   any(pattern in line_stripped for pattern in view_patterns):
                    
                    # æå–å‡½æ•°å
                    if 'function ' in line_stripped:
                        try:
                            func_start = line_stripped.find('function ') + 9
                            func_end = line_stripped.find('(', func_start)
                            if func_end != -1:
                                func_name = line_stripped[func_start:func_end].strip()
                                
                                function_info = {
                                    'name': func_name,
                                    'type': 'getter',
                                    'file_path': file_path,
                                    'line_number': i + 1,
                                    'content': line.strip()
                                }
                                getter_setter_functions.append(function_info)
                                file_functions.append(func_name)
                        except:
                            continue
                
                # æ£€æŸ¥setterå‡½æ•°
                elif any(pattern in line_stripped for pattern in setter_patterns):
                    
                    if 'function ' in line_stripped:
                        try:
                            func_start = line_stripped.find('function ') + 9
                            func_end = line_stripped.find('(', func_start)
                            if func_end != -1:
                                func_name = line_stripped[func_start:func_end].strip()
                                
                                function_info = {
                                    'name': func_name,
                                    'type': 'setter',
                                    'file_path': file_path,
                                    'line_number': i + 1,
                                    'content': line.strip()
                                }
                                getter_setter_functions.append(function_info)
                                file_functions.append(func_name)
                        except:
                            continue
            
            if file_functions:
                logger.info(f"ğŸ“ {file_path}: å‘ç° {len(file_functions)} ä¸ªgetter/setterå‡½æ•°: {', '.join(file_functions[:5])}{'...' if len(file_functions) > 5 else ''}")
        
        # æŒ‰ç±»å‹ç»Ÿè®¡
        getters = [f for f in getter_setter_functions if f['type'] == 'getter']
        setters = [f for f in getter_setter_functions if f['type'] == 'setter']
        
        logger.info(f"âœ… æå–å®Œæˆ - æ€»è®¡: {len(getter_setter_functions)} ä¸ªå‡½æ•° (Getter: {len(getters)}, Setter: {len(setters)})")
        
        return getter_setter_functions
    
    def _build_getter_setter_reinforcement_prompt(self, 
                                                 missing_functions: List[Dict[str, str]],
                                                 current_mermaid: str,
                                                 project_name: str) -> str:
        """æ„å»ºä¸“é—¨çš„getter/setterå‡½æ•°å¼ºåŒ–prompt"""
        
        # æŒ‰ç±»å‹åˆ†ç»„å‡½æ•°
        getters = [f for f in missing_functions if f['type'] == 'getter']
        setters = [f for f in missing_functions if f['type'] == 'setter']
        
        functions_summary = f"å‘ç° {len(getters)} ä¸ªGetterå‡½æ•°å’Œ {len(setters)} ä¸ªSetterå‡½æ•°å¯èƒ½è¢«é—æ¼"
        
        # åˆ—å‡ºé—æ¼çš„å‡½æ•°
        missing_list = "**è¢«é—æ¼çš„Getterå‡½æ•°:**\n"
        for func in getters:
            missing_list += f"- {func['name']}() åœ¨ {func['file_path']}\n"
        
        missing_list += "\n**è¢«é—æ¼çš„Setterå‡½æ•°:**\n"
        for func in setters:
            missing_list += f"- {func['name']}() åœ¨ {func['file_path']}\n"
        
        # æˆªæ–­current_mermaidä»¥æ§åˆ¶prompté•¿åº¦
        if len(current_mermaid) > 3000:
            mermaid_preview = current_mermaid[:3000] + "\n... (å›¾è¡¨å†…å®¹è¾ƒé•¿ï¼Œä»…æ˜¾ç¤ºå‰éƒ¨åˆ†)"
        else:
            mermaid_preview = current_mermaid
        
        prompt = f"""
ä½ æ˜¯ {project_name} é¡¹ç›®çš„èµ„æ·±æ¶æ„å¸ˆï¼Œå‘ç°ä¸šåŠ¡æµç¨‹å›¾ä¸­**é—æ¼äº†é‡è¦çš„Getter/Setterå‡½æ•°**ã€‚

**å½“å‰ä¸šåŠ¡æµç¨‹å›¾:**
```mermaid
{mermaid_preview}
```

**ğŸ” å‘ç°çš„é—®é¢˜:**
{functions_summary}

{missing_list}

**ğŸ¯ ä¸“é¡¹ä»»åŠ¡è¦æ±‚:**
1. **å¿…é¡»ä¿æŒsequenceDiagramæ ¼å¼** - ç¡®ä¿è¾“å‡ºä»¥ `sequenceDiagram` å¼€å¤´
2. **ä¿ç•™æ‰€æœ‰ç°æœ‰å†…å®¹** - ç»å¯¹ä¸èƒ½åˆ é™¤ä»»ä½•participantæˆ–äº¤äº’
3. **è¡¥å……æ‰€æœ‰é—æ¼çš„Getterå‡½æ•°** - æ¯ä¸ªGetterå‡½æ•°éƒ½å¿…é¡»æ·»åŠ åˆ°å›¾ä¸­
4. **è¡¥å……æ‰€æœ‰é—æ¼çš„Setterå‡½æ•°** - æ¯ä¸ªSetterå‡½æ•°éƒ½å¿…é¡»æ·»åŠ åˆ°å›¾ä¸­
5. **ä½¿ç”¨æ­£ç¡®çš„äº¤äº’æ ¼å¼** - ç¡®ä¿å‡½æ•°åã€å‚æ•°å’Œè¿”å›å€¼å‡†ç¡®
6. **ä¿æŒåŸå§‹åˆçº¦å** - ä½¿ç”¨å…·ä½“çš„åˆçº¦åï¼Œä¸èƒ½ä½¿ç”¨é€šç”¨åç§°

**è¾“å‡ºæ ¼å¼:**
## è¡¥å……åˆ†ææè¿°
[æè¿°è¡¥å……äº†å“ªäº›è¢«é—æ¼çš„Getter/Setterå‡½æ•°ï¼Œä»¥åŠå®ƒä»¬çš„ä½œç”¨]

## è¡¥å……åçš„å®Œæ•´ä¸šåŠ¡æµç¨‹å›¾
```mermaid
sequenceDiagram
    [ä¿ç•™æ‰€æœ‰åŸæœ‰participantå’Œäº¤äº’]
    [æ–°å¢æ‰€æœ‰è¢«é—æ¼çš„Getterå‡½æ•°äº¤äº’]
    [æ–°å¢æ‰€æœ‰è¢«é—æ¼çš„Setterå‡½æ•°äº¤äº’]
    [ç¡®ä¿æ¯ä¸ªå‡½æ•°éƒ½æœ‰æ­£ç¡®çš„å‚æ•°å’Œè¿”å›å€¼]
```



**ğŸ”¥ å…³é”®è¦æ±‚:**
- **å¿…é¡»è¡¥å……æ‰€æœ‰åˆ—å‡ºçš„é—æ¼å‡½æ•°** - ä¸€ä¸ªéƒ½ä¸èƒ½å°‘
- ç»å¯¹ä¿æŒåŸæœ‰å›¾è¡¨çš„å®Œæ•´æ€§
- ä½¿ç”¨å…·ä½“çš„åˆçº¦åï¼Œä¸èƒ½ä½¿ç”¨é€šç”¨åç§°
- ç¡®ä¿å‡½æ•°ç­¾åå’Œå‚æ•°å‡†ç¡®æ— è¯¯
"""
        
        return prompt
    
    def _select_files_for_reinforcement(self, 
                                      files_content: Dict[str, str],
                                      analysis_steps: List[BusinessFlowStepResult]) -> Dict[str, str]:
        """é€‰æ‹©éœ€è¦å¼ºåŒ–åˆ†æçš„é‡è¦æ–‡ä»¶"""
        
        logger.info("ğŸ¯ é€‰æ‹©æ–‡ä»¶è¿›è¡Œå¼ºåŒ–åˆ†æ")
        
        # é€‰æ‹©æœ€é‡è¦çš„æ–‡ä»¶è¿›è¡Œå¼ºåŒ–
        selected_files = {}
        
        # é€‰æ‹©æœ€é‡è¦çš„æ–‡ä»¶
        logger.info("ğŸ“‹ é€‰æ‹©æœ€é‡è¦çš„æ–‡ä»¶è¿›è¡Œå¼ºåŒ–")
        prioritized_files = self._prioritize_files_for_flow_analysis(files_content)
        for file_path, content in prioritized_files[:3]:  # é€‰æ‹©å‰3ä¸ªé‡è¦æ–‡ä»¶
            selected_files[file_path] = content
        
        logger.info(f"âœ… å·²é€‰æ‹© {len(selected_files)} ä¸ªæ–‡ä»¶è¿›è¡Œå¼ºåŒ–åˆ†æ: {list(selected_files.keys())}")
        return selected_files
    
    def _analyze_file_for_reinforcement(self, 
                                      file_path: str,
                                      file_content: str,
                                      current_mermaid: str,
                                      project_name: str,
                                      step_id: int) -> BusinessFlowStepResult:
        """å¯¹å•ä¸ªæ–‡ä»¶è¿›è¡Œå¼ºåŒ–åˆ†æ"""
        
        self._log_mermaid_generation_start(f"å¼ºåŒ–åˆ†æ-æ–‡ä»¶{step_id}", file_path)
        
        # æ„å»ºå¼ºåŒ–åˆ†æprompt
        prompt = self._build_reinforcement_prompt(file_path, file_content, current_mermaid, project_name)
        
        # è®¡ç®—tokenä½¿ç”¨é‡
        token_usage = self.token_calculator.calculate_prompt_tokens(prompt, self.model)
        
        # è°ƒç”¨Claudeè¿›è¡Œå¼ºåŒ–åˆ†æ
        logger.info(f"ğŸ“¤ å‘é€å¼ºåŒ–åˆ†æè¯·æ±‚ - æ–‡ä»¶: {file_path}")
        analysis_result = ask_claude_for_code_analysis(prompt)
        
        # è§£æå¼ºåŒ–åˆ†æç»“æœ
        flow_description, interactions, enhanced_mermaid = \
            self._parse_reinforcement_result(analysis_result)
        
        # è®°å½•å¼ºåŒ–åˆ†æç»“æœ
        self._log_mermaid_generation_result(
            enhanced_mermaid, 
            f"å¼ºåŒ–åˆ†æ-{file_path}",
            step_id
        )
        
        return BusinessFlowStepResult(
            step_id=step_id,
            files_analyzed=[file_path],
            flow_description=flow_description,
            key_interactions=interactions,
            mermaid_fragment=enhanced_mermaid,
            token_usage=token_usage,
            is_reinforcement=True
        )
    
    def _build_reinforcement_prompt(self, 
                                  file_path: str,
                                  file_content: str,
                                  current_mermaid: str,
                                  project_name: str) -> str:
        """æ„å»ºå¼ºåŒ–åˆ†æprompt"""
        
        # æ™ºèƒ½æˆªæ–­æ–‡ä»¶å†…å®¹
        truncated_content = file_content[:6000] + ("\n... (å†…å®¹å·²æˆªæ–­)" if len(file_content) > 6000 else "")
        
        # æ™ºèƒ½æˆªæ–­å½“å‰mermaidå›¾
        if len(current_mermaid) > 4000:
            mermaid_preview = current_mermaid[:4000] + "\n... (mermaidå›¾å†…å®¹è¾ƒé•¿ï¼Œä»…æ˜¾ç¤ºå‰éƒ¨åˆ†)"
        else:
            mermaid_preview = current_mermaid
        
        prompt = f"""
ä½ æ˜¯ {project_name} é¡¹ç›®çš„èµ„æ·±æ¶æ„å¸ˆï¼Œç°åœ¨éœ€è¦å¯¹ä¸šåŠ¡æµç¨‹å›¾è¿›è¡Œ**å¼ºåŒ–åˆ†æ**ï¼Œ**å¿…é¡»è¦†ç›–æ‰€æœ‰å‡½æ•°ï¼Œä¸èƒ½é—æ¼ä»»ä½•ä¸€ä¸ª**ã€‚

**å¼ºåŒ–ç›®æ ‡æ–‡ä»¶: {file_path}**

**æ–‡ä»¶è¯¦ç»†å†…å®¹:**
{truncated_content}

**å½“å‰ä¸šåŠ¡æµç¨‹å›¾:**
```mermaid
{mermaid_preview}
```

**ğŸ” å¼ºåŒ–ä»»åŠ¡è¦æ±‚:**
1. **å¿…é¡»ä¿æŒsequenceDiagramæ ¼å¼** - ç¡®ä¿è¾“å‡ºä»¥ `sequenceDiagram` å¼€å¤´
2. **ä¿ç•™æ‰€æœ‰ç°æœ‰å†…å®¹** - ç»å¯¹ä¸èƒ½åˆ é™¤ä»»ä½•participantæˆ–äº¤äº’
3. **å…¨å‡½æ•°è¦†ç›–åˆ†æ** - è¯†åˆ« {file_path} ä¸­çš„**æ¯ä¸€ä¸ªå‡½æ•°**ï¼ŒåŒ…æ‹¬ï¼š
   - âœ… **Public/Externalå‡½æ•°** - æ‰€æœ‰å¯¹å¤–æš´éœ²çš„å‡½æ•°
   - âœ… **Getterå‡½æ•°** - æ‰€æœ‰è·å–çŠ¶æ€å˜é‡çš„å‡½æ•°ï¼ˆå¦‚ getValue, getBalance, isActiveï¼‰
   - âœ… **Setterå‡½æ•°** - æ‰€æœ‰è®¾ç½®çŠ¶æ€å˜é‡çš„å‡½æ•°ï¼ˆå¦‚ setValue, setOwner, setConfigï¼‰
   - âœ… **View/Pureå‡½æ•°** - æ‰€æœ‰æŸ¥è¯¢ç±»å‡½æ•°ï¼Œæ— è®ºå¤šç®€å•
   - âœ… **æ„é€ å‡½æ•°** - constructorå‡½æ•°
   - âœ… **äº‹ä»¶è§¦å‘** - æ‰€æœ‰emitè¯­å¥
4. **è¡¥å……é—æ¼äº¤äº’** - ç‰¹åˆ«å…³æ³¨ç®€å•çš„getter/setterå‡½æ•°ï¼Œå®ƒä»¬ç»å¸¸è¢«å¿½ç•¥
5. **å¢åŠ å…·ä½“ç»†èŠ‚** - ä¸ºæ¯ä¸ªå‡½æ•°è°ƒç”¨æ·»åŠ å…·ä½“å‚æ•°å’Œè¿”å›å€¼ä¿¡æ¯
6. **ä¼˜åŒ–äº¤äº’æè¿°** - **å¿…é¡»ä½¿ç”¨åŸå§‹çš„åˆçº¦åå’Œå‡½æ•°å**

**å…³é”®æ ¼å¼è¦æ±‚:**
- **åˆçº¦å**: ä½¿ç”¨ {file_path} ä¸­çš„åŸå§‹åˆçº¦åï¼Œä¸èƒ½ä½¿ç”¨é€šç”¨åç§°
- **å‡½æ•°å**: ä½¿ç”¨ä»£ç ä¸­çš„å‡†ç¡®å‡½æ•°åï¼ŒåŒ…å«å®Œæ•´çš„å‡½æ•°ç­¾å
- **å‚æ•°ç±»å‹**: åŒ…å«å‡†ç¡®çš„å‚æ•°ç±»å‹ (å¦‚: address, uint256, string, bool)
- **è¿”å›å€¼**: æ˜ç¡®æ ‡æ³¨å‡½æ•°è¿”å›å€¼ç±»å‹å’Œå«ä¹‰

**è¾“å‡ºæ ¼å¼:**
## å¼ºåŒ–åˆ†ææè¿°
[è¯¦ç»†æè¿°å¯¹ {file_path} çš„**å…¨å‡½æ•°è¦†ç›–åˆ†æ**ï¼Œåˆ—å‡ºæ‰€æœ‰å‘ç°çš„å‡½æ•°ï¼ŒåŒ…æ‹¬è¢«é—æ¼çš„getter/setterå‡½æ•°]

## å¼ºåŒ–åçš„å®Œæ•´ä¸šåŠ¡æµç¨‹å›¾
```mermaid
sequenceDiagram
    [ä¿ç•™æ‰€æœ‰åŸæœ‰participantå’Œäº¤äº’]
    [æ–°å¢ {file_path} çš„**æ‰€æœ‰å‡½æ•°**äº¤äº’ï¼ŒåŒ…æ‹¬getter/setter]
    [ç¡®ä¿æ¯ä¸ªå‡½æ•°è°ƒç”¨éƒ½æœ‰æ˜ç¡®çš„å‚æ•°ç±»å‹å’Œè¿”å›å€¼]
```



**ğŸ”¥ å…³é”®è¦æ±‚:**
- **ç»å¯¹ä¸èƒ½é—æ¼ä»»ä½•å‡½æ•°** - åŒ…æ‹¬æœ€ç®€å•çš„getter/setter
- ç»å¯¹ä¿æŒåŸæœ‰å›¾è¡¨çš„å®Œæ•´æ€§
- **ç»å¯¹ä¸èƒ½ä½¿ç”¨é€šç”¨åç§°å¦‚ "Contract", "Token", "System"ï¼Œå¿…é¡»ä½¿ç”¨å…·ä½“çš„åˆçº¦å**
- ä¸“æ³¨**100%è¦†ç›–** {file_path} ä¸­çš„æ‰€æœ‰å‡½æ•°
"""
        
        return prompt
    
    def _parse_reinforcement_result(self, analysis_result: str) -> Tuple[str, List[Dict], str]:
        """è§£æå¼ºåŒ–åˆ†æç»“æœï¼Œç›´æ¥ä½¿ç”¨å®Œæ•´ç»“æœ"""
        
        logger.info("ğŸ” ç›´æ¥ä½¿ç”¨AIå¼ºåŒ–åˆ†æçš„å®Œæ•´ç»“æœ")
        
        # ç®€å•å¤„ç†ï¼šç›´æ¥ä½¿ç”¨å®Œæ•´çš„åˆ†æç»“æœ
        flow_description = "AIå¼ºåŒ–åˆ†æç»“æœ"
        interactions = [{"type": "reinforcement", "description": "ç›´æ¥ä½¿ç”¨AIå®Œæ•´ç»“æœ"}]
        
        # ç›´æ¥ä½¿ç”¨åˆ†æç»“æœä½œä¸ºmermaidå›¾å†…å®¹
        enhanced_mermaid = analysis_result
        
        logger.info(f"âœ… ä½¿ç”¨å¼ºåŒ–å®Œæ•´ç»“æœï¼Œé•¿åº¦: {len(enhanced_mermaid)}å­—ç¬¦")
        
        return flow_description, interactions, enhanced_mermaid
    
    def _parse_incremental_result(self, analysis_result: str) -> Tuple[str, List[Dict], str]:
        """è§£æå¢é‡åˆ†æç»“æœï¼Œç›´æ¥ä½¿ç”¨å®Œæ•´ç»“æœ"""
        
        logger.info("ğŸ” ç›´æ¥ä½¿ç”¨AIç”Ÿæˆçš„å®Œæ•´ç»“æœ")
        
        # ç®€å•å¤„ç†ï¼šç›´æ¥ä½¿ç”¨å®Œæ•´çš„åˆ†æç»“æœä½œä¸ºæè¿°
        flow_description = "AIç”Ÿæˆçš„ä¸šåŠ¡æµç¨‹åˆ†æ"
        interactions = [{"type": "incremental", "description": "ç›´æ¥ä½¿ç”¨AIå®Œæ•´ç»“æœ"}]
        
        # ç›´æ¥ä½¿ç”¨åˆ†æç»“æœä½œä¸ºmermaidå›¾å†…å®¹
        extended_mermaid = analysis_result
        
        logger.info(f"âœ… ä½¿ç”¨å®Œæ•´ç»“æœï¼Œé•¿åº¦: {len(extended_mermaid)}å­—ç¬¦")
        
        return flow_description, interactions, extended_mermaid
    
    def _generate_global_overview_mermaid(self, 
                                        folder_analyses: Dict[str, FolderAnalysisResult],
                                        project_name: str) -> str:
        """ç”Ÿæˆå…¨å±€æ¦‚è§ˆMermaidå›¾ï¼Œç›´æ¥ä½¿ç”¨AIå®Œæ•´ç»“æœ"""
        
        if not folder_analyses:
            logger.warning("âš ï¸  æ²¡æœ‰æ–‡ä»¶å¤¹åˆ†æç»“æœï¼Œæ— æ³•ç”Ÿæˆå…¨å±€æ¦‚è§ˆå›¾")
            return ""
        
        self._log_mermaid_generation_start("å…¨å±€æ¦‚è§ˆå›¾ç”Ÿæˆ", f"{len(folder_analyses)}ä¸ªæ–‡ä»¶å¤¹")
        
        # æ„å»ºå…¨å±€æ¦‚è§ˆprompt
        prompt = f"""
è¯·ä¸º {project_name} é¡¹ç›®ç”Ÿæˆå…¨å±€æ¶æ„æ¦‚è§ˆå›¾ï¼ŒåŸºäºå„æ–‡ä»¶å¤¹çš„åˆ†æç»“æœã€‚

**é¡¹ç›®æ–‡ä»¶å¤¹ç»“æ„:**
"""
        
        for folder_path, folder_result in folder_analyses.items():
            prompt += f"""
- **{folder_path}/** ({folder_result.files_count} ä¸ªæ–‡ä»¶)
  æ¦‚è¿°: {folder_result.folder_summary[:200]}...
"""
        
        prompt += f"""

**ä»»åŠ¡è¦æ±‚:**
1. åˆ›å»ºé¡¹ç›®çº§åˆ«çš„é«˜å±‚æ¶æ„å›¾
2. å±•ç¤ºå„æ–‡ä»¶å¤¹/æ¨¡å—ä¹‹é—´çš„å…³ç³»
3. çªå‡ºä¸»è¦çš„æ•°æ®æµå’Œæ§åˆ¶æµ
4. ä½¿ç”¨æ¸…æ™°çš„æ¨¡å—åŒ–è®¾è®¡
5. **ä½¿ç”¨å…·ä½“çš„æ¨¡å—åç§°** - åŸºäºæ–‡ä»¶å¤¹åç§°ä½¿ç”¨å‡†ç¡®çš„æè¿°

è¯·ç”Ÿæˆç®€æ´ä½†ä¿¡æ¯ä¸°å¯Œçš„å…¨å±€æ¶æ„å›¾ï¼Œä½¿ç”¨å…·ä½“çš„æ¨¡å—åç§°è€Œéé€šç”¨æœ¯è¯­ã€‚
"""
        
        try:
            logger.info("ğŸ“¤ å‘é€å…¨å±€æ¦‚è§ˆå›¾ç”Ÿæˆè¯·æ±‚")
            analysis_result = ask_claude_for_code_analysis(prompt)
            
            # ç›´æ¥ä½¿ç”¨AIç”Ÿæˆçš„å®Œæ•´ç»“æœ
            if analysis_result:
                logger.info(f"âœ… ç›´æ¥ä½¿ç”¨AIæ¦‚è§ˆç»“æœï¼Œé•¿åº¦: {len(analysis_result)}å­—ç¬¦")
                self._log_mermaid_generation_result(analysis_result, "å…¨å±€æ¦‚è§ˆå›¾")
                return analysis_result
            else:
                logger.warning("âš ï¸  AIè¿”å›ç©ºç»“æœ")
                
        except Exception as e:
            logger.warning(f"âŒ ç”Ÿæˆå…¨å±€æ¦‚è§ˆå›¾å¤±è´¥: {e}")
        
        # å¤‡ç”¨ç®€å•æè¿°
        backup_description = f"{project_name} é¡¹ç›®æ¶æ„æ¦‚è§ˆ\n\nåŒ…å«ä»¥ä¸‹æ¨¡å—:\n"
        for folder_path, folder_result in folder_analyses.items():
            backup_description += f"- {folder_result.folder_name}: {folder_result.files_count}ä¸ªæ–‡ä»¶\n"
        
        logger.info("ğŸ”„ ä½¿ç”¨å¤‡ç”¨ç®€å•æ¦‚è§ˆ")
        self._log_mermaid_generation_result(backup_description, "å¤‡ç”¨å…¨å±€æ¦‚è§ˆå›¾")
        
        return backup_description
    
    def _merge_token_usage(self, usage1: TokenUsage, usage2: TokenUsage) -> TokenUsage:
        """åˆå¹¶ä¸¤ä¸ªtokenä½¿ç”¨é‡"""
        return TokenUsage(
            input_tokens=usage1.input_tokens + usage2.input_tokens,
            estimated_output_tokens=usage1.estimated_output_tokens + usage2.estimated_output_tokens,
            total_tokens=usage1.total_tokens + usage2.total_tokens,
            is_within_limit=usage1.is_within_limit and usage2.is_within_limit,
            model_limit=usage1.model_limit,
            recommendation=f"åˆå¹¶ä½¿ç”¨é‡: {usage1.total_tokens + usage2.total_tokens:,} tokens"
        )
    
    def analyze_business_flow_incremental(self, 
                                        files_content: Dict[str, str],
                                        project_name: str) -> CompleteBusinessFlowResult:
        """çœŸæ­£çš„å¢é‡å¼ä¸šåŠ¡æµç¨‹åˆ†æ - åŸºäºmmdæ–‡ä»¶é€æ­¥æ„å»º
        
        Args:
            files_content: æ–‡ä»¶å†…å®¹æ˜ å°„
            project_name: é¡¹ç›®åç§°
            
        Returns:
            å®Œæ•´çš„ä¸šåŠ¡æµç¨‹åˆ†æç»“æœ
        """
        logger.info(f"ğŸš€ å¼€å§‹çœŸæ­£çš„å¢é‡å¼ä¸šåŠ¡æµç¨‹åˆ†æ: {project_name} ({len(files_content)} ä¸ªæ–‡ä»¶)")
        
        # é‡ç½®åˆ†æå†å²
        self.analysis_history = []
        
        # ç¬¬ä¸€æ­¥ï¼šæŒ‰ä¼˜å…ˆçº§æ’åºæ–‡ä»¶
        sorted_files = self._prioritize_files_for_flow_analysis(files_content)
        
        # ç¬¬äºŒæ­¥ï¼šçœŸæ­£çš„å¢é‡åˆ†æ - ç´¯ç§¯æ„å»ºmermaidå›¾
        cumulative_mermaid = ""  # ç´¯ç§¯çš„mermaidå›¾
        
        for step_id, (file_path, content) in enumerate(sorted_files, 1):
            logger.info(f"ğŸ”„ å¢é‡åˆ†ææ­¥éª¤ {step_id}: {file_path}")
            
            # è¿›è¡Œå•æ–‡ä»¶å¢é‡åˆ†æ
            step_result = self._analyze_single_file_incremental(
                step_id, file_path, content, cumulative_mermaid, project_name)
            
            self.analysis_history.append(step_result)
            
            # æ›´æ–°ç´¯ç§¯çš„mermaidå›¾
            cumulative_mermaid = step_result.mermaid_fragment
            
            logger.info(f"æ­¥éª¤ {step_id} å®Œæˆï¼Œç´¯ç§¯mermaidå›¾é•¿åº¦: {len(cumulative_mermaid)}")
        
        # ç¬¬ä¸‰æ­¥ï¼šæœ€ç»ˆä¼˜åŒ–ç´¯ç§¯çš„mermaidå›¾
        final_result = self._finalize_cumulative_mermaid(
            project_name, files_content, self.analysis_history, cumulative_mermaid)
        
        logger.info(f"å¢é‡å¼ä¸šåŠ¡æµç¨‹åˆ†æå®Œæˆï¼Œå…± {len(self.analysis_history)} ä¸ªæ­¥éª¤")
        return final_result
    
    def _prioritize_files_for_flow_analysis(self, 
                                          files_content: Dict[str, str]) -> List[Tuple[str, str]]:
        """ä¸ºä¸šåŠ¡æµç¨‹åˆ†ææ’åºæ–‡ä»¶ä¼˜å…ˆçº§
        
        Args:
            files_content: æ–‡ä»¶å†…å®¹æ˜ å°„
            
        Returns:
            æŒ‰ä¼˜å…ˆçº§æ’åºçš„æ–‡ä»¶åˆ—è¡¨
        """
        logger.info(f"ğŸ“Š å¼€å§‹ä¸º {len(files_content)} ä¸ªæ–‡ä»¶è®¡ç®—ä¼˜å…ˆçº§")
        
        file_priorities = []
        
        for file_path, content in files_content.items():
            priority = self._calculate_business_flow_priority(file_path, content)
            file_priorities.append((priority, file_path, content))
        
        # æŒ‰ä¼˜å…ˆçº§é™åºæ’åº
        file_priorities.sort(key=lambda x: x[0], reverse=True)
        
        # è®°å½•ä¼˜å…ˆçº§æ’åºç»“æœ
        logger.info("ğŸ“‹ æ–‡ä»¶ä¼˜å…ˆçº§æ’åºå®Œæˆï¼Œå‰5ä¸ªé«˜ä¼˜å…ˆçº§æ–‡ä»¶:")
        for i, (priority, file_path, _) in enumerate(file_priorities[:5]):
            logger.info(f"  {i+1}. {file_path} (ä¼˜å…ˆçº§: {priority})")
        
        if len(file_priorities) > 5:
            logger.info(f"  ... ä»¥åŠå…¶ä»– {len(file_priorities) - 5} ä¸ªæ–‡ä»¶")
        
        # è¿”å›æ–‡ä»¶è·¯å¾„å’Œå†…å®¹çš„å…ƒç»„åˆ—è¡¨
        return [(file_path, content) for _, file_path, content in file_priorities]
    
    def _calculate_business_flow_priority(self, file_path: str, content: str) -> int:
        """è®¡ç®—æ–‡ä»¶åœ¨ä¸šåŠ¡æµç¨‹åˆ†æä¸­çš„ä¼˜å…ˆçº§"""
        priority = 0
        file_name = file_path.lower()
        
        # å·¥å‚æ¨¡å¼æ–‡ä»¶ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
        if any(keyword in file_name for keyword in ['factory', 'manager', 'controller']):
            priority += 1000
        
        # æ ¸å¿ƒä¸šåŠ¡åˆçº¦
        if any(keyword in file_name for keyword in ['claim', 'deposit', 'withdraw', 'transfer']):
            priority += 800
        
        # è®¿é—®æ§åˆ¶æ–‡ä»¶
        if any(keyword in file_name for keyword in ['access', 'auth', 'permission', 'role']):
            priority += 600
        
        # åŸºç¡€åŠŸèƒ½æ–‡ä»¶
        if any(keyword in file_name for keyword in ['base', 'closable', 'pausable']):
            priority += 400
        
        # æ¥å£æ–‡ä»¶
        if file_name.startswith('i') and file_name.endswith('.sol'):
            priority += 200
        
        # åŸºäºå†…å®¹å¤æ‚åº¦
        function_count = content.count('function ')
        event_count = content.count('event ')
        modifier_count = content.count('modifier ')
        
        priority += function_count * 10
        priority += event_count * 5
        priority += modifier_count * 8
        
        # åŸºäºæ–‡ä»¶å¤§å°
        if len(content) > 10000:
            priority += 100
        elif len(content) > 5000:
            priority += 50
        
        return priority
    
    def _analyze_single_file_incremental(self, 
                                        step_id: int,
                                        file_path: str,
                                        file_content: str,
                                        existing_mermaid: str,
                                        project_name: str) -> BusinessFlowStepResult:
        """çœŸæ­£çš„å•æ–‡ä»¶å¢é‡åˆ†æ
        
        Args:
            step_id: æ­¥éª¤ID
            file_path: å½“å‰åˆ†æçš„æ–‡ä»¶è·¯å¾„
            file_content: å½“å‰æ–‡ä»¶å†…å®¹
            existing_mermaid: å·²æœ‰çš„ç´¯ç§¯mermaidå›¾
            project_name: é¡¹ç›®åç§°
            
        Returns:
            æ­¥éª¤åˆ†æç»“æœï¼ŒåŒ…å«æ‰©å±•åçš„mermaidå›¾
        """
        self._log_mermaid_generation_start(f"æ­¥éª¤{step_id}å¢é‡åˆ†æ", file_path)
        
        # æ„å»ºå¢é‡åˆ†æprompt
        prompt = self._build_true_incremental_prompt(
            file_path, file_content, existing_mermaid, step_id, project_name)
        
        # è®¡ç®—tokenä½¿ç”¨é‡
        token_usage = self.token_calculator.calculate_prompt_tokens(prompt, self.model)
        
        # è°ƒç”¨Claudeè¿›è¡Œå¢é‡åˆ†æ
        logger.info(f"ğŸ“¤ å‘é€å¢é‡åˆ†æè¯·æ±‚ - æ–‡ä»¶: {file_path}, æ­¥éª¤: {step_id}")
        analysis_result = ask_claude_for_code_analysis(prompt)
        
        # è§£æåˆ†æç»“æœï¼Œè·å–æ‰©å±•åçš„å®Œæ•´mermaidå›¾
        flow_description, interactions, extended_mermaid = \
            self._parse_incremental_result(analysis_result)
        
        # è®°å½•åˆ†æç»“æœ
        self._log_mermaid_generation_result(
            extended_mermaid, 
            f"æ­¥éª¤{step_id}å¢é‡åˆ†æ-{file_path}",
            step_id
        )
        
        return BusinessFlowStepResult(
            step_id=step_id,
            files_analyzed=[file_path],  # åªåŒ…å«å½“å‰æ–‡ä»¶
            flow_description=flow_description,
            key_interactions=interactions,
            mermaid_fragment=extended_mermaid,  # è¿™æ˜¯ç´¯ç§¯çš„å®Œæ•´å›¾
            token_usage=token_usage
        )
    
    def _build_true_incremental_prompt(self, 
                                      file_path: str,
                                      file_content: str,
                                      existing_mermaid: str,
                                      step_id: int,
                                      project_name: str) -> str:
        """æ„å»ºçœŸæ­£çš„å¢é‡åˆ†æprompt - åŸºäºå·²æœ‰mermaidå›¾æ‰©å±•"""
        
        if step_id == 1:
            # ç¬¬ä¸€ä¸ªæ–‡ä»¶ï¼Œåˆ›å»ºåˆå§‹mermaidå›¾ - é™åˆ¶æ–‡ä»¶å†…å®¹é•¿åº¦
            truncated_content = file_content[:8000] + ("\n... (å†…å®¹å·²æˆªæ–­)" if len(file_content) > 8000 else "")
            
            prompt = f"""
è¯·ä¸º {project_name} é¡¹ç›®åˆ†æç¬¬ä¸€ä¸ªæ–‡ä»¶å¹¶åˆ›å»ºåˆå§‹çš„ä¸šåŠ¡æµç¨‹å›¾ï¼Œ**å¿…é¡»è¦†ç›–æ–‡ä»¶ä¸­çš„æ‰€æœ‰å‡½æ•°**ã€‚

**å½“å‰åˆ†ææ–‡ä»¶: {file_path}**

**æ–‡ä»¶å†…å®¹:**
{truncated_content}

**ğŸ¯ ä»»åŠ¡è¦æ±‚ - 100%å‡½æ•°è¦†ç›–:**
1. **å¿…é¡»ç”ŸæˆsequenceDiagramæ ¼å¼** - ä»¥ `sequenceDiagram` å¼€å¤´
2. **å…¨å‡½æ•°è¦†ç›–åˆ†æ** - åˆ†æ {file_path} ä¸­çš„**æ¯ä¸€ä¸ªå‡½æ•°**ï¼ŒåŒ…æ‹¬ï¼š
   - âœ… **Public/Externalå‡½æ•°** - æ‰€æœ‰å¯¹å¤–æš´éœ²çš„å‡½æ•°
   - âœ… **Getterå‡½æ•°** - æ‰€æœ‰è·å–çŠ¶æ€å˜é‡çš„å‡½æ•°ï¼ˆå¦‚ getValue, getBalance, isActiveï¼‰
   - âœ… **Setterå‡½æ•°** - æ‰€æœ‰è®¾ç½®çŠ¶æ€å˜é‡çš„å‡½æ•°ï¼ˆå¦‚ setValue, setOwner, setConfigï¼‰
   - âœ… **View/Pureå‡½æ•°** - æ‰€æœ‰æŸ¥è¯¢ç±»å‡½æ•°ï¼Œæ— è®ºå¤šç®€å•
   - âœ… **æ„é€ å‡½æ•°** - constructorå‡½æ•°
   - âœ… **äº‹ä»¶è§¦å‘** - æ‰€æœ‰emitè¯­å¥
   - âœ… **ä¿®é¥°ç¬¦å‡½æ•°** - é‡è¦çš„modifieråº”ç”¨
3. **åˆ›å»ºå®Œæ•´çš„Mermaidåºåˆ—å›¾** - **å¿…é¡»ä½¿ç”¨åŸå§‹çš„åˆçº¦åå’Œå‡½æ•°å**
4. **ç¡®ä¿å›¾è¡¨ç»“æ„æ¸…æ™°** - ä¸ºåç»­æ–‡ä»¶æ‰©å±•åšå¥½å‡†å¤‡ï¼Œä½†ä¸èƒ½é—æ¼ä»»ä½•å‡½æ•°

**å…³é”®æ ¼å¼è¦æ±‚ - å¿…é¡»ä¸¥æ ¼éµå®ˆ:**
- **åˆçº¦å**: ä½¿ç”¨æ–‡ä»¶ä¸­çš„åŸå§‹åˆçº¦å (å¦‚: ERC20AssetGateway, PlanFactory, GMEvent)
- **å‡½æ•°å**: ä½¿ç”¨ä»£ç ä¸­çš„å‡†ç¡®å‡½æ•°å (å¦‚: constructor, transfer, approve, confirmJoin)
- **å‚æ•°**: åŒ…å«å‡½æ•°çš„çœŸå®å‚æ•°åå’Œç±»å‹ (å¦‚: address _user, uint256 _amount)
- **è¿”å›å€¼**: æ˜ç¡®æ ‡æ³¨å‡½æ•°è¿”å›å€¼ç±»å‹å’Œå«ä¹‰
- **ä¿®é¥°ç¬¦**: åŒ…å«é‡è¦çš„ä¿®é¥°ç¬¦æ£€æŸ¥ (å¦‚: onlyOwner, requireRole)

**è¾“å‡ºæ ¼å¼:**
## ä¸šåŠ¡æµç¨‹æè¿°
[è¯¦ç»†æè¿° {file_path} çš„**æ‰€æœ‰å‡½æ•°**ä¸šåŠ¡é€»è¾‘ï¼Œä½¿ç”¨åŸå§‹åˆçº¦åå’Œå‡½æ•°å]

## å®Œæ•´Mermaidå›¾
```mermaid
sequenceDiagram
    [åˆ›å»ºè¯¦ç»†çš„åºåˆ—å›¾ï¼Œä¸¥æ ¼ä½¿ç”¨åŸå§‹åˆçº¦åå’Œå‡½æ•°å]
    [**å¿…é¡»åŒ…å«æ–‡ä»¶ä¸­çš„æ‰€æœ‰å‡½æ•°**ï¼ŒåŒ…æ‹¬ç®€å•çš„getter/setter]
    [æ ¼å¼ç¤ºä¾‹: User->>ERC20Token: balanceOf(address owner) returns uint256]
    [æ ¼å¼ç¤ºä¾‹: Owner->>ERC20Token: setOwner(address newOwner)]
```


"""
        else:
            # åç»­æ–‡ä»¶ï¼ŒåŸºäºå·²æœ‰mermaidå›¾æ‰©å±• - æ™ºèƒ½æ§åˆ¶å†…å®¹é•¿åº¦
            truncated_content = file_content[:5000] + ("\n... (å†…å®¹å·²æˆªæ–­)" if len(file_content) > 5000 else "")
            
            # å¦‚æœexisting_mermaidå¤ªé•¿ï¼Œä¹Ÿéœ€è¦é€‚å½“æˆªæ–­æç¤º
            if len(existing_mermaid) > 3000:
                mermaid_preview = existing_mermaid[:3000] + "\n... (å·²æœ‰å›¾è¡¨å†…å®¹è¾ƒé•¿ï¼Œä»…æ˜¾ç¤ºå‰éƒ¨åˆ†)"
            else:
                mermaid_preview = existing_mermaid
            
            prompt = f"""
è¯·ä¸º {project_name} é¡¹ç›®æ‰©å±•ä¸šåŠ¡æµç¨‹å›¾ï¼Œæ·»åŠ æ–°æ–‡ä»¶ {file_path} çš„**æ‰€æœ‰å‡½æ•°**ä¸šåŠ¡é€»è¾‘ã€‚

**å½“å‰è¦æ·»åŠ çš„æ–‡ä»¶: {file_path}**

**æ–°æ–‡ä»¶å†…å®¹:**
{truncated_content}

**å·²æœ‰çš„ä¸šåŠ¡æµç¨‹å›¾:**
```mermaid
{mermaid_preview}
```

**ğŸ¯ å…³é”®ä»»åŠ¡è¦æ±‚ - 100%å‡½æ•°è¦†ç›–:**
1. **å¿…é¡»ä¿æŒsequenceDiagramæ ¼å¼** - ç¡®ä¿è¾“å‡ºä»¥ `sequenceDiagram` å¼€å¤´
2. **ç»å¯¹ä¿ç•™**å·²æœ‰Mermaidå›¾ä¸­çš„æ‰€æœ‰å†…å®¹ï¼Œä¸€ä¸ªäº¤äº’éƒ½ä¸èƒ½ä¸¢å¤±
3. **å…¨å‡½æ•°è¦†ç›–åˆ†æ** - åˆ†ææ–°æ–‡ä»¶ {file_path} ä¸­çš„**æ¯ä¸€ä¸ªå‡½æ•°**
4. **å°†æ–°æ–‡ä»¶çš„æ‰€æœ‰å‡½æ•°ä¸šåŠ¡æµç¨‹æ‰©å±•åˆ°å·²æœ‰å›¾ä¸­**
5. **å¿…é¡»ä½¿ç”¨åŸå§‹çš„åˆçº¦åå’Œå‡½æ•°å**ï¼Œç¡®ä¿æ–°å¢çš„äº¤äº’åŒ…å«å…·ä½“çš„å‡½æ•°åå’Œå‚æ•°
6. **ä¿æŒå›¾è¡¨çš„é€»è¾‘é¡ºåºå’Œæ¸…æ™°ç»“æ„**

**è¾“å‡ºæ ¼å¼:**
## ä¸šåŠ¡æµç¨‹æè¿°
[è¯¦ç»†æè¿° {file_path} çš„**æ‰€æœ‰å‡½æ•°**å¦‚ä½•èå…¥ç°æœ‰ä¸šåŠ¡æµç¨‹ï¼Œä½¿ç”¨åŸå§‹åˆçº¦åå’Œå‡½æ•°å]

## æ‰©å±•åçš„å®Œæ•´Mermaidå›¾
```mermaid
sequenceDiagram
    [åŒ…å«æ‰€æœ‰åŸæœ‰å†…å®¹ + æ–°å¢çš„ {file_path} çš„**æ‰€æœ‰å‡½æ•°**äº¤äº’]
    [ç¡®ä¿æ‰€æœ‰åŸæœ‰çš„äº¤äº’éƒ½å®Œæ•´ä¿ç•™]
    [**å¿…é¡»åŒ…å«æ–°æ–‡ä»¶ä¸­çš„æ‰€æœ‰å‡½æ•°**ï¼ŒåŒ…æ‹¬ç®€å•çš„getter/setter]
```


"""
        
        return prompt
    
    def _finalize_cumulative_mermaid(self, 
                                    project_name: str,
                                    files_content: Dict[str, str],
                                    step_results: List[BusinessFlowStepResult],
                                    cumulative_mermaid: str) -> CompleteBusinessFlowResult:
        """ä¼˜åŒ–æœ€ç»ˆçš„ç´¯ç§¯mermaidå›¾ï¼Œç›´æ¥ä½¿ç”¨AIç”Ÿæˆçš„å®Œæ•´ç»“æœ"""
        
        logger.info("ğŸ”§ ä¼˜åŒ–æœ€ç»ˆçš„ç´¯ç§¯mermaidå›¾")
        self._log_mermaid_generation_start("æœ€ç»ˆä¼˜åŒ–", f"ç´¯ç§¯å›¾é•¿åº¦: {len(cumulative_mermaid)}å­—ç¬¦")
        
        # æ„å»ºæœ€ç»ˆä¼˜åŒ–prompt
        final_prompt = self._build_final_optimization_prompt(project_name, cumulative_mermaid)
        
        # è®¡ç®—tokenä½¿ç”¨é‡
        token_usage = self.token_calculator.calculate_prompt_tokens(final_prompt, self.model)
        logger.info(f"ğŸ“Š æœ€ç»ˆä¼˜åŒ–Tokenä½¿ç”¨é¢„ä¼°: {token_usage.total_tokens:,}")
        
        # è°ƒç”¨Claudeè¿›è¡Œæœ€ç»ˆä¼˜åŒ–
        logger.info("ğŸ“¤ å‘é€æœ€ç»ˆä¼˜åŒ–è¯·æ±‚")
        final_analysis = ask_claude_for_code_analysis(final_prompt)
        
        # ç›´æ¥ä½¿ç”¨AIç”Ÿæˆçš„å®Œæ•´ç»“æœ
        original_length = len(cumulative_mermaid)
        
        if final_analysis and len(final_analysis) > 100:  # åŸºæœ¬çš„é•¿åº¦æ£€æŸ¥
            final_mermaid = final_analysis
            business_summary = final_analysis  # ç›´æ¥ä½¿ç”¨å®Œæ•´ç»“æœä½œä¸ºæ€»ç»“
            final_length = len(final_mermaid)
            logger.info(f"âœ… ç›´æ¥ä½¿ç”¨AIä¼˜åŒ–ç»“æœï¼Œé•¿åº¦: {final_length}å­—ç¬¦")
        else:
            # å¦‚æœAIè¿”å›ç»“æœå¤ªçŸ­æˆ–ä¸ºç©ºï¼Œä½¿ç”¨åŸå§‹ç´¯ç§¯å›¾
            logger.warning("âš ï¸  AIè¿”å›ç»“æœå¤ªçŸ­æˆ–ä¸ºç©ºï¼Œä½¿ç”¨åŸå§‹ç´¯ç§¯å›¾")
            final_mermaid = cumulative_mermaid
            business_summary = f"{project_name}é¡¹ç›®ä¸šåŠ¡æµç¨‹åˆ†æå®Œæˆ"
            final_length = len(final_mermaid)
        
        # è®°å½•æœ€ç»ˆä¼˜åŒ–æ•ˆæœ
        self._log_mermaid_optimization(original_length, final_length, "æœ€ç»ˆä¼˜åŒ–")
        self._log_mermaid_generation_result(final_mermaid, "é¡¹ç›®æœ€ç»ˆåˆ†æ")
        
        # è®¡ç®—æ€»tokenä½¿ç”¨é‡
        total_token_usage = self._calculate_total_token_usage(step_results, token_usage)
        
        # è®°å½•æœ€ç»ˆç»Ÿè®¡
        logger.info(f"ğŸ‰ é¡¹ç›® {project_name} åˆ†æå®Œæˆ!")
        logger.info(f"ğŸ“ˆ æœ€ç»ˆç»Ÿè®¡ - æ–‡ä»¶æ•°: {len(files_content)}, åˆ†ææ­¥éª¤: {len(step_results)}")
        logger.info(f"ğŸ’° æ€»Tokenæ¶ˆè€—: {total_token_usage.total_tokens:,}")
        logger.info(f"ğŸ“ æœ€ç»ˆMermaidå›¾é•¿åº¦: {len(final_mermaid):,}å­—ç¬¦")
        
        return CompleteBusinessFlowResult(
            project_name=project_name,
            total_files=len(files_content),
            analysis_strategy="incremental",
            analysis_steps=step_results,
            final_mermaid_graph=final_mermaid,
            business_summary=business_summary,
            folder_analyses={},
            global_mermaid_graph=final_mermaid,
            total_token_usage=total_token_usage
        )
    
    def _build_final_optimization_prompt(self, 
                                        project_name: str,
                                        cumulative_mermaid: str) -> str:
        """æ„å»ºæœ€ç»ˆä¼˜åŒ–prompt - ä¼˜åŒ–ç´¯ç§¯çš„mermaidå›¾"""
        
        newline = '\n'  # å®šä¹‰æ¢è¡Œç¬¦å˜é‡ï¼Œé¿å…f-stringä¸­çš„åæ–œæ é—®é¢˜
        
        prompt = f"""
è¯·ä¼˜åŒ– {project_name} é¡¹ç›®çš„å®Œæ•´ä¸šåŠ¡æµç¨‹å›¾ï¼Œ**ç¡®ä¿è¦†ç›–æ‰€æœ‰å‡½æ•°ï¼ŒåŒ…æ‹¬getter/setter**ï¼ŒåŒæ—¶ä¿æŒå›¾è¡¨æ¸…æ™°ã€é€»è¾‘è¿è´¯ã€‚

**å½“å‰çš„å®Œæ•´ä¸šåŠ¡æµç¨‹å›¾:**
```mermaid
{cumulative_mermaid}
```

**ğŸ¯ ä¼˜åŒ–ä»»åŠ¡è¦æ±‚:**
1. **å¿…é¡»ä¿æŒsequenceDiagramæ ¼å¼** - ç¡®ä¿è¾“å‡ºä»¥ `sequenceDiagram` å¼€å¤´
2. **ä¿ç•™æ‰€æœ‰ç°æœ‰å†…å®¹** - ç»å¯¹ä¸èƒ½åˆ é™¤ä»»ä½•participantæˆ–äº¤äº’ï¼ŒåŒ…æ‹¬æ‰€æœ‰getter/setterå‡½æ•°
3. **éªŒè¯å‡½æ•°è¦†ç›–å®Œæ•´æ€§** - ç¡®ä¿åŒ…å«äº†æ‰€æœ‰ç±»å‹çš„å‡½æ•°
4. **ä¼˜åŒ–äº¤äº’çš„é€»è¾‘é¡ºåº**ï¼Œç¡®ä¿ä¸šåŠ¡æµç¨‹çš„æ—¶åºåˆç†
5. **æ·»åŠ é€‚å½“çš„æ³¨é‡Šå’Œåˆ†ç»„**ï¼ˆä½¿ç”¨ %% æ³¨é‡Šå’Œ Noteï¼‰
6. **ä¿æŒæ‰€æœ‰åŸå§‹åˆçº¦åå’Œå‡½æ•°å** - ç¡®ä¿æ‰€æœ‰å‡½æ•°åå’Œå‚æ•°éƒ½å‡†ç¡®æ— è¯¯
7. **æ£€æŸ¥å¹¶ä¿®æ­£å¯èƒ½çš„è¯­æ³•é”™è¯¯**

**è¾“å‡ºæ ¼å¼:**
## ä¸šåŠ¡æµç¨‹æ€»ç»“
[ç®€è¦æ€»ç»“ {project_name} é¡¹ç›®çš„æ ¸å¿ƒä¸šåŠ¡æµç¨‹ï¼Œ**åŒ…æ‹¬æ‰€æœ‰å‡½æ•°ç±»å‹**ï¼Œä½¿ç”¨åŸå§‹åˆçº¦åå’Œå‡½æ•°å]

## ä¼˜åŒ–åçš„å®Œæ•´ä¸šåŠ¡æµç¨‹å›¾
```mermaid
{cumulative_mermaid.split(newline)[0] if cumulative_mermaid else 'sequenceDiagram'}
    [ä¿ç•™æ‰€æœ‰åŸæœ‰participantå’Œäº¤äº’ï¼ŒåŒ…æ‹¬æ‰€æœ‰åŸå§‹åˆçº¦åå’Œå‡½æ•°å]
    [**ç¡®ä¿åŒ…å«æ‰€æœ‰å‡½æ•°ï¼ŒåŒ…æ‹¬ç®€å•çš„getter/setter**]
    [ä¼˜åŒ–é¡ºåºå’Œç»“æ„ï¼Œæ·»åŠ é€‚å½“æ³¨é‡Š]
    [ç¡®ä¿è¯­æ³•æ­£ç¡®ï¼Œé€»è¾‘æ¸…æ™°ï¼Œä½†ç»ä¸ä¿®æ”¹åˆçº¦åå’Œå‡½æ•°å]
```

**ğŸ”¥ é‡è¦æé†’:**
- **ç»å¯¹ä¸èƒ½åˆ é™¤ä»»ä½•å‡½æ•°äº¤äº’** - åŒ…æ‹¬æœ€ç®€å•çš„getter/setterå‡½æ•°
- åªèƒ½ä¼˜åŒ–ç»“æ„å’Œé¡ºåºï¼Œä¸èƒ½åˆ é™¤ä»»ä½•ç°æœ‰å†…å®¹
- **ç»å¯¹ä¸èƒ½ä¿®æ”¹ä»»ä½•åˆçº¦åã€å‡½æ•°åæˆ–å‚æ•°å**
- ç¡®ä¿æ‰€æœ‰åŸæœ‰çš„äº¤äº’éƒ½å®Œæ•´ä¿ç•™ï¼ŒåŒ…æ‹¬åŸå§‹çš„å‘½å
- ä¼˜åŒ–åº”è¯¥è®©å›¾è¡¨æ›´æ¸…æ™°ï¼Œè€Œä¸æ˜¯æ›´ç®€åŒ–
"""
        
        return prompt
    
    def _extract_final_mermaid(self, analysis_result: str) -> str:
        """ç›´æ¥ä½¿ç”¨AIç”Ÿæˆçš„å®Œæ•´ç»“æœ"""
        
        logger.info("ğŸ” ç›´æ¥ä½¿ç”¨æœ€ç»ˆAIç”Ÿæˆçš„å®Œæ•´ç»“æœ")
        logger.info(f"âœ… ä½¿ç”¨å®Œæ•´ç»“æœï¼Œé•¿åº¦: {len(analysis_result)}å­—ç¬¦")
        
        return analysis_result
    
    def _extract_business_summary(self, analysis_result: str) -> str:
        """ç›´æ¥ä½¿ç”¨AIç”Ÿæˆçš„å®Œæ•´ç»“æœä½œä¸ºä¸šåŠ¡æ€»ç»“"""
        
        logger.info("ğŸ” ç›´æ¥ä½¿ç”¨AIç”Ÿæˆçš„å®Œæ•´ç»“æœä½œä¸ºä¸šåŠ¡æ€»ç»“")
        logger.info(f"âœ… ä½¿ç”¨å®Œæ•´ç»“æœä½œä¸ºæ€»ç»“ï¼Œé•¿åº¦: {len(analysis_result)}å­—ç¬¦")
        
        return analysis_result
    

    
    def _calculate_total_token_usage(self, 
                                   step_results: List[BusinessFlowStepResult],
                                   final_usage: TokenUsage) -> TokenUsage:
        """è®¡ç®—æ€»çš„tokenä½¿ç”¨é‡"""
        
        total_input = sum(step.token_usage.input_tokens for step in step_results) + final_usage.input_tokens
        total_output = sum(step.token_usage.estimated_output_tokens for step in step_results) + final_usage.estimated_output_tokens
        total_tokens = total_input + total_output
        
        return TokenUsage(
            input_tokens=total_input,
            estimated_output_tokens=total_output,
            total_tokens=total_tokens,
            is_within_limit=True,  # æ€»è®¡ä¸æ£€æŸ¥é™åˆ¶
            model_limit=final_usage.model_limit,
            recommendation=f"æ€»è®¡ä½¿ç”¨ {total_tokens:,} tokens"
        )
    
    def _merge_folder_diagrams(self, 
                             folder_analyses: Dict[str, FolderAnalysisResult],
                             project_name: str) -> str:
        """åˆå¹¶å¤šä¸ªæ–‡ä»¶å¤¹çš„diagramç”Ÿæˆå…¨å±€ä¸šåŠ¡æµå›¾ï¼Œç›´æ¥ä½¿ç”¨AIå®Œæ•´ç»“æœ"""
        
        if not folder_analyses:
            logger.warning("âš ï¸  æ²¡æœ‰æ–‡ä»¶å¤¹åˆ†æç»“æœï¼Œæ— æ³•ç”Ÿæˆå…¨å±€å›¾")
            return ""
        
        self._log_mermaid_generation_start("å¤šæ–‡ä»¶å¤¹diagramåˆå¹¶", f"{len(folder_analyses)}ä¸ªæ–‡ä»¶å¤¹")
        
        # æ”¶é›†æ‰€æœ‰æ–‡ä»¶å¤¹çš„diagramå†…å®¹
        folder_diagrams = {}
        for folder_path, folder_result in folder_analyses.items():
            if folder_result.folder_mermaid_graph:
                folder_diagrams[folder_path] = {
                    'diagram': folder_result.folder_mermaid_graph,
                    'summary': folder_result.folder_summary,
                    'files_count': folder_result.files_count
                }
        
        if not folder_diagrams:
            logger.warning("âš ï¸  æ‰€æœ‰æ–‡ä»¶å¤¹éƒ½æ²¡æœ‰ç”Ÿæˆæœ‰æ•ˆçš„diagram")
            return ""
        
        # æ„å»ºåˆå¹¶prompt
        prompt = self._build_folder_merge_prompt(folder_diagrams, project_name)
        
        try:
            logger.info("ğŸ“¤ å‘é€å¤šæ–‡ä»¶å¤¹diagramåˆå¹¶è¯·æ±‚")
            analysis_result = ask_claude_for_code_analysis(prompt)
            
            # ç›´æ¥ä½¿ç”¨AIç”Ÿæˆçš„å®Œæ•´ç»“æœ
            if analysis_result:
                logger.info(f"âœ… ç›´æ¥ä½¿ç”¨AIåˆå¹¶ç»“æœï¼Œé•¿åº¦: {len(analysis_result)}å­—ç¬¦")
                self._log_mermaid_generation_result(analysis_result, "å¤šæ–‡ä»¶å¤¹åˆå¹¶ç»“æœ")
                return analysis_result
            else:
                logger.warning("âš ï¸  AIè¿”å›ç©ºç»“æœï¼Œä½¿ç”¨ç®€åŒ–åˆå¹¶ç­–ç•¥")
                return self._simple_merge_diagrams(folder_diagrams, project_name)
            
        except Exception as e:
            logger.warning(f"âŒ å¤šæ–‡ä»¶å¤¹diagramåˆå¹¶å¤±è´¥: {e}")
            return self._simple_merge_diagrams(folder_diagrams, project_name)

    def _build_folder_merge_prompt(self, 
                                 folder_diagrams: Dict[str, Dict],
                                 project_name: str) -> str:
        """æ„å»ºæ–‡ä»¶å¤¹diagramåˆå¹¶prompt"""
        
        diagrams_content = ""
        for folder_path, folder_data in folder_diagrams.items():
            diagrams_content += f"""
**æ–‡ä»¶å¤¹: {folder_path}** ({folder_data['files_count']} ä¸ªæ–‡ä»¶)
åŠŸèƒ½æ¦‚è¿°: {folder_data['summary'][:200]}...

```mermaid
{folder_data['diagram']}
```

---
"""
        
        prompt = f"""
è¯·å°† {project_name} é¡¹ç›®çš„å¤šä¸ªæ–‡ä»¶å¤¹ä¸šåŠ¡æµç¨‹å›¾åˆå¹¶æˆä¸€ä¸ªå®Œæ•´çš„é¡¹ç›®çº§ä¸šåŠ¡æµç¨‹å›¾ã€‚

**å„æ–‡ä»¶å¤¹çš„ä¸šåŠ¡æµç¨‹å›¾:**
{diagrams_content}

**åˆå¹¶ä»»åŠ¡è¦æ±‚:**
1. **ç”ŸæˆsequenceDiagramæ ¼å¼** - å¿…é¡»ä»¥ `sequenceDiagram` å¼€å¤´
2. **ä¿ç•™æ ¸å¿ƒä¸šåŠ¡æµç¨‹** - æå–å„æ–‡ä»¶å¤¹çš„ä¸»è¦ä¸šåŠ¡é€»è¾‘å’Œäº¤äº’
3. **å»ºç«‹è·¨æ–‡ä»¶å¤¹è¿æ¥** - è¯†åˆ«æ–‡ä»¶å¤¹é—´çš„è°ƒç”¨å…³ç³»å’Œæ•°æ®æµ
4. **ä½¿ç”¨å…·ä½“åç§°** - ä¿æŒåŸå§‹åˆçº¦åå’Œå‡½æ•°åï¼Œé¿å…é€šç”¨åç§°
5. **ç®€åŒ–é‡å¤äº¤äº’** - åˆå¹¶ç›¸ä¼¼çš„äº¤äº’ï¼Œçªå‡ºæ ¸å¿ƒæµç¨‹
6. **ä¿æŒé€»è¾‘æ¸…æ™°** - ç¡®ä¿åˆå¹¶åçš„æµç¨‹å›¾é€»è¾‘è¿è´¯

**è¾“å‡ºæ ¼å¼:**
## é¡¹ç›®æ•´ä½“ä¸šåŠ¡æµç¨‹è¯´æ˜
[ç®€è¦è¯´æ˜åˆå¹¶åçš„æ•´ä½“ä¸šåŠ¡æµç¨‹]

## åˆå¹¶åçš„å®Œæ•´ä¸šåŠ¡æµç¨‹å›¾
```mermaid
sequenceDiagram
    [ç”Ÿæˆå®Œæ•´çš„é¡¹ç›®çº§ä¸šåŠ¡æµç¨‹å›¾]
    [åŒ…å«å„æ–‡ä»¶å¤¹çš„æ ¸å¿ƒäº¤äº’]
    [å»ºç«‹è·¨æ–‡ä»¶å¤¹çš„ä¸šåŠ¡è¿æ¥]
    [ä½¿ç”¨å…·ä½“çš„åˆçº¦åå’Œå‡½æ•°å]
```

è¯·ç¡®ä¿ç”Ÿæˆçš„æ˜¯ä¸€ä¸ªç»Ÿä¸€ã€è¿è´¯çš„é¡¹ç›®çº§ä¸šåŠ¡æµç¨‹å›¾ã€‚
"""
        
        return prompt

    def _simple_merge_diagrams(self, 
                             folder_diagrams: Dict[str, Dict],
                             project_name: str) -> str:
        """ç®€å•åˆå¹¶å¤šä¸ªdiagramçš„å¤‡ç”¨æ–¹æ³•ï¼Œç›´æ¥æ–‡æœ¬æ‹¼æ¥"""
        
        logger.info("ğŸ”„ ä½¿ç”¨ç®€åŒ–ç­–ç•¥ç›´æ¥æ‹¼æ¥å¤šä¸ªdiagram")
        
        # ç®€å•çš„æ–‡æœ¬æ‹¼æ¥
        merged_content = f"{project_name} é¡¹ç›®æ•´ä½“ä¸šåŠ¡æµç¨‹ (ç®€åŒ–åˆå¹¶)\n\n"
        
        for folder_path, folder_data in folder_diagrams.items():
            folder_name = Path(folder_path).name
            merged_content += f"=== {folder_name} æ–‡ä»¶å¤¹ä¸šåŠ¡æµç¨‹ ===\n"
            merged_content += f"æ–‡ä»¶æ•°: {folder_data['files_count']}\n"
            merged_content += f"åŠŸèƒ½: {folder_data['summary'][:100]}...\n"
            merged_content += f"è¯¦ç»†æµç¨‹:\n{folder_data['diagram']}\n\n"
        
        logger.info(f"âœ… ç®€åŒ–æ‹¼æ¥å®Œæˆï¼Œæ€»é•¿åº¦: {len(merged_content)}å­—ç¬¦")
        self._log_mermaid_generation_result(merged_content, "ç®€åŒ–åˆå¹¶ç»“æœ")
        
        return merged_content

# ä¾¿æ·å‡½æ•°
def analyze_business_flow(files_content: Dict[str, str], 
                         project_name: str,
                         model: str = "claude-3-5-sonnet-20241022") -> CompleteBusinessFlowResult:
    """ä¾¿æ·çš„ä¸šåŠ¡æµç¨‹åˆ†æå‡½æ•°
    
    Args:
        files_content: æ–‡ä»¶å†…å®¹æ˜ å°„
        project_name: é¡¹ç›®åç§°
        model: ä½¿ç”¨çš„AIæ¨¡å‹
        
    Returns:
        å®Œæ•´çš„ä¸šåŠ¡æµç¨‹åˆ†æç»“æœ
    """
    logger.info(f"ğŸš€ å¯åŠ¨ä¾¿æ·ä¸šåŠ¡æµç¨‹åˆ†æ: {project_name}")
    logger.info(f"ğŸ“‹ åˆ†æå‚æ•° - æ–‡ä»¶æ•°: {len(files_content)}, æ¨¡å‹: {model}")
    
    analyzer = BusinessFlowAnalyzer(model)
    result = analyzer.analyze_business_flow_incremental(files_content, project_name)
    
    logger.info(f"ğŸ‰ ä¾¿æ·åˆ†æå®Œæˆ - é¡¹ç›®: {project_name}, æœ€ç»ˆå›¾è¡¨é•¿åº¦: {len(result.final_mermaid_graph)}å­—ç¬¦")
    
    return result

def analyze_business_flow_from_path(project_path: str, 
                                  project_name: str = None,
                                  file_extensions: List[str] = ['.sol', '.py', '.js', '.ts'],
                                  model: str = "claude-3-5-sonnet-20241022") -> CompleteBusinessFlowResult:
    """ä»é¡¹ç›®è·¯å¾„åˆ†æä¸šåŠ¡æµç¨‹
    
    Args:
        project_path: é¡¹ç›®è·¯å¾„
        project_name: é¡¹ç›®åç§°
        file_extensions: è¦åˆ†æçš„æ–‡ä»¶æ‰©å±•å
        model: ä½¿ç”¨çš„AIæ¨¡å‹
        
    Returns:
        å®Œæ•´çš„ä¸šåŠ¡æµç¨‹åˆ†æç»“æœ
    """
    from pathlib import Path
    
    project_path = Path(project_path)
    if not project_name:
        project_name = project_path.name
    
    logger.info(f"ğŸš€ å¯åŠ¨è·¯å¾„ä¸šåŠ¡æµç¨‹åˆ†æ: {project_name}")
    logger.info(f"ğŸ“‚ é¡¹ç›®è·¯å¾„: {project_path}")
    logger.info(f"ğŸ” æ–‡ä»¶æ‰©å±•å: {file_extensions}")
    logger.info(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {model}")
    
    # è¯»å–é¡¹ç›®æ–‡ä»¶
    files_content = {}
    total_files_found = 0
    
    for ext in file_extensions:
        ext_files = list(project_path.glob(f"*{ext}"))
        total_files_found += len(ext_files)
        logger.info(f"ğŸ“„ å‘ç° {len(ext_files)} ä¸ª {ext} æ–‡ä»¶")
        
        for file_path in ext_files:
            if file_path.is_file():
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                        relative_path = str(file_path.relative_to(project_path))
                        files_content[relative_path] = content
                        logger.info(f"âœ… æˆåŠŸè¯»å–æ–‡ä»¶: {relative_path} ({len(content):,} å­—ç¬¦)")
                except Exception as e:
                    logger.warning(f"âŒ è¯»å–æ–‡ä»¶ {file_path} å¤±è´¥: {e}")
    
    logger.info(f"ğŸ“Š æ–‡ä»¶è¯»å–å®Œæˆ - æ€»è®¡å‘ç°: {total_files_found} ä¸ªæ–‡ä»¶, æˆåŠŸè¯»å–: {len(files_content)} ä¸ªæ–‡ä»¶")
    
    if not files_content:
        logger.warning("âš ï¸  æ²¡æœ‰æ‰¾åˆ°å¯åˆ†æçš„æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥è·¯å¾„å’Œæ–‡ä»¶æ‰©å±•å")
        # è¿”å›ç©ºç»“æœ
        return CompleteBusinessFlowResult(
            project_name=project_name,
            total_files=0,
            analysis_strategy="path_based",
            analysis_steps=[],
            final_mermaid_graph="",
            business_summary="æœªæ‰¾åˆ°å¯åˆ†æçš„æ–‡ä»¶",
            folder_analyses={},
            global_mermaid_graph="",
            total_token_usage=TokenUsage(0, 0, 0, True, 200000, "æ— æ–‡ä»¶å¯åˆ†æ")
        )
    
    result = analyze_business_flow(files_content, project_name, model)
    
    logger.info(f"ğŸ‰ è·¯å¾„åˆ†æå®Œæˆ - é¡¹ç›®: {project_name}")
    logger.info(f"ğŸ“ˆ åˆ†æç»Ÿè®¡ - å¤„ç†æ–‡ä»¶: {len(files_content)}, åˆ†ææ­¥éª¤: {len(result.analysis_steps)}")
    logger.info(f"ğŸ’° Tokenæ¶ˆè€—: {result.total_token_usage.total_tokens:,}")
    logger.info(f"ğŸ“ æœ€ç»ˆå›¾è¡¨: {len(result.final_mermaid_graph):,} å­—ç¬¦")
    
    return result