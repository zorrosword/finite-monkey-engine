# Tokenè®¡ç®—å™¨ - é¢„ä¼°APIè°ƒç”¨çš„tokenä½¿ç”¨é‡
# Token Calculator - Estimate token usage for API calls

import re
import logging
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass

logger = logging.getLogger(__name__)

@dataclass
class TokenUsage:
    """Tokenä½¿ç”¨æƒ…å†µ"""
    input_tokens: int           # è¾“å…¥tokenæ•°é‡
    estimated_output_tokens: int # é¢„ä¼°è¾“å‡ºtokenæ•°é‡
    total_tokens: int          # æ€»tokenæ•°é‡
    is_within_limit: bool      # æ˜¯å¦åœ¨é™åˆ¶èŒƒå›´å†…
    model_limit: int           # æ¨¡å‹tokené™åˆ¶
    recommendation: str        # å»ºè®®

class TokenCalculator:
    """Tokenè®¡ç®—å™¨ - ä¼°ç®—APIè°ƒç”¨çš„tokenä½¿ç”¨é‡"""
    
    # ä¸åŒæ¨¡å‹çš„tokené™åˆ¶
    MODEL_LIMITS = {
        'claude-3-5-sonnet-20241022': 200000,  # Claude 3.5 Sonnet (ä¸»è¦æ¨¡å‹)
        'claude-sonnet-4-20250514': 200000,    # Claude 4 Sonnet (å¤‡ç”¨)
        'gpt-4.1': 128000,                     # GPT-4 Turbo
        'gpt-4o-mini': 128000,                 # GPT-4o mini
        'gpt-4o': 128000,                      # GPT-4o
        'deepseek-reasoner': 32000,            # DeepSeek
        'default': 8000                        # é»˜è®¤ä¿å®ˆå€¼
    }
    
    # è¾“å‡ºtokenä¼°ç®—å€æ•°ï¼ˆåŸºäºç»éªŒï¼‰
    OUTPUT_TOKEN_MULTIPLIERS = {
        'project_overview': 0.3,    # æ¦‚è§ˆåˆ†æè¾“å‡ºç›¸å¯¹è¾ƒå°‘
        'module_detail': 0.5,       # æ¨¡å—è¯¦ç»†åˆ†æè¾“å‡ºä¸­ç­‰
        'dependency_analysis': 0.2,  # ä¾èµ–åˆ†æè¾“å‡ºè¾ƒå°‘
        'mermaid_generation': 0.4,  # Mermaidå›¾ç”Ÿæˆä¸­ç­‰è¾“å‡º
        'smart_query': 0.6,         # æ™ºèƒ½æŸ¥è¯¢å¯èƒ½è¾“å‡ºè¾ƒå¤š
        'default': 0.3              # é»˜è®¤å€æ•°
    }
    
    def __init__(self):
        """åˆå§‹åŒ–tokenè®¡ç®—å™¨"""
        logger.info("åˆå§‹åŒ–Tokenè®¡ç®—å™¨")
    
    def estimate_tokens(self, text: str) -> int:
        """ä¼°ç®—æ–‡æœ¬çš„tokenæ•°é‡
        
        ä½¿ç”¨ç®€åŒ–çš„tokenä¼°ç®—æ–¹æ³•ï¼š
        - è‹±æ–‡ï¼šçº¦4ä¸ªå­—ç¬¦ = 1ä¸ªtoken
        - ä¸­æ–‡ï¼šçº¦1.5ä¸ªå­—ç¬¦ = 1ä¸ªtoken
        - ä»£ç ï¼šçº¦3ä¸ªå­—ç¬¦ = 1ä¸ªtoken
        
        Args:
            text: è¦ä¼°ç®—çš„æ–‡æœ¬
            
        Returns:
            é¢„ä¼°çš„tokenæ•°é‡
        """
        if not text:
            return 0
        
        # ç»Ÿè®¡ä¸åŒç±»å‹çš„å­—ç¬¦
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
        english_chars = len(re.findall(r'[a-zA-Z]', text))
        code_chars = len(re.findall(r'[{}()[\];=+\-*/<>]', text))
        other_chars = len(text) - chinese_chars - english_chars - code_chars
        
        # æŒ‰ä¸åŒè§„åˆ™è®¡ç®—token
        tokens = 0
        tokens += chinese_chars / 1.5      # ä¸­æ–‡å­—ç¬¦
        tokens += english_chars / 4        # è‹±æ–‡å­—ç¬¦
        tokens += code_chars / 3           # ä»£ç å­—ç¬¦
        tokens += other_chars / 4          # å…¶ä»–å­—ç¬¦
        
        return int(tokens)
    
    def calculate_prompt_tokens(self, prompt: str, model: str = "default") -> TokenUsage:
        """è®¡ç®—promptçš„tokenä½¿ç”¨æƒ…å†µ
        
        Args:
            prompt: è¦åˆ†æçš„prompt
            model: ä½¿ç”¨çš„æ¨¡å‹åç§°
            
        Returns:
            Tokenä½¿ç”¨æƒ…å†µ
        """
        input_tokens = self.estimate_tokens(prompt)
        model_limit = self.MODEL_LIMITS.get(model, self.MODEL_LIMITS['default'])
        
        # é¢„ä¼°è¾“å‡ºtokenï¼ˆä¿å®ˆä¼°ç®—ï¼Œç•™å‡ºè¶³å¤Ÿç©ºé—´ï¼‰
        estimated_output = min(input_tokens * 0.3, 4000)  # æœ€å¤š4000ä¸ªè¾“å‡ºtoken
        total_tokens = input_tokens + estimated_output
        
        # åˆ¤æ–­æ˜¯å¦åœ¨é™åˆ¶èŒƒå›´å†…ï¼ˆç•™20%çš„å®‰å…¨è¾¹è·ï¼‰
        safe_limit = int(model_limit * 0.8)
        is_within_limit = total_tokens <= safe_limit
        
        # ç”Ÿæˆå»ºè®®
        recommendation = self._generate_recommendation(
            input_tokens, total_tokens, model_limit, is_within_limit)
        
        return TokenUsage(
            input_tokens=input_tokens,
            estimated_output_tokens=int(estimated_output),
            total_tokens=total_tokens,
            is_within_limit=is_within_limit,
            model_limit=model_limit,
            recommendation=recommendation
        )
    
    def calculate_files_tokens(self, files_content: Dict[str, str], 
                             analysis_type: str = "default",
                             model: str = "default") -> TokenUsage:
        """è®¡ç®—æ–‡ä»¶å†…å®¹çš„tokenä½¿ç”¨æƒ…å†µ
        
        Args:
            files_content: æ–‡ä»¶å†…å®¹æ˜ å°„
            analysis_type: åˆ†æç±»å‹
            model: ä½¿ç”¨çš„æ¨¡å‹
            
        Returns:
            Tokenä½¿ç”¨æƒ…å†µ
        """
        # è®¡ç®—æ‰€æœ‰æ–‡ä»¶çš„tokenæ€»æ•°
        total_content_tokens = 0
        for file_path, content in files_content.items():
            file_tokens = self.estimate_tokens(content)
            total_content_tokens += file_tokens
            logger.debug(f"æ–‡ä»¶ {file_path}: {file_tokens} tokens")
        
        # æ·»åŠ promptæ¨¡æ¿çš„tokenï¼ˆä¼°ç®—ï¼‰
        prompt_template_tokens = 1000  # ç³»ç»Ÿpromptå’ŒæŒ‡ä»¤çš„ä¼°ç®—token
        
        input_tokens = total_content_tokens + prompt_template_tokens
        
        # æ ¹æ®åˆ†æç±»å‹ä¼°ç®—è¾“å‡ºtoken
        output_multiplier = self.OUTPUT_TOKEN_MULTIPLIERS.get(
            analysis_type, self.OUTPUT_TOKEN_MULTIPLIERS['default'])
        estimated_output = int(input_tokens * output_multiplier)
        
        # é™åˆ¶æœ€å¤§è¾“å‡ºtoken
        max_output = 6000  # è®¾ç½®æœ€å¤§è¾“å‡ºé™åˆ¶
        estimated_output = min(estimated_output, max_output)
        
        total_tokens = input_tokens + estimated_output
        
        model_limit = self.MODEL_LIMITS.get(model, self.MODEL_LIMITS['default'])
        safe_limit = int(model_limit * 0.8)
        is_within_limit = total_tokens <= safe_limit
        
        recommendation = self._generate_recommendation(
            input_tokens, total_tokens, model_limit, is_within_limit)
        
        return TokenUsage(
            input_tokens=input_tokens,
            estimated_output_tokens=estimated_output,
            total_tokens=total_tokens,
            is_within_limit=is_within_limit,
            model_limit=model_limit,
            recommendation=recommendation
        )
    
    def suggest_batch_size(self, files_content: Dict[str, str], 
                          model: str = "default",
                          analysis_type: str = "default") -> Tuple[int, List[List[str]]]:
        """å»ºè®®æ‰¹æ¬¡å¤§å°å’Œæ–‡ä»¶åˆ†ç»„
        
        Args:
            files_content: æ–‡ä»¶å†…å®¹æ˜ å°„
            model: ä½¿ç”¨çš„æ¨¡å‹
            analysis_type: åˆ†æç±»å‹
            
        Returns:
            å»ºè®®çš„æ‰¹æ¬¡å¤§å°å’Œæ–‡ä»¶åˆ†ç»„
        """
        model_limit = self.MODEL_LIMITS.get(model, self.MODEL_LIMITS['default'])
        safe_limit = int(model_limit * 0.6)  # ä½¿ç”¨60%çš„é™åˆ¶ä½œä¸ºå®‰å…¨é˜ˆå€¼
        
        # è®¡ç®—æ¯ä¸ªæ–‡ä»¶çš„tokenæ•°
        file_tokens = {}
        for file_path, content in files_content.items():
            file_tokens[file_path] = self.estimate_tokens(content)
        
        # æŒ‰tokenæ•°é‡æ’åºæ–‡ä»¶
        sorted_files = sorted(file_tokens.items(), key=lambda x: x[1])
        
        # æ™ºèƒ½åˆ†ç»„
        batches = []
        current_batch = []
        current_tokens = 1000  # é¢„ç•™prompt token
        
        for file_path, tokens in sorted_files:
            # æ£€æŸ¥æ·»åŠ è¿™ä¸ªæ–‡ä»¶æ˜¯å¦ä¼šè¶…é™
            if current_tokens + tokens > safe_limit:
                # å¦‚æœå½“å‰æ‰¹æ¬¡ä¸ä¸ºç©ºï¼Œä¿å­˜å®ƒ
                if current_batch:
                    batches.append(current_batch)
                    current_batch = []
                    current_tokens = 1000
                
                # å¦‚æœå•ä¸ªæ–‡ä»¶å°±è¶…é™ï¼Œéœ€è¦æˆªæ–­
                if tokens > safe_limit:
                    logger.warning(f"æ–‡ä»¶ {file_path} å•ç‹¬å°±è¶…è¿‡tokené™åˆ¶ï¼Œå°†è¢«æˆªæ–­")
                    current_batch.append(file_path)
                    batches.append(current_batch)
                    current_batch = []
                    current_tokens = 1000
                else:
                    current_batch.append(file_path)
                    current_tokens += tokens
            else:
                current_batch.append(file_path)
                current_tokens += tokens
        
        # æ·»åŠ æœ€åä¸€ä¸ªæ‰¹æ¬¡
        if current_batch:
            batches.append(current_batch)
        
        optimal_batch_size = sum(len(batch) for batch in batches) // len(batches) if batches else 1
        
        logger.info(f"å»ºè®®æ‰¹æ¬¡å¤§å°: {optimal_batch_size}, æ€»æ‰¹æ¬¡æ•°: {len(batches)}")
        return optimal_batch_size, batches
    
    def truncate_content_by_tokens(self, content: str, max_tokens: int) -> str:
        """æŒ‰tokenæ•°é‡æˆªæ–­å†…å®¹
        
        Args:
            content: åŸå§‹å†…å®¹
            max_tokens: æœ€å¤§tokenæ•°é‡
            
        Returns:
            æˆªæ–­åçš„å†…å®¹
        """
        current_tokens = self.estimate_tokens(content)
        
        if current_tokens <= max_tokens:
            return content
        
        # è®¡ç®—éœ€è¦ä¿ç•™çš„æ¯”ä¾‹
        keep_ratio = max_tokens / current_tokens
        
        # æ™ºèƒ½æˆªæ–­ï¼šä¿ç•™å¼€å¤´å’Œéƒ¨åˆ†ç»“å°¾
        lines = content.split('\n')
        total_lines = len(lines)
        
        # ä¿ç•™70%ç»™å¼€å¤´ï¼Œ30%ç»™ç»“å°¾
        head_lines = int(total_lines * keep_ratio * 0.7)
        tail_lines = int(total_lines * keep_ratio * 0.3)
        
        if head_lines + tail_lines >= total_lines:
            return content
        
        truncated_lines = []
        
        # æ·»åŠ å¼€å¤´éƒ¨åˆ†
        truncated_lines.extend(lines[:head_lines])
        
        # æ·»åŠ æˆªæ–­æ ‡è®°
        truncated_lines.append("")
        truncated_lines.append("// ... [å†…å®¹å› tokené™åˆ¶è¢«æˆªæ–­] ...")
        truncated_lines.append("")
        
        # æ·»åŠ ç»“å°¾éƒ¨åˆ†
        if tail_lines > 0:
            truncated_lines.extend(lines[-tail_lines:])
        
        truncated_content = '\n'.join(truncated_lines)
        
        # éªŒè¯æˆªæ–­åçš„tokenæ•°é‡
        new_tokens = self.estimate_tokens(truncated_content)
        logger.info(f"å†…å®¹æˆªæ–­: {current_tokens} -> {new_tokens} tokens (ç›®æ ‡: {max_tokens})")
        
        return truncated_content
    
    def _generate_recommendation(self, input_tokens: int, total_tokens: int, 
                               model_limit: int, is_within_limit: bool) -> str:
        """ç”Ÿæˆtokenä½¿ç”¨å»ºè®®"""
        
        if is_within_limit:
            usage_percent = (total_tokens / model_limit) * 100
            if usage_percent < 50:
                return f"âœ… Tokenä½¿ç”¨è‰¯å¥½ ({usage_percent:.1f}%)"
            else:
                return f"âš ï¸ Tokenä½¿ç”¨è¾ƒé«˜ ({usage_percent:.1f}%)ï¼Œå»ºè®®ä¼˜åŒ–"
        else:
            excess_tokens = total_tokens - model_limit
            return f"âŒ Tokenè¶…é™ (è¶…å‡º {excess_tokens} tokens)ï¼Œéœ€è¦åˆ†æ‰¹å¤„ç†æˆ–æˆªæ–­å†…å®¹"
    
    def get_model_info(self, model: str = "default") -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        limit = self.MODEL_LIMITS.get(model, self.MODEL_LIMITS['default'])
        
        return {
            "model": model,
            "token_limit": limit,
            "safe_limit": int(limit * 0.8),
            "recommended_batch_limit": int(limit * 0.6),
            "description": f"æ¨¡å‹ {model} çš„tokené™åˆ¶ä¸º {limit:,}"
        }
    
    def print_token_analysis(self, usage: TokenUsage, title: str = "Tokenåˆ†æ"):
        """æ‰“å°tokenåˆ†æç»“æœ"""
        print(f"\nğŸ“Š {title}")
        print("-" * 40)
        print(f"   ğŸ“¥ è¾“å…¥Token: {usage.input_tokens:,}")
        print(f"   ğŸ“¤ é¢„ä¼°è¾“å‡ºToken: {usage.estimated_output_tokens:,}")
        print(f"   ğŸ“Š æ€»Token: {usage.total_tokens:,}")
        print(f"   ğŸ¯ æ¨¡å‹é™åˆ¶: {usage.model_limit:,}")
        print(f"   ğŸ“ˆ ä½¿ç”¨ç‡: {(usage.total_tokens/usage.model_limit)*100:.1f}%")
        print(f"   ğŸ’¡ å»ºè®®: {usage.recommendation}")

# ä¾¿æ·å‡½æ•°
def quick_token_check(text: str, model: str = "claude-3-5-sonnet-20241022") -> TokenUsage:
    """å¿«é€Ÿæ£€æŸ¥æ–‡æœ¬çš„tokenä½¿ç”¨æƒ…å†µ"""
    calculator = TokenCalculator()
    return calculator.calculate_prompt_tokens(text, model)

def estimate_file_tokens(file_content: str) -> int:
    """ä¼°ç®—æ–‡ä»¶çš„tokenæ•°é‡"""
    calculator = TokenCalculator()
    return calculator.estimate_tokens(file_content)

def suggest_optimal_batching(files_content: Dict[str, str], 
                           model: str = "claude-3-5-sonnet-20241022") -> Tuple[int, List[List[str]]]:
    """å»ºè®®æœ€ä¼˜çš„æ–‡ä»¶åˆ†æ‰¹æ–¹æ¡ˆ"""
    calculator = TokenCalculator()
    return calculator.suggest_batch_size(files_content, model) 